{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crossing - 3T-18L\n",
    "\n",
    "### **3T-18L: 3 Teams composed of 18 agents (6,6,6) **\n",
    "\n",
    "The Notebook performs hyperparameter optimization on the Crossing Game.\n",
    "\n",
    "The following hyperparameters will be explored:\n",
    "- Temperature\n",
    "- River penalty\n",
    "- Proximity of 2nd food pile on the other side of the river"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.4\n",
      "Pytorch version: 0.4.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is the Crossing game environment\n",
    "from teams_env import CrossingEnv\n",
    "from tribes_model import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Culture = Pacifist\n",
    "\n",
    "Run finish_episode() specific to this culture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode(learners, optimizers, gamma, cuda):\n",
    "    \"\"\" \n",
    "    Note that in RL, policy gradient is calculated at the end of an episode and only then used to \n",
    "    update the weights of an agent's policy. This is very different compared to image recog.\n",
    "    \n",
    "    The code will perform policy update on each learning agent independently. Reward for each time \n",
    "    step is stored in the list policy.rewards[] --> r(t)\n",
    "    \"\"\"  \n",
    "    \n",
    "    num_learners = len(learners)\n",
    "    total_norms = [0 for i in range(num_learners)]\n",
    "    policy_losses = [[] for i in range(num_learners)]\n",
    "    losses = [[] for i in range(num_learners)]\n",
    "    T_reward = []\n",
    "\n",
    "   \n",
    "    for i in range(num_learners):\n",
    "\n",
    "        R = 0\n",
    "        saved_actions = learners[i].saved_actions\n",
    "        \n",
    "        for t in tribes:\n",
    "            if t.name is learners[i].tribe:\n",
    "                T_reward = t.tribal_awards(tag_hist = learners[i].tag_hist)\n",
    " \n",
    "                # For debug only\n",
    "                # print('Agent{} receives tribal award from Tribe{}'.format(i,t.name))\n",
    "                # print (T_reward)\n",
    "                # print (learners[i].rewards)\n",
    "                \n",
    "        # Do not implement actor-critic for now\n",
    "        # value_losses = []\n",
    "        \n",
    "        rewards = deque()\n",
    "\n",
    "        for r,T in zip(learners[i].rewards[::-1],T_reward[::-1]):\n",
    "            # The agent is incentivized to cooperate by an award of 30% of what the tribe takes\n",
    "            # in by all its members\n",
    "            R = r + T + gamma * R\n",
    "            rewards.appendleft(R)\n",
    "            \n",
    "        rewards = list(rewards)\n",
    "        rewards = torch.Tensor(rewards)\n",
    "        if cuda:\n",
    "            rewards = rewards.cuda()\n",
    "\n",
    "        # z-score rewards\n",
    "        rewards = (rewards - rewards.mean()) / (1.1e-7+rewards.std())\n",
    "        \n",
    "        #Debug     \n",
    "        #print (rewards)       \n",
    "        \n",
    "        \"\"\"\n",
    "        Do not implement actor-critic for now!!!\n",
    "        for (log_prob, state_value), r in zip(saved_actions, rewards):\n",
    "            reward = r - state_value.data[0]\n",
    "            policy_losses.append(-log_prob * Variable(reward))\n",
    "            r = torch.Tensor([r])\n",
    "            if cuda:\n",
    "                r = r.cuda()\n",
    "            value_losses.append(torch.nn.functional.smooth_l1_loss(state_value,\n",
    "                                                               Variable(r)))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "        loss.backward()        \n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        for log_prob, r in zip(saved_actions, rewards):\n",
    "            r = torch.Tensor([r])\n",
    "            if cuda:\n",
    "                r = r.cuda()\n",
    "            policy_losses[i].append(-log_prob * Variable(r))\n",
    "\n",
    "        optimizers[i].zero_grad()\n",
    "        losses[i] = torch.stack(policy_losses[i]).sum()\n",
    "        losses[i].backward()\n",
    "        \n",
    "        # Gradient Clipping Update: prevent exploding gradient\n",
    "        total_norms[i] = torch.nn.utils.clip_grad_norm_(learners[i].parameters(), 8000)\n",
    "        \n",
    "        optimizers[i].step()\n",
    "        learners[i].clear_history()   # clear an agent's history at the end of episode\n",
    "\n",
    "\n",
    "    return total_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learner agent 0\n",
      "Learner agent 1\n",
      "Learner agent 2\n",
      "Learner agent 3\n",
      "Learner agent 4\n",
      "Learner agent 5\n",
      "Learner agent 6\n",
      "Learner agent 7\n",
      "Learner agent 8\n",
      "Learner agent 9\n",
      "Learner agent 10\n",
      "Learner agent 11\n",
      "Learner agent 12\n",
      "Learner agent 13\n",
      "Learner agent 14\n",
      "Learner agent 15\n",
      "Learner agent 16\n",
      "Learner agent 17\n",
      "........................................................................................................................................................................................................\n",
      "Episode 200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: -3.262\n",
      "Learner:1\tReward total:7\tRunning mean: -6.336\n",
      "Learner:2\tReward total:1\tRunning mean: -2.989\n",
      "Learner:3\tReward total:1.0\tRunning mean: -3.383\n",
      "Learner:4\tReward total:0\tRunning mean: -0.8841\n",
      "Learner:5\tReward total:0\tRunning mean: -4.448\n",
      "Learner:6\tReward total:-58.0\tRunning mean: -6.705\n",
      "Learner:7\tReward total:-95.0\tRunning mean: -5.366\n",
      "Learner:8\tReward total:5\tRunning mean: 1.621\n",
      "Learner:9\tReward total:7\tRunning mean: -0.2383\n",
      "Learner:10\tReward total:27\tRunning mean: 9.182\n",
      "Learner:11\tReward total:40\tRunning mean: 17.3\n",
      "Learner:12\tReward total:29\tRunning mean: 1.77\n",
      "Learner:13\tReward total:-13.0\tRunning mean: 14.62\n",
      "Learner:14\tReward total:53\tRunning mean: 20.45\n",
      "Learner:15\tReward total:50\tRunning mean: 45.12\n",
      "Learner:16\tReward total:53\tRunning mean: 26.67\n",
      "Learner:17\tReward total:42\tRunning mean: 12.81\n",
      "Max Norms =  ['0.00', '188.38', '176.54', '170.76', '0.00', '0.00', '207.40', '155.65', '156.09', '195.89', '198.58', '190.45', '147.65', '188.22', '227.23', '251.24', '176.86', '207.73']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 400 complete\n",
      "Learner:0\tReward total:21\tRunning mean: 0.09398\n",
      "Learner:1\tReward total:9\tRunning mean: 1.104\n",
      "Learner:2\tReward total:38\tRunning mean: 13.67\n",
      "Learner:3\tReward total:1\tRunning mean: -4.695\n",
      "Learner:4\tReward total:0\tRunning mean: -0.5788\n",
      "Learner:5\tReward total:-48.0\tRunning mean: -11.18\n",
      "Learner:6\tReward total:1\tRunning mean: -2.613\n",
      "Learner:7\tReward total:33\tRunning mean: 1.185\n",
      "Learner:8\tReward total:35\tRunning mean: 22.62\n",
      "Learner:9\tReward total:27\tRunning mean: 25.48\n",
      "Learner:10\tReward total:25\tRunning mean: 24.58\n",
      "Learner:11\tReward total:34\tRunning mean: 28.2\n",
      "Learner:12\tReward total:38\tRunning mean: 23.83\n",
      "Learner:13\tReward total:15\tRunning mean: 33.61\n",
      "Learner:14\tReward total:18\tRunning mean: 35.04\n",
      "Learner:15\tReward total:34\tRunning mean: 43.56\n",
      "Learner:16\tReward total:9\tRunning mean: 39.3\n",
      "Learner:17\tReward total:27\tRunning mean: 33.03\n",
      "Max Norms =  ['236.52', '169.02', '210.46', '166.30', '0.00', '212.46', '157.32', '105.07', '123.89', '184.15', '118.66', '66.47', '134.21', '111.22', '239.53', '113.28', '171.77', '120.28']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 600 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 11.31\n",
      "Learner:1\tReward total:0\tRunning mean: 11.45\n",
      "Learner:2\tReward total:0\tRunning mean: 18.41\n",
      "Learner:3\tReward total:15\tRunning mean: 1.175\n",
      "Learner:4\tReward total:0\tRunning mean: 0.1764\n",
      "Learner:5\tReward total:-7.0\tRunning mean: -3.568\n",
      "Learner:6\tReward total:24\tRunning mean: 6.194\n",
      "Learner:7\tReward total:0\tRunning mean: 12.53\n",
      "Learner:8\tReward total:39\tRunning mean: 19.31\n",
      "Learner:9\tReward total:0\tRunning mean: 24.01\n",
      "Learner:10\tReward total:5\tRunning mean: 18.73\n",
      "Learner:11\tReward total:10\tRunning mean: 22.2\n",
      "Learner:12\tReward total:49\tRunning mean: 33.89\n",
      "Learner:13\tReward total:50\tRunning mean: 39.77\n",
      "Learner:14\tReward total:50\tRunning mean: 34.9\n",
      "Learner:15\tReward total:52\tRunning mean: 44.36\n",
      "Learner:16\tReward total:32\tRunning mean: 39.58\n",
      "Learner:17\tReward total:49\tRunning mean: 39.84\n",
      "Max Norms =  ['0.00', '0.00', '0.00', '118.26', '0.00', '218.08', '197.98', '0.00', '92.43', '0.00', '60.82', '67.67', '79.27', '43.50', '166.36', '50.42', '82.80', '2.33']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 800 complete\n",
      "Learner:0\tReward total:15\tRunning mean: 15.2\n",
      "Learner:1\tReward total:0\tRunning mean: 11.46\n",
      "Learner:2\tReward total:7\tRunning mean: 15.73\n",
      "Learner:3\tReward total:15\tRunning mean: 6.017\n",
      "Learner:4\tReward total:0\tRunning mean: 1.169\n",
      "Learner:5\tReward total:0\tRunning mean: -0.5889\n",
      "Learner:6\tReward total:0\tRunning mean: 5.176\n",
      "Learner:7\tReward total:28\tRunning mean: 17.37\n",
      "Learner:8\tReward total:48\tRunning mean: 14.29\n",
      "Learner:9\tReward total:20\tRunning mean: 17.24\n",
      "Learner:10\tReward total:1\tRunning mean: 14.75\n",
      "Learner:11\tReward total:0\tRunning mean: 17.09\n",
      "Learner:12\tReward total:49\tRunning mean: 41.01\n",
      "Learner:13\tReward total:8\tRunning mean: 40.96\n",
      "Learner:14\tReward total:49\tRunning mean: 37.74\n",
      "Learner:15\tReward total:51\tRunning mean: 43.43\n",
      "Learner:16\tReward total:51\tRunning mean: 44.67\n",
      "Learner:17\tReward total:49\tRunning mean: 42.45\n",
      "Max Norms =  ['145.33', '0.00', '74.03', '126.33', '0.00', '0.00', '0.00', '67.18', '87.70', '40.34', '122.16', '0.00', '18.66', '184.55', '205.29', '37.71', '60.80', '21.91']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1000 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 11.05\n",
      "Learner:1\tReward total:46\tRunning mean: 8.292\n",
      "Learner:2\tReward total:0\tRunning mean: 13.77\n",
      "Learner:3\tReward total:44\tRunning mean: 5.956\n",
      "Learner:4\tReward total:0\tRunning mean: 2.382\n",
      "Learner:5\tReward total:1\tRunning mean: 0.3984\n",
      "Learner:6\tReward total:23\tRunning mean: 9.595\n",
      "Learner:7\tReward total:0\tRunning mean: 29.52\n",
      "Learner:8\tReward total:0\tRunning mean: 11.2\n",
      "Learner:9\tReward total:0\tRunning mean: 16.37\n",
      "Learner:10\tReward total:0\tRunning mean: 21.96\n",
      "Learner:11\tReward total:0\tRunning mean: 19.48\n",
      "Learner:12\tReward total:49\tRunning mean: 43.76\n",
      "Learner:13\tReward total:50\tRunning mean: 32.07\n",
      "Learner:14\tReward total:50\tRunning mean: 40.78\n",
      "Learner:15\tReward total:30\tRunning mean: 26.09\n",
      "Learner:16\tReward total:51\tRunning mean: 47.13\n",
      "Learner:17\tReward total:49\tRunning mean: 46.14\n",
      "Max Norms =  ['0.00', '14.51', '0.00', '73.50', '0.00', '184.24', '177.52', '0.00', '0.00', '0.00', '0.00', '0.00', '19.48', '52.76', '60.56', '131.19', '37.95', '0.59']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1200 complete\n",
      "Learner:0\tReward total:1\tRunning mean: 12.28\n",
      "Learner:1\tReward total:0\tRunning mean: 11.42\n",
      "Learner:2\tReward total:0\tRunning mean: 10.56\n",
      "Learner:3\tReward total:0\tRunning mean: 10.98\n",
      "Learner:4\tReward total:0\tRunning mean: 4.269\n",
      "Learner:5\tReward total:0\tRunning mean: 4.308\n",
      "Learner:6\tReward total:0\tRunning mean: 6.692\n",
      "Learner:7\tReward total:47\tRunning mean: 31.77\n",
      "Learner:8\tReward total:42\tRunning mean: 13.49\n",
      "Learner:9\tReward total:2\tRunning mean: 16.82\n",
      "Learner:10\tReward total:29\tRunning mean: 18.59\n",
      "Learner:11\tReward total:19\tRunning mean: 18.55\n",
      "Learner:12\tReward total:47\tRunning mean: 38.45\n",
      "Learner:13\tReward total:29\tRunning mean: 32.36\n",
      "Learner:14\tReward total:28\tRunning mean: 38.08\n",
      "Learner:15\tReward total:51\tRunning mean: 29.93\n",
      "Learner:16\tReward total:50\tRunning mean: 43.47\n",
      "Learner:17\tReward total:46\tRunning mean: 38.24\n",
      "Max Norms =  ['300.74', '0.00', '0.00', '0.00', '0.00', '0.00', '0.00', '3.40', '128.72', '129.94', '70.52', '49.17', '13.78', '24.26', '67.36', '291.61', '3.83', '111.47']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1400 complete\n",
      "Learner:0\tReward total:3\tRunning mean: 13.93\n",
      "Learner:1\tReward total:35\tRunning mean: 9.778\n",
      "Learner:2\tReward total:16\tRunning mean: 14.27\n",
      "Learner:3\tReward total:19\tRunning mean: 14.56\n",
      "Learner:4\tReward total:3\tRunning mean: 3.903\n",
      "Learner:5\tReward total:28\tRunning mean: 7.708\n",
      "Learner:6\tReward total:0\tRunning mean: 7.895\n",
      "Learner:7\tReward total:25\tRunning mean: 30.05\n",
      "Learner:8\tReward total:46\tRunning mean: 34.6\n",
      "Learner:9\tReward total:0\tRunning mean: 18.57\n",
      "Learner:10\tReward total:14\tRunning mean: 15.28\n",
      "Learner:11\tReward total:0\tRunning mean: 10.89\n",
      "Learner:12\tReward total:25\tRunning mean: 35.56\n",
      "Learner:13\tReward total:18\tRunning mean: 31.45\n",
      "Learner:14\tReward total:7\tRunning mean: 24.76\n",
      "Learner:15\tReward total:12\tRunning mean: 37.14\n",
      "Learner:16\tReward total:48\tRunning mean: 35.46\n",
      "Learner:17\tReward total:44\tRunning mean: 35.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['141.67', '59.86', '117.75', '104.24', '216.07', '141.57', '0.00', '57.89', '187.46', '0.00', '151.90', '0.00', '22.72', '25.64', '126.63', '115.11', '26.12', '92.32']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1600 complete\n",
      "Learner:0\tReward total:45\tRunning mean: 14.17\n",
      "Learner:1\tReward total:5\tRunning mean: 8.84\n",
      "Learner:2\tReward total:0\tRunning mean: 20.0\n",
      "Learner:3\tReward total:46\tRunning mean: 21.95\n",
      "Learner:4\tReward total:0\tRunning mean: 6.596\n",
      "Learner:5\tReward total:0\tRunning mean: 8.532\n",
      "Learner:6\tReward total:46\tRunning mean: 13.8\n",
      "Learner:7\tReward total:44\tRunning mean: 23.3\n",
      "Learner:8\tReward total:46\tRunning mean: 43.91\n",
      "Learner:9\tReward total:3\tRunning mean: 15.73\n",
      "Learner:10\tReward total:3\tRunning mean: 8.677\n",
      "Learner:11\tReward total:0\tRunning mean: 5.751\n",
      "Learner:12\tReward total:4\tRunning mean: 36.68\n",
      "Learner:13\tReward total:4\tRunning mean: 33.31\n",
      "Learner:14\tReward total:3\tRunning mean: 13.44\n",
      "Learner:15\tReward total:48\tRunning mean: 34.78\n",
      "Learner:16\tReward total:51\tRunning mean: 38.5\n",
      "Learner:17\tReward total:47\tRunning mean: 38.31\n",
      "Max Norms =  ['74.35', '101.22', '0.00', '28.37', '0.00', '0.00', '118.47', '1.80', '48.69', '62.44', '17.29', '0.00', '1.82', '8.24', '89.41', '20.30', '34.89', '11.17']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 1800 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 16.53\n",
      "Learner:1\tReward total:0\tRunning mean: 16.11\n",
      "Learner:2\tReward total:15\tRunning mean: 24.91\n",
      "Learner:3\tReward total:0\tRunning mean: 7.284\n",
      "Learner:4\tReward total:48\tRunning mean: 11.08\n",
      "Learner:5\tReward total:49\tRunning mean: 17.19\n",
      "Learner:6\tReward total:8\tRunning mean: 20.35\n",
      "Learner:7\tReward total:24\tRunning mean: 30.86\n",
      "Learner:8\tReward total:48\tRunning mean: 47.32\n",
      "Learner:9\tReward total:1\tRunning mean: 16.42\n",
      "Learner:10\tReward total:0\tRunning mean: 12.26\n",
      "Learner:11\tReward total:1\tRunning mean: 8.168\n",
      "Learner:12\tReward total:48\tRunning mean: 31.21\n",
      "Learner:13\tReward total:50\tRunning mean: 32.16\n",
      "Learner:14\tReward total:1\tRunning mean: 8.342\n",
      "Learner:15\tReward total:50\tRunning mean: 21.61\n",
      "Learner:16\tReward total:49\tRunning mean: 37.35\n",
      "Learner:17\tReward total:0\tRunning mean: 28.18\n",
      "Max Norms =  ['0.00', '0.00', '229.97', '0.00', '128.52', '68.97', '66.02', '11.42', '34.73', '42.04', '0.00', '134.08', '2.14', '0.58', '247.96', '3.97', '3.21', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2000 complete\n",
      "Learner:0\tReward total:48\tRunning mean: 19.71\n",
      "Learner:1\tReward total:0\tRunning mean: 22.05\n",
      "Learner:2\tReward total:51\tRunning mean: 26.03\n",
      "Learner:3\tReward total:0\tRunning mean: 0.9759\n",
      "Learner:4\tReward total:50\tRunning mean: 17.53\n",
      "Learner:5\tReward total:30\tRunning mean: 21.5\n",
      "Learner:6\tReward total:6\tRunning mean: 43.0\n",
      "Learner:7\tReward total:45\tRunning mean: 36.06\n",
      "Learner:8\tReward total:50\tRunning mean: 48.06\n",
      "Learner:9\tReward total:0\tRunning mean: 16.06\n",
      "Learner:10\tReward total:0\tRunning mean: 3.287\n",
      "Learner:11\tReward total:1\tRunning mean: 6.754\n",
      "Learner:12\tReward total:48\tRunning mean: 19.63\n",
      "Learner:13\tReward total:2\tRunning mean: 30.03\n",
      "Learner:14\tReward total:0\tRunning mean: 11.83\n",
      "Learner:15\tReward total:49\tRunning mean: 23.63\n",
      "Learner:16\tReward total:2\tRunning mean: 37.68\n",
      "Learner:17\tReward total:0\tRunning mean: 30.06\n",
      "Max Norms =  ['74.14', '0.00', '121.99', '0.00', '62.48', '163.85', '121.88', '5.80', '13.44', '0.00', '0.00', '10.13', '0.11', '4.29', '0.00', '0.89', '1.16', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2200 complete\n",
      "Learner:0\tReward total:0\tRunning mean: 14.63\n",
      "Learner:1\tReward total:0\tRunning mean: 25.86\n",
      "Learner:2\tReward total:49\tRunning mean: 31.39\n",
      "Learner:3\tReward total:0\tRunning mean: 0.1307\n",
      "Learner:4\tReward total:0\tRunning mean: 16.24\n",
      "Learner:5\tReward total:50\tRunning mean: 21.48\n",
      "Learner:6\tReward total:0\tRunning mean: 20.56\n",
      "Learner:7\tReward total:47\tRunning mean: 35.75\n",
      "Learner:8\tReward total:48\tRunning mean: 48.22\n",
      "Learner:9\tReward total:1\tRunning mean: 14.64\n",
      "Learner:10\tReward total:0\tRunning mean: 2.09\n",
      "Learner:11\tReward total:0\tRunning mean: 4.433\n",
      "Learner:12\tReward total:49\tRunning mean: 29.25\n",
      "Learner:13\tReward total:3\tRunning mean: 29.03\n",
      "Learner:14\tReward total:1\tRunning mean: 4.63\n",
      "Learner:15\tReward total:50\tRunning mean: 22.18\n",
      "Learner:16\tReward total:51\tRunning mean: 33.93\n",
      "Learner:17\tReward total:47\tRunning mean: 28.65\n",
      "Max Norms =  ['0.00', '0.00', '4.69', '0.00', '0.00', '128.63', '0.00', '0.14', '74.21', '4.84', '0.00', '0.00', '0.18', '13.71', '47.90', '30.26', '4.39', '21.22']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2400 complete\n",
      "Learner:0\tReward total:3\tRunning mean: 8.934\n",
      "Learner:1\tReward total:0\tRunning mean: 23.52\n",
      "Learner:2\tReward total:47\tRunning mean: 38.32\n",
      "Learner:3\tReward total:0\tRunning mean: 0.01752\n",
      "Learner:4\tReward total:2\tRunning mean: 24.37\n",
      "Learner:5\tReward total:44\tRunning mean: 26.71\n",
      "Learner:6\tReward total:1\tRunning mean: 20.49\n",
      "Learner:7\tReward total:48\tRunning mean: 28.44\n",
      "Learner:8\tReward total:48\tRunning mean: 48.49\n",
      "Learner:9\tReward total:49\tRunning mean: 11.36\n",
      "Learner:10\tReward total:1\tRunning mean: 2.453\n",
      "Learner:11\tReward total:1\tRunning mean: 6.422\n",
      "Learner:12\tReward total:48\tRunning mean: 28.89\n",
      "Learner:13\tReward total:49\tRunning mean: 25.01\n",
      "Learner:14\tReward total:3\tRunning mean: 7.549\n",
      "Learner:15\tReward total:50\tRunning mean: 23.7\n",
      "Learner:16\tReward total:3\tRunning mean: 26.55\n",
      "Learner:17\tReward total:1\tRunning mean: 27.41\n",
      "Max Norms =  ['117.25', '0.00', '1.03', '0.00', '207.86', '143.01', '265.29', '2.34', '11.98', '32.86', '115.81', '103.86', '0.05', '0.22', '37.46', '0.15', '5.93', '33.95']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2600 complete\n",
      "Learner:0\tReward total:1\tRunning mean: 13.28\n",
      "Learner:1\tReward total:0\tRunning mean: 12.08\n",
      "Learner:2\tReward total:49\tRunning mean: 27.46\n",
      "Learner:3\tReward total:0\tRunning mean: 0.002347\n",
      "Learner:4\tReward total:50\tRunning mean: 17.5\n",
      "Learner:5\tReward total:47\tRunning mean: 24.37\n",
      "Learner:6\tReward total:50\tRunning mean: 18.37\n",
      "Learner:7\tReward total:47\tRunning mean: 28.96\n",
      "Learner:8\tReward total:49\tRunning mean: 48.41\n",
      "Learner:9\tReward total:0\tRunning mean: 15.04\n",
      "Learner:10\tReward total:0\tRunning mean: 14.17\n",
      "Learner:11\tReward total:0\tRunning mean: 15.95\n",
      "Learner:12\tReward total:46\tRunning mean: 21.72\n",
      "Learner:13\tReward total:0\tRunning mean: 28.2\n",
      "Learner:14\tReward total:4\tRunning mean: 20.01\n",
      "Learner:15\tReward total:50\tRunning mean: 31.98\n",
      "Learner:16\tReward total:3\tRunning mean: 25.36\n",
      "Learner:17\tReward total:0\tRunning mean: 11.44\n",
      "Max Norms =  ['35.57', '0.00', '12.45', '0.00', '28.15', '34.80', '213.86', '0.21', '22.13', '0.00', '0.00', '0.00', '39.70', '0.00', '56.38', '0.32', '1.77', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 2800 complete\n",
      "Learner:0\tReward total:49\tRunning mean: 30.42\n",
      "Learner:1\tReward total:0\tRunning mean: 1.649\n",
      "Learner:2\tReward total:0\tRunning mean: 28.62\n",
      "Learner:3\tReward total:0\tRunning mean: 0.0003144\n",
      "Learner:4\tReward total:3\tRunning mean: 19.04\n",
      "Learner:5\tReward total:0\tRunning mean: 27.16\n",
      "Learner:6\tReward total:0\tRunning mean: 3.992\n",
      "Learner:7\tReward total:46\tRunning mean: 32.56\n",
      "Learner:8\tReward total:50\tRunning mean: 47.87\n",
      "Learner:9\tReward total:49\tRunning mean: 16.61\n",
      "Learner:10\tReward total:0\tRunning mean: 12.27\n",
      "Learner:11\tReward total:47\tRunning mean: 22.21\n",
      "Learner:12\tReward total:47\tRunning mean: 22.98\n",
      "Learner:13\tReward total:47\tRunning mean: 19.98\n",
      "Learner:14\tReward total:8\tRunning mean: 32.49\n",
      "Learner:15\tReward total:50\tRunning mean: 41.53\n",
      "Learner:16\tReward total:2\tRunning mean: 18.86\n",
      "Learner:17\tReward total:0\tRunning mean: 2.745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Norms =  ['73.95', '0.00', '0.00', '0.00', '182.50', '0.00', '0.00', '0.40', '7.87', '0.30', '0.00', '36.90', '0.05', '0.19', '161.28', '0.25', '0.15', '0.00']\n",
      "........................................................................................................................................................................................................\n",
      "Episode 3000 complete\n",
      "Learner:0\tReward total:49\tRunning mean: 33.93\n",
      "Learner:1\tReward total:0\tRunning mean: 0.293\n",
      "Learner:2\tReward total:49\tRunning mean: 23.86\n",
      "Learner:3\tReward total:0\tRunning mean: 4.213e-05\n",
      "Learner:4\tReward total:0\tRunning mean: 13.89\n",
      "Learner:5\tReward total:49\tRunning mean: 32.05\n",
      "Learner:6\tReward total:0\tRunning mean: 1.488\n",
      "Learner:7\tReward total:50\tRunning mean: 30.46\n",
      "Learner:8\tReward total:49\tRunning mean: 50.41\n",
      "Learner:9\tReward total:0\tRunning mean: 15.46\n",
      "Learner:10\tReward total:48\tRunning mean: 14.42\n",
      "Learner:11\tReward total:0\tRunning mean: 11.44\n",
      "Learner:12\tReward total:0\tRunning mean: 18.52\n",
      "Learner:13\tReward total:48\tRunning mean: 21.64\n",
      "Learner:14\tReward total:4\tRunning mean: 42.18\n",
      "Learner:15\tReward total:2\tRunning mean: 43.58\n",
      "Learner:16\tReward total:2\tRunning mean: 23.67\n",
      "Learner:17\tReward total:0\tRunning mean: 1.443\n",
      "Max Norms =  ['151.99', '0.00', '26.99', '0.00', '0.00', '120.79', '0.00', '1.49', '9.74', '0.00', '12.95', '0.00', '0.00', '0.30', '92.62', '0.10', '0.06', '0.00']\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import sys\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "# Initialize agents parameters\n",
    "#   18 agents - 18 learning agents, 0 trained agent, 0 random agent\n",
    "num_learners = 18\n",
    "num_trained = 0\n",
    "num_rdn = 0\n",
    "num_statics = num_trained + num_rdn\n",
    "num_agents = num_learners + num_statics  \n",
    "\n",
    "# Initialize environment\n",
    "game = \"Crossing\"\n",
    "num_actions = 8                       # 8 actions in Gathering\n",
    "\n",
    "# Initialize training parameters\n",
    "warm_start = False\n",
    "num_frames = 5      # environ observation consists of a list of 5 stacked frames per agent\n",
    "max_episodes = 3000\n",
    "max_frames_ep = 0   # track highest number of frames an episode can last\n",
    "\n",
    "# These trainer parameters works for Atari Breakout\n",
    "gamma = 0.99  \n",
    "lr = 1e-3\n",
    "\n",
    "# Parameter sets\n",
    "parameters =[ \n",
    "            #{'temp_start':10.0, 'river_penalty':0},  # Temperature for explore/exploit; penalty per step in river\n",
    "            #{'temp_start':10.0, 'river_penalty':-0.5},\n",
    "            #{'temp_start':10.0, 'river_penalty':-1.0},\n",
    "            #{'temp_start':5.0, 'river_penalty':0},\n",
    "            #{'temp_start':5.0, 'river_penalty':-0.5},\n",
    "            #{'temp_start':5.0, 'river_penalty':-1.0},\n",
    "            #{'temp_start':2.0, 'river_penalty':0},\n",
    "            #{'temp_start':2.0, 'river_penalty':-0.5},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':800},\n",
    "            {'temp_start':5.0, 'river_penalty':-1.0, 'game_steps':2000},\n",
    "            {'temp_start':10.0, 'river_penalty':-1.0, 'game_steps':2000},\n",
    "            {'temp_start':40.0, 'river_penalty':-1.0, 'game_steps':800}\n",
    "            ]\n",
    "temp_end = 1.0\n",
    "\n",
    "log_interval = 200\n",
    "save_interval = 500\n",
    "\n",
    "def unpack_env_obs(env_obs):\n",
    "    \"\"\"\n",
    "    Gathering is a partially-observable Markov Game. env_obs returned by GatheringEnv is a numpy \n",
    "    array of dimension (num_agent, 800), which represents the agents' observations of the game.\n",
    "\n",
    "    The 800 elements (view_box) encodes 5 layers of 10x20 pixels frames in the format:\n",
    "    (viewbox_width, viewbox_depth, 5).\n",
    "    \n",
    "    This code reshapes the above into stacked frames that can be accepted by the Policy class:\n",
    "    (batch_idx, in_channel, width, height)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    num_agents = len(env_obs)  # environ observations is a list of agents' observations\n",
    "    \n",
    "    obs = []\n",
    "    for i in range(num_agents):\n",
    "        x = env_obs[i]   # take the indexed agent's observation\n",
    "        x = torch.Tensor(x)   # Convert to tensor\n",
    "        \n",
    "        # Policy is a 3-layer CNN\n",
    "        x = x.view(1, 10, 20, -1)  # reshape into environment defined stacked frames\n",
    "        x = x.permute(0, 3, 1, 2)  # permute to Policy accepted stacked frames\n",
    "        obs.append(x)\n",
    "        \n",
    "    return obs  # return a list of Tensors\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "For now, we do not implement LSTM            \n",
    "# LSTM Change: Need to cycle hx and cx thru function\n",
    "def select_action(model, state, lstm_hc, cuda):\n",
    "    hx , cx = lstm_hc \n",
    "    num_frames, height, width = state.shape\n",
    "    state = torch.FloatTensor(state.reshape(-1, num_frames, height, width))\n",
    "\n",
    "    if cuda:\n",
    "        state = state.cuda()\n",
    "\n",
    "    probs, value, (hx, cx) = model((Variable(state), (hx, cx)))\n",
    "\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "    # LSTM Change: Need to cycle hx and cx thru function\n",
    "    return action.data[0], log_prob, value, (hx, cx)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def select_learner_action(model, obs, cuda):\n",
    "    \"\"\"\n",
    "    This code expects obs to be an array of stacked frames of the following dim:\n",
    "    (batch_idx, in_channel, width, height)\n",
    "    \n",
    "    This is inputted into model - the agent's Policy, which outputs a probability \n",
    "    distribution over available actions.\n",
    "    \n",
    "    Policy gradient is implemented using torch.distributions.Categorical. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Policy is a 3-layer CNN\n",
    "    # _, num_frames, width, height = obs.shape\n",
    "    # obs = torch.FloatTensor(obs.reshape(-1, num_frames, width, height))\n",
    "    \n",
    "    # Policy is a 2-layer NN for now\n",
    "    # obs = obs.view(1, -1)\n",
    "   \n",
    "    if cuda:\n",
    "        obs = obs.cuda()\n",
    "      \n",
    "    probs = model(obs)\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    log_prob = m.log_prob(action)\n",
    "\n",
    "    return action.item(), log_prob \n",
    "\n",
    "\n",
    "def load_info(agents, narrate=False):\n",
    "    for i in range(num_agents):    \n",
    "        agents[i].load_info(info[i])\n",
    "        if narrate:\n",
    "            if agents[i].tagged:\n",
    "                print('frame {}, agent{} is tagged'.format(frame,i))\n",
    "            if agents[i].laser_fired:\n",
    "                print('frame {}, agent{} fires its laser'.format(frame,i))\n",
    "                print('and hit {} US and {} THEM'.format(agents[i].US_hit, agents[i].THEM_hit))\n",
    "    return\n",
    "\n",
    "\n",
    "# The main code starts here!!!\n",
    "\n",
    "culture = {'name':'pacifist', 'penalty':-1.0}\n",
    "\n",
    "for parameter in parameters:   # Go down the list of parameter sets\n",
    "    \n",
    "    temp_start = parameter['temp_start']\n",
    "    river_penalty = parameter['river_penalty']\n",
    "    max_frames = parameter['game_steps']\n",
    "\n",
    "    # Data structure for agents\n",
    "    agents = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    tags = []\n",
    "    rewards = []\n",
    "    optimizers = []\n",
    "\n",
    "    # Cold start\n",
    "    if warm_start is False:\n",
    "   \n",
    "        # Initialize learner agents, then load static agents (trained followed by random)\n",
    "        for i in range(num_learners):\n",
    "            print(\"Learner agent {}\".format(i))\n",
    "            agents.append(Policy(num_frames, num_actions, i)) # No weights loaded for learning agent\n",
    "            optimizers.append(optim.Adam(agents[i].parameters(), lr=lr))\n",
    "        \n",
    "            # set up optimizer - this works for Atari Breakout\n",
    "            # optimizers.append(optim.RMSprop(agents[i].parameters(), lr=lr, weight_decay=0.1)) \n",
    "        \n",
    "        for i in range(num_learners, num_learners+num_trained):\n",
    "            print (\"Learning with trained agents - not implemented yet!\")\n",
    "            raise\n",
    "            \"\"\"\n",
    "            Disable for now! No need to train with trained agents.\n",
    "            agents.append(Policy(num_frames, num_actions, i))\n",
    "            agents[i].load_weights()         # load weight for static agent        \n",
    "            \"\"\"\n",
    "        for i in range(num_learners+num_trained, num_agents):\n",
    "            print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "\n",
    "    \n",
    "        # Initialize all agent data\n",
    "        actions = [0 for i in range(num_agents)]\n",
    "        log_probs = [0 for i in range(num_agents)]\n",
    "        tags = [0 for i in range(num_agents)]\n",
    "        rewards = [0 for i in range(num_agents)]\n",
    "\n",
    "        # Keep track of rewards learned by learners\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "        running_reward = [None for i in range(num_learners)]   # running average\n",
    "        running_rewards = [[] for i in range(num_learners)]   # history of running averages\n",
    "        best_reward = [0 for i in range(num_learners)]    # best running average (for storing best_model)\n",
    "\n",
    "        # This is to support warm start for training\n",
    "        prior_eps = 0\n",
    "\n",
    "    # Warm start\n",
    "    if warm_start:\n",
    "        print (\"Cannot warm start\")\n",
    "        raise\n",
    "    \n",
    "        \"\"\"\n",
    "        # Disable for now!  Need to ensure model can support training on GPU and game playing\n",
    "        # on both CPU and GPU.\n",
    "    \n",
    "        data_file = 'results/{}.p'.format(game)\n",
    "\n",
    "        try:\n",
    "            with open(data_file, 'rb') as f:\n",
    "                running_rewards = pickle.load(f)\n",
    "                running_reward = running_rewards[-1]\n",
    "\n",
    "            prior_eps = len(running_rewards)\n",
    "\n",
    "            model_file = 'saved_models/actor_critic_{}_ep_{}.p'.format(game, prior_eps)\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model Save and Load Update: Include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                model, optimizer = saved_model\n",
    "\n",
    "        except OSError:\n",
    "            print('Saved file not found. Creating new cold start model.')\n",
    "            model = Policy(input_channels=num_frames, num_actions=num_actions)\n",
    "            optimizer = optim.RMSprop(model.parameters(), lr=lr,\n",
    "                                      weight_decay=0.1)\n",
    "            running_rewards = []\n",
    "            prior_eps = 0\n",
    "        \"\"\"\n",
    "\n",
    "    # Establish tribal association\n",
    "\n",
    "    tribes = []\n",
    "    tribes.append(Tribe(name='Vikings',color='blue', culture=culture, \\\n",
    "                    agents=[agents[0], agents[1], agents[2], agents[3], agents[4], agents[5]]))\n",
    "    tribes.append(Tribe(name='Saxons', color='red', culture=culture, \\\n",
    "                    agents=[agents[6], agents[7], agents[8], agents[9], agents[10], agents[11]]))\n",
    "    tribes.append(Tribe(name='Franks', color='purple', culture=culture, \\\n",
    "                    agents=[agents[12], agents[13], agents[14], agents[15], agents[16], agents[17]]))\n",
    "    # tribes.append(Tribe(name='Crazies', color='yellow', agents=[agents[9]]))   # random agents are crazy!!!\n",
    "\n",
    "\n",
    "    # 18 agents in 3 tribes, used map defined in default.txt\n",
    "    agent_colors = [agent.color for agent in agents]\n",
    "    agent_tribes = [agent.tribe for agent in agents]\n",
    "    env = CrossingEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, \\\n",
    "                       map_name='river_near', river_penalty=river_penalty)    \n",
    "   \n",
    "    \n",
    "    cuda = torch.cuda.is_available()\n",
    "\n",
    "    if cuda:\n",
    "        for i in range(num_learners):    # Learning agents need to utilize GPU\n",
    "            agents[i].cuda()\n",
    "\n",
    "        \n",
    "    for ep in range(max_episodes):\n",
    "    \n",
    "        print('.', end='')  # To show progress\n",
    "    \n",
    "        # Anneal temperature from temp_start to temp_end\n",
    "        for i in range(num_learners):    # For learning agents\n",
    "            agents[i].temperature = max(temp_end, temp_start - (temp_start - temp_end) * (ep / max_episodes))\n",
    "\n",
    "        env_obs = env.reset()  # Env return observations\n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(env_obs))\n",
    "        # print (env_obs[0].shape)\n",
    "    \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "\n",
    "        for i in range(num_learners):    # Reset agent info - laser tag statistics\n",
    "            agents[i].reset_info()   \n",
    "\n",
    "        # For Debug only\n",
    "        # print (len(agents_obs))\n",
    "        # print (agents_obs[0].shape)\n",
    "    \n",
    "        \"\"\"\n",
    "        For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "        state = np.stack([state]*num_frames)\n",
    "\n",
    "        # LSTM change - reset LSTM hidden units when episode begins\n",
    "        cx = Variable(torch.zeros(1, 256))\n",
    "        hx = Variable(torch.zeros(1, 256))\n",
    "        if cuda:\n",
    "            cx = cx.cuda()\n",
    "            hx = hx.cuda()\n",
    "        \"\"\"\n",
    "\n",
    "        episode_reward = [0 for i in range(num_learners)]   # reward for an episode\n",
    "    \n",
    "        for frame in range(max_frames):\n",
    "\n",
    "            \"\"\"\n",
    "            For now, we do not implement LSTM\n",
    "            # Select action\n",
    "            # LSTM Change: Need to cycle hx and cx thru select_action\n",
    "            action, log_prob, value, (hx,cx)  = select_action(model, state, (hx,cx), cuda)        \n",
    "            \"\"\"\n",
    "\n",
    "            for i in range(num_learners):    # For learning agents\n",
    "                actions[i], log_probs[i] = select_learner_action(agents[i], agents_obs[i], cuda)\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                agents[i].saved_actions.append((log_probs[i]))\n",
    "            \n",
    "                # Do not implement LSTM for now\n",
    "                # actions[i].saved_actions.append((log_prob, value))\n",
    "            \n",
    "            for i in range(num_learners, num_learners+num_trained):\n",
    "                print (\"No trained agent exist yet!\")\n",
    "                raise\n",
    "            for i in range(num_learners+num_trained, num_agents):   # For random agents\n",
    "                actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                if actions[i] is 6:\n",
    "                    tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "\n",
    "            # For Debug only\n",
    "            # if frame % 20 == 0:\n",
    "            #    print (actions) \n",
    "            #    print (log_probs)\n",
    "            \n",
    "            # Perform step        \n",
    "            env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "            \"\"\"\n",
    "            For Debug only\n",
    "            print (env_obs)\n",
    "            print (reward)\n",
    "            print (done) \n",
    "            \"\"\"\n",
    "       \n",
    "            # Unpack observations into data structure compatible with agent Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "            load_info(agents, narrate=False)   # Load agent info for AI agents\n",
    "\n",
    "            # For learner agents only, generate reward statistics and reward stack for policy gradient\n",
    "            for i in range(num_learners):\n",
    "                agents[i].rewards.append(reward[i])  # Stack rewards (for policy gradient)\n",
    "                episode_reward[i] += reward[i]   # accumulate episode reward \n",
    "            \n",
    "            \"\"\"\n",
    "            For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "            # Evict oldest diff add new diff to state\n",
    "            next_state = np.stack([next_state]*num_frames)\n",
    "            next_state[1:, :, :] = state[:-1, :, :]\n",
    "            state = next_state\n",
    "            \"\"\"\n",
    "\n",
    "            if any(done):\n",
    "                print(\"Done after {} frames\".format(frame))\n",
    "                break\n",
    "            \n",
    "        if frame > max_frames_ep:\n",
    "            max_frames_ep = frame    # Keep track of highest frames/episode\n",
    "\n",
    "        # Update reward statistics for learners\n",
    "        for i in range(num_learners):\n",
    "            if running_reward[i] is None:\n",
    "                running_reward[i] = episode_reward[i]\n",
    "            running_reward[i] = running_reward[i] * 0.99 + episode_reward[i] * 0.01\n",
    "            running_rewards[i].append(running_reward[i])\n",
    "\n",
    "        # Track Episode #, temp and highest frames/episode\n",
    "        if (ep+prior_eps+1) % log_interval == 0: \n",
    "            verbose_str = '\\nEpisode {} complete'.format(ep+prior_eps+1)\n",
    "            # verbose_str += '\\tTemp = {:.4}'.format(model.temperature)\n",
    "            # verbose_str += '\\tMax frames = {}'.format(max_frames_ep+1)\n",
    "            print(verbose_str)\n",
    "    \n",
    "            # Display rewards and running rewards for learning agents\n",
    "            for i in range(num_learners):\n",
    "                verbose_str = 'Learner:{}'.format(i)\n",
    "                verbose_str += '\\tReward total:{}'.format(episode_reward[i])\n",
    "                verbose_str += '\\tRunning mean: {:.4}'.format(running_reward[i])\n",
    "                print(verbose_str)\n",
    "    \n",
    "        # Update model\n",
    "        total_norms = finish_episode(agents[0:num_learners], optimizers[0:num_learners], gamma, cuda)\n",
    "\n",
    "        if (ep+prior_eps+1) % log_interval == 0:\n",
    "            print('Max Norms = ',[\"%0.2f\" % i for i in total_norms])\n",
    "        \n",
    "        if (ep+prior_eps+1) % save_interval == 0: \n",
    "            for i in range(num_learners):\n",
    "                model_dir = 'models/3T-18L/'\n",
    "                results_dir = 'results/3T-18L/'\n",
    "\n",
    "                model_file = model_dir+'{}/p{}_t{}_rp{}_{}gs/MA{}_{}_ep{}.p'.format(culture['name'], \\\n",
    "                        culture['penalty'], temp_start, river_penalty, max_frames, i, game, ep+prior_eps+1)\n",
    "                data_file = results_dir+'{}/p{}_t{}_rp{}_{}gs/MA{}_{}.p'.format(culture['name'], \\\n",
    "                        culture['penalty'], temp_start, river_penalty, max_frames, i, game)\n",
    "\n",
    "                os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "                os.makedirs(os.path.dirname(data_file), exist_ok=True)\n",
    "                \n",
    "                with open(model_file, 'wb') as f:\n",
    "                    # Model Save and Load Update: Include both model and optim parameters \n",
    "                    pickle.dump((agents[i].cpu(), optimizers[i]), f)\n",
    "\n",
    "                if cuda:\n",
    "                    agents[i] = agents[i].cuda()\n",
    "\n",
    "                with open(data_file, 'wb') as f:\n",
    "                    pickle.dump(running_rewards[i], f)    \n",
    "            \n",
    "    env.close()  # Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.river_penalty"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
