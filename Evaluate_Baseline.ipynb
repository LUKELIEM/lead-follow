{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Baseline\n",
    "\n",
    "### **1T-10L: 1 Teams composed of 10 agents **\n",
    "\n",
    "We run single/multiple-play to evaluate whether adjusting temperature and steps/episode (Baseline) can induce more agents to cross over and gather from the 2nd food pile.\n",
    "\n",
    "<img src=\"images/Crossing01.png\" width=\"600\">\n",
    "\n",
    "<img src=\"images/Crossing-river.png\" width=\"600\">\n",
    "\n",
    "The Crossing Game presents a more difficult problem than the Gathering game. In place of a single food pile, there are 2 food piles separated by a fixed distance or a barrier:\n",
    "\n",
    "* The smaller food pile is located closed to the agents, but has fewer food units than the number of agents. \n",
    "* The larger food pile has more food units than the number of agents but is located further away. The agents cannot see it unless they move away from the 1st food pile.\n",
    "* If there is a river, the agent suffers a -1.0 penalty for each game step in the river.\n",
    "\n",
    "The game thus deals with two challenging issues that are difficult for reinforcement learning algorithms:\n",
    "\n",
    "1. Sparce reward - the long distance an agent needs to explore with no reward to get to the 2nd food pile\n",
    "2. Local Optima - the presence of the 1st smaller food pile which the agents can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.8\n",
      "Pytorch version: 1.0.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is the Crossing game environment\n",
    "from teams_env import CrossingEnv\n",
    "from teams_model import *\n",
    "from interface import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Models\n",
    "\n",
    "The code block contains the folder locations of the trained models of follower agents as well as the parameters used in their training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    # Agents trained in map = food_d37\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/',   # scenario=1\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/',   # scenario=2\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_600gs/',   # scenario=3\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/',   # scenario=4\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_600gs/',   # scenario=5\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_300gs/',   # scenario=6\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_600gs/',   # scenario=7\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_1200gs/',   # scenario=8\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_300gs/',   # scenario=9\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_600gs/',   # scenario=10\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_1200gs/',   # scenario=11\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_300gs/',   # scenario=12\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_600gs/',   # scenario=13\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_1200gs/',   # scenario=14\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_300gs/',   # scenario=15\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_600gs/',   # scenario=16\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_1200gs/',   # scenario=17\n",
    "\n",
    "    # Agents trained in map = food_d37_river_w1_d25\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/\",   # scenario=18\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/\",   # scenario=19 \n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/\",   # scenario=20\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/\"   # scenario=21   \n",
    "]\n",
    "\n",
    "# Parameter sets pertaining to the trained models in the folders above (not used in the code)\n",
    "parameters =[ \n",
    "            # Temperature for explore/exploit; penalty per step in river; game steps per episode\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300}\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play A Single Game - Baseline\n",
    "\n",
    "Play a single game with rendering to observe agents' learning and resulting behaviors.\n",
    "\n",
    "User can change the scenario to load agent models from different folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location='cpu' to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-91da4dcab1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0;31m# Model File include both model and optim parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Load saved model for agent {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    369\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mroot_key\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m                 deserialized_objects[root_key] = restore_location(\n\u001b[0;32m--> 505\u001b[0;31m                     data_type(size), location)\n\u001b[0m\u001b[1;32m    506\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mroot_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mview_metadata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m     80\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location='cpu' to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from teams_env import CrossingEnv\n",
    "\n",
    "game = 'Crossing'\n",
    "map_name = \"food_d37_river_w1_d25\"\n",
    "# map_name = \"food_d37\"\n",
    "\n",
    "culture = \"pacifist\"\n",
    "scenario = 19\n",
    "dir_name = folders[scenario-1]\n",
    "episodes = 3000  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 10 agents - 0 teams of 0 AI agents each and 0 random agent\n",
    "num_ai_agents = 10\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_actions = 8                       # There are 8 actions defined in Gathering\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 1000\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_ai_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                # Model File include both model and optim parameters\n",
    "                saved_model = pickle.load(f)\n",
    "                agents[i], _ = saved_model\n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        agents.append(Policy(num_frames, num_actions, i))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Establish tribal association\n",
    "tribes = []\n",
    "tribes.append(Tribe(name='Vikings',color='blue', culture=culture, \\\n",
    "                    agents=[agents[0], agents[1], agents[2], agents[3], agents[4], \\\n",
    "                           agents[5], agents[6], agents[7], agents[8], agents[9]]))\n",
    "\n",
    "#tribes.append(Tribe(name='Saxons', color='red', culture=culture, \\\n",
    "#                    agents=[agents[4], agents[5], agents[6], agents[7]]))\n",
    "#tribes.append(Tribe(name='Franks', color='purple', culture=culture, \\\n",
    "#                    agents=[agents[8], agents[9], agents[10], agents[11]]))\n",
    "# tribes.append(Tribe(name='Crazies', color='yellow', agents=[agents[3], \\\n",
    "#                    agents[4], agents[5]]))   # random agents are crazy!!!\n",
    "\n",
    "# Set up agent and tribe info to pass into env\n",
    "agent_colors = [agent.color for agent in agents]\n",
    "agent_tribes = [agent.tribe for agent in agents]\n",
    "tribe_names = [tribe.name for tribe in tribes]\n",
    "    \n",
    "env = CrossingEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty, tribes=tribe_names, \\\n",
    "                  debug_agent=0)    \n",
    "    \n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    for i in range(num_ai_agents):    # Reset agent info - laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "            \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            US_hits[i] += agents[i].US_hit\n",
    "            THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for i in range(num_ai_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        env.render()\n",
    "        time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i in range(num_ai_agents):\n",
    "    agent_tags = sum(agents[i].tag_hist)\n",
    "    total_tags += agent_tags\n",
    "    print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/frame))\n",
    "\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "\n",
    "    agent_US_hits = sum(agents[i].US_hits)\n",
    "    agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "    total_US_hits += agent_US_hits\n",
    "    total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "    print('US agents hit = {}'.format(agent_US_hits))\n",
    "    print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "print ('Total rewards gathered = {}'.format(total_rewards))\n",
    "print ('Av. rewards per agent = {0:.2f}'.format(total_rewards/num_ai_agents))\n",
    "print ('Num laser fired = {}'.format(total_tags))\n",
    "print ('Total US Hit (friendly fire) = {}'.format(total_US_hits))\n",
    "print ('Total THEM Hit = {}'.format(total_THEM_hits))\n",
    "print ('friendly fire (%) = {0:.3f}'.format(total_US_hits/(total_US_hits+total_THEM_hits+1e-7)))\n",
    "\n",
    "for (i, loc) in env.consumption:\n",
    "    if loc[0] > second_pile_x:\n",
    "        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "        crossed[i] = 1\n",
    "        \n",
    "print (\"Num agents gathering from 2nd food pile: {}\".format(sum(crossed)))\n",
    "\n",
    "print ('\\nStatistics by Team')\n",
    "print ('===================')\n",
    "top_tribe = None\n",
    "top_tribe_reward = 0\n",
    "\n",
    "for i, tribe in enumerate(tribes):\n",
    "    if tribe.name is not 'Crazies':\n",
    "        tribe_reward = sum(tribe.sum_rewards())\n",
    "        print ('Tribe {} has total reward of {}'.format(tribe.name, tribe_reward))\n",
    "                           \n",
    "        if tribe_reward > top_tribe_reward:   # Keep track of dominating team\n",
    "            top_tribe_reward = tribe_reward\n",
    "            top_tribe = tribe.name\n",
    "\n",
    "# Team dominance calculation\n",
    "if len(tribes) > 1:\n",
    "    print ('Dominating Team: {}'.format(top_tribe))\n",
    "    dominance = top_tribe_reward/((total_rewards-top_tribe_reward+1.1e-7)/(len(tribes)-1))    \n",
    "    print ('Team dominance: {0:.2f}x'.format(dominance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Stats - Baseline (Map = food_d37)\n",
    "\n",
    "Our research requires gathering game stats for agents and teams over 30 episodes of game play:\n",
    "\n",
    "* Average agent reward - average number of apples gathered per agent per episode  \n",
    "* The number of agents gathering apples at the 2nd food pile \n",
    "\n",
    "Rendering is disabled to speed things up.\n",
    "\n",
    "<img src=\"images/Crossing01.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 274.7\n",
      "Av. agent reward = 27.47\n",
      "Agents crossed (2nd food pile) = 6.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 274.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 6.6\n",
      "Agent1 reward is 5.5\n",
      "Agent2 reward is 37.9\n",
      "Agent3 reward is 26.5\n",
      "Agent4 reward is 2.7\n",
      "Agent5 reward is 23.0\n",
      "Agent6 reward is 32.5\n",
      "Agent7 reward is 85.0\n",
      "Agent8 reward is 32.8\n",
      "Agent9 reward is 22.1\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 353.6\n",
      "Av. agent reward = 35.36\n",
      "Agents crossed (2nd food pile) = 6.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 353.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 10.8\n",
      "Agent1 reward is 18.5\n",
      "Agent2 reward is 52.0\n",
      "Agent3 reward is 33.1\n",
      "Agent4 reward is 23.9\n",
      "Agent5 reward is 52.4\n",
      "Agent6 reward is 22.0\n",
      "Agent7 reward is 58.7\n",
      "Agent8 reward is 55.3\n",
      "Agent9 reward is 26.8\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 361.1\n",
      "Av. agent reward = 36.11\n",
      "Agents crossed (2nd food pile) = 5.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 361.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 17.2\n",
      "Agent1 reward is 12.1\n",
      "Agent2 reward is 66.6\n",
      "Agent3 reward is 56.4\n",
      "Agent4 reward is 6.9\n",
      "Agent5 reward is 49.9\n",
      "Agent6 reward is 25.5\n",
      "Agent7 reward is 49.1\n",
      "Agent8 reward is 47.9\n",
      "Agent9 reward is 29.5\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 359.7\n",
      "Av. agent reward = 35.97\n",
      "Agents crossed (2nd food pile) = 4.8\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 359.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 29.3\n",
      "Agent1 reward is 19.1\n",
      "Agent2 reward is 87.3\n",
      "Agent3 reward is 42.4\n",
      "Agent4 reward is 14.7\n",
      "Agent5 reward is 0.4\n",
      "Agent6 reward is 26.2\n",
      "Agent7 reward is 58.3\n",
      "Agent8 reward is 53.9\n",
      "Agent9 reward is 28.1\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 313.8\n",
      "Av. agent reward = 31.38\n",
      "Agents crossed (2nd food pile) = 4.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 313.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 18.3\n",
      "Agent1 reward is 25.7\n",
      "Agent2 reward is 14.4\n",
      "Agent3 reward is 27.1\n",
      "Agent4 reward is 7.6\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 25.3\n",
      "Agent7 reward is 136.5\n",
      "Agent8 reward is 26.3\n",
      "Agent9 reward is 32.7\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 354.6\n",
      "Av. agent reward = 35.46\n",
      "Agents crossed (2nd food pile) = 4.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 354.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 28.6\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 59.4\n",
      "Agent3 reward is 56.4\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 29.3\n",
      "Agent7 reward is 99.3\n",
      "Agent8 reward is 48.2\n",
      "Agent9 reward is 33.4\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_600gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 296.8\n",
      "Av. agent reward = 29.68\n",
      "Agents crossed (2nd food pile) = 3.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 296.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 19.4\n",
      "Agent1 reward is 97.3\n",
      "Agent2 reward is 18.6\n",
      "Agent3 reward is 72.8\n",
      "Agent4 reward is 13.1\n",
      "Agent5 reward is 7.2\n",
      "Agent6 reward is 7.2\n",
      "Agent7 reward is 18.8\n",
      "Agent8 reward is 23.5\n",
      "Agent9 reward is 18.8\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 341.0\n",
      "Av. agent reward = 34.10\n",
      "Agents crossed (2nd food pile) = 3.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 341.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 8.6\n",
      "Agent1 reward is 109.3\n",
      "Agent2 reward is 39.1\n",
      "Agent3 reward is 92.5\n",
      "Agent4 reward is 11.7\n",
      "Agent5 reward is 11.2\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 18.4\n",
      "Agent8 reward is 25.4\n",
      "Agent9 reward is 24.9\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 332.6\n",
      "Av. agent reward = 33.26\n",
      "Agents crossed (2nd food pile) = 4.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 332.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 24.2\n",
      "Agent1 reward is 32.1\n",
      "Agent2 reward is 23.7\n",
      "Agent3 reward is 142.9\n",
      "Agent4 reward is 19.2\n",
      "Agent5 reward is 0.2\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 28.9\n",
      "Agent8 reward is 28.6\n",
      "Agent9 reward is 32.7\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 352.6\n",
      "Av. agent reward = 35.26\n",
      "Agents crossed (2nd food pile) = 5.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 352.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 22.0\n",
      "Agent1 reward is 33.0\n",
      "Agent2 reward is 26.0\n",
      "Agent3 reward is 153.5\n",
      "Agent4 reward is 26.6\n",
      "Agent5 reward is 3.3\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 26.5\n",
      "Agent8 reward is 28.0\n",
      "Agent9 reward is 33.6\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 353.0\n",
      "Av. agent reward = 35.30\n",
      "Agents crossed (2nd food pile) = 4.8\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 353.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 22.2\n",
      "Agent1 reward is 26.8\n",
      "Agent2 reward is 26.9\n",
      "Agent3 reward is 163.2\n",
      "Agent4 reward is 23.4\n",
      "Agent5 reward is 0.2\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 28.4\n",
      "Agent8 reward is 29.4\n",
      "Agent9 reward is 32.6\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 371.6\n",
      "Av. agent reward = 37.16\n",
      "Agents crossed (2nd food pile) = 5.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 371.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 44.2\n",
      "Agent1 reward is 68.1\n",
      "Agent2 reward is 34.4\n",
      "Agent3 reward is 75.1\n",
      "Agent4 reward is 48.2\n",
      "Agent5 reward is 10.9\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 30.0\n",
      "Agent8 reward is 28.1\n",
      "Agent9 reward is 32.6\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 246.1\n",
      "Av. agent reward = 24.61\n",
      "Agents crossed (2nd food pile) = 3.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 246.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.2\n",
      "Agent1 reward is 52.4\n",
      "Agent2 reward is 28.0\n",
      "Agent3 reward is 13.5\n",
      "Agent4 reward is 15.8\n",
      "Agent5 reward is 31.0\n",
      "Agent6 reward is 1.2\n",
      "Agent7 reward is 13.8\n",
      "Agent8 reward is 68.5\n",
      "Agent9 reward is 21.8\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1000 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 341.2\n",
      "Av. agent reward = 34.12\n",
      "Agents crossed (2nd food pile) = 5.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 341.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 8.0\n",
      "Agent1 reward is 18.3\n",
      "Agent2 reward is 20.8\n",
      "Agent3 reward is 58.0\n",
      "Agent4 reward is 29.1\n",
      "Agent5 reward is 19.1\n",
      "Agent6 reward is 7.0\n",
      "Agent7 reward is 58.3\n",
      "Agent8 reward is 42.4\n",
      "Agent9 reward is 80.2\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 355.9\n",
      "Av. agent reward = 35.59\n",
      "Agents crossed (2nd food pile) = 5.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 355.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 37.9\n",
      "Agent1 reward is 30.9\n",
      "Agent2 reward is 27.5\n",
      "Agent3 reward is 64.6\n",
      "Agent4 reward is 11.8\n",
      "Agent5 reward is 33.1\n",
      "Agent6 reward is 29.2\n",
      "Agent7 reward is 60.8\n",
      "Agent8 reward is 6.1\n",
      "Agent9 reward is 54.0\n",
      "Training time per epochs: 2.04 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 352.1\n",
      "Av. agent reward = 35.21\n",
      "Agents crossed (2nd food pile) = 4.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 352.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 29.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 20.6\n",
      "Agent3 reward is 44.5\n",
      "Agent4 reward is 20.6\n",
      "Agent5 reward is 31.2\n",
      "Agent6 reward is 24.8\n",
      "Agent7 reward is 60.4\n",
      "Agent8 reward is 46.9\n",
      "Agent9 reward is 74.0\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 343.2\n",
      "Av. agent reward = 34.32\n",
      "Agents crossed (2nd food pile) = 3.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 343.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 16.8\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 22.7\n",
      "Agent3 reward is 55.7\n",
      "Agent4 reward is 18.3\n",
      "Agent5 reward is 31.0\n",
      "Agent6 reward is 24.4\n",
      "Agent7 reward is 92.1\n",
      "Agent8 reward is 79.7\n",
      "Agent9 reward is 2.4\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 355.0\n",
      "Av. agent reward = 35.50\n",
      "Agents crossed (2nd food pile) = 3.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 355.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 61.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 15.2\n",
      "Agent3 reward is 64.0\n",
      "Agent4 reward is 28.1\n",
      "Agent5 reward is 28.9\n",
      "Agent6 reward is 20.7\n",
      "Agent7 reward is 96.4\n",
      "Agent8 reward is 9.1\n",
      "Agent9 reward is 31.6\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_600gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 312.2\n",
      "Av. agent reward = 31.22\n",
      "Agents crossed (2nd food pile) = 3.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 312.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 70.7\n",
      "Agent2 reward is 63.7\n",
      "Agent3 reward is 0.1\n",
      "Agent4 reward is 2.8\n",
      "Agent5 reward is 39.2\n",
      "Agent6 reward is 9.6\n",
      "Agent7 reward is 68.1\n",
      "Agent8 reward is 31.4\n",
      "Agent9 reward is 26.6\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 349.5\n",
      "Av. agent reward = 34.95\n",
      "Agents crossed (2nd food pile) = 4.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 349.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 81.8\n",
      "Agent2 reward is 74.3\n",
      "Agent3 reward is 2.1\n",
      "Agent4 reward is 11.2\n",
      "Agent5 reward is 37.2\n",
      "Agent6 reward is 28.7\n",
      "Agent7 reward is 55.2\n",
      "Agent8 reward is 29.8\n",
      "Agent9 reward is 29.2\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 356.3\n",
      "Av. agent reward = 35.63\n",
      "Agents crossed (2nd food pile) = 5.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 356.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 61.1\n",
      "Agent2 reward is 45.2\n",
      "Agent3 reward is 18.8\n",
      "Agent4 reward is 16.1\n",
      "Agent5 reward is 32.6\n",
      "Agent6 reward is 29.9\n",
      "Agent7 reward is 69.3\n",
      "Agent8 reward is 51.9\n",
      "Agent9 reward is 31.4\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 370.8\n",
      "Av. agent reward = 37.08\n",
      "Agents crossed (2nd food pile) = 4.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 370.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 97.6\n",
      "Agent2 reward is 52.4\n",
      "Agent3 reward is 13.3\n",
      "Agent4 reward is 36.6\n",
      "Agent5 reward is 75.6\n",
      "Agent6 reward is 29.2\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 38.6\n",
      "Agent9 reward is 27.5\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 368.4\n",
      "Av. agent reward = 36.84\n",
      "Agents crossed (2nd food pile) = 3.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 368.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 98.2\n",
      "Agent3 reward is 16.9\n",
      "Agent4 reward is 53.8\n",
      "Agent5 reward is 99.1\n",
      "Agent6 reward is 30.1\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 41.3\n",
      "Agent9 reward is 29.0\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 272.5\n",
      "Av. agent reward = 27.25\n",
      "Agents crossed (2nd food pile) = 1.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 272.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.2\n",
      "Agent3 reward is 12.5\n",
      "Agent4 reward is 22.9\n",
      "Agent5 reward is 171.1\n",
      "Agent6 reward is 6.5\n",
      "Agent7 reward is 0.5\n",
      "Agent8 reward is 28.7\n",
      "Agent9 reward is 30.0\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 179.7\n",
      "Av. agent reward = 17.97\n",
      "Agents crossed (2nd food pile) = 4.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 179.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 9.5\n",
      "Agent1 reward is 3.2\n",
      "Agent2 reward is 34.8\n",
      "Agent3 reward is 4.6\n",
      "Agent4 reward is 37.4\n",
      "Agent5 reward is 2.1\n",
      "Agent6 reward is 9.7\n",
      "Agent7 reward is 28.6\n",
      "Agent8 reward is 21.4\n",
      "Agent9 reward is 28.4\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 342.0\n",
      "Av. agent reward = 34.20\n",
      "Agents crossed (2nd food pile) = 4.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 342.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 25.2\n",
      "Agent1 reward is 64.4\n",
      "Agent2 reward is 0.6\n",
      "Agent3 reward is 32.5\n",
      "Agent4 reward is 2.2\n",
      "Agent5 reward is 66.5\n",
      "Agent6 reward is 22.3\n",
      "Agent7 reward is 19.1\n",
      "Agent8 reward is 80.2\n",
      "Agent9 reward is 29.0\n",
      "Training time per epochs: 2.04 sec\n",
      "###### Trained episodes = 1500 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 365.3\n",
      "Av. agent reward = 36.53\n",
      "Agents crossed (2nd food pile) = 5.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 365.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 39.8\n",
      "Agent1 reward is 56.0\n",
      "Agent2 reward is 1.4\n",
      "Agent3 reward is 21.0\n",
      "Agent4 reward is 0.2\n",
      "Agent5 reward is 70.3\n",
      "Agent6 reward is 46.0\n",
      "Agent7 reward is 24.3\n",
      "Agent8 reward is 71.7\n",
      "Agent9 reward is 34.7\n",
      "Training time per epochs: 2.04 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 364.7\n",
      "Av. agent reward = 36.47\n",
      "Agents crossed (2nd food pile) = 5.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 364.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 52.0\n",
      "Agent1 reward is 42.2\n",
      "Agent2 reward is 29.5\n",
      "Agent3 reward is 7.5\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 65.6\n",
      "Agent6 reward is 55.4\n",
      "Agent7 reward is 29.7\n",
      "Agent8 reward is 44.1\n",
      "Agent9 reward is 38.7\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 337.2\n",
      "Av. agent reward = 33.72\n",
      "Agents crossed (2nd food pile) = 2.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 337.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 12.0\n",
      "Agent1 reward is 8.6\n",
      "Agent2 reward is 4.5\n",
      "Agent3 reward is 14.4\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 91.2\n",
      "Agent6 reward is 27.6\n",
      "Agent7 reward is 28.6\n",
      "Agent8 reward is 119.0\n",
      "Agent9 reward is 31.3\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 375.0\n",
      "Av. agent reward = 37.50\n",
      "Agents crossed (2nd food pile) = 4.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 375.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 36.6\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 43.1\n",
      "Agent3 reward is 27.3\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 70.2\n",
      "Agent6 reward is 45.4\n",
      "Agent7 reward is 30.0\n",
      "Agent8 reward is 93.5\n",
      "Agent9 reward is 28.8\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_600gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 295.3\n",
      "Av. agent reward = 29.53\n",
      "Agents crossed (2nd food pile) = 2.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 295.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.3\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 1.9\n",
      "Agent3 reward is 113.2\n",
      "Agent4 reward is 0.6\n",
      "Agent5 reward is 17.3\n",
      "Agent6 reward is 87.8\n",
      "Agent7 reward is 24.1\n",
      "Agent8 reward is 23.2\n",
      "Agent9 reward is 25.9\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 329.9\n",
      "Av. agent reward = 32.99\n",
      "Agents crossed (2nd food pile) = 2.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 329.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 23.5\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 1.9\n",
      "Agent3 reward is 131.4\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 16.3\n",
      "Agent6 reward is 83.8\n",
      "Agent7 reward is 18.9\n",
      "Agent8 reward is 28.6\n",
      "Agent9 reward is 25.4\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 330.6\n",
      "Av. agent reward = 33.06\n",
      "Agents crossed (2nd food pile) = 1.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 330.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 15.2\n",
      "Agent3 reward is 236.5\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 31.1\n",
      "Agent6 reward is 15.0\n",
      "Agent7 reward is 1.8\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 349.3\n",
      "Av. agent reward = 34.93\n",
      "Agents crossed (2nd food pile) = 1.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 349.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 5.6\n",
      "Agent3 reward is 248.5\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 28.1\n",
      "Agent6 reward is 29.3\n",
      "Agent7 reward is 8.1\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 29.7\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 356.3\n",
      "Av. agent reward = 35.63\n",
      "Agents crossed (2nd food pile) = 1.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 356.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 6.6\n",
      "Agent3 reward is 263.8\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 28.1\n",
      "Agent6 reward is 27.4\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 30.4\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 93.0\n",
      "Av. agent reward = 9.30\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 93.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 2.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 30.0\n",
      "Agent6 reward is 29.0\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 32.0\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_1200gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 277.1\n",
      "Av. agent reward = 27.71\n",
      "Agents crossed (2nd food pile) = 1.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 277.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 5.9\n",
      "Agent1 reward is 0.7\n",
      "Agent2 reward is 21.7\n",
      "Agent3 reward is 21.8\n",
      "Agent4 reward is 36.3\n",
      "Agent5 reward is 10.4\n",
      "Agent6 reward is 7.8\n",
      "Agent7 reward is 132.2\n",
      "Agent8 reward is 15.0\n",
      "Agent9 reward is 25.3\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 299.6\n",
      "Av. agent reward = 29.96\n",
      "Agents crossed (2nd food pile) = 2.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 299.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.2\n",
      "Agent1 reward is 1.6\n",
      "Agent2 reward is 24.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 61.1\n",
      "Agent5 reward is 8.8\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 151.0\n",
      "Agent8 reward is 23.0\n",
      "Agent9 reward is 29.7\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 310.5\n",
      "Av. agent reward = 31.05\n",
      "Agents crossed (2nd food pile) = 1.8\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 310.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 29.6\n",
      "Agent2 reward is 26.9\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 171.2\n",
      "Agent5 reward is 1.5\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 31.7\n",
      "Agent8 reward is 20.1\n",
      "Agent9 reward is 29.3\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 2000 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 346.6\n",
      "Av. agent reward = 34.66\n",
      "Agents crossed (2nd food pile) = 2.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 346.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 61.0\n",
      "Agent2 reward is 14.3\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 113.3\n",
      "Agent5 reward is 45.9\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 53.2\n",
      "Agent8 reward is 26.0\n",
      "Agent9 reward is 32.8\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 355.4\n",
      "Av. agent reward = 35.54\n",
      "Agents crossed (2nd food pile) = 2.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 355.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 89.9\n",
      "Agent2 reward is 74.5\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 103.0\n",
      "Agent5 reward is 31.7\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 10.1\n",
      "Agent8 reward is 13.4\n",
      "Agent9 reward is 32.7\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 329.0\n",
      "Av. agent reward = 32.90\n",
      "Agents crossed (2nd food pile) = 1.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 329.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 228.8\n",
      "Agent2 reward is 32.6\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 30.2\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 3.0\n",
      "Agent9 reward is 34.3\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 196.2\n",
      "Av. agent reward = 19.62\n",
      "Agents crossed (2nd food pile) = 2.8\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 196.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 4.7\n",
      "Agent1 reward is 2.2\n",
      "Agent2 reward is 0.1\n",
      "Agent3 reward is 57.7\n",
      "Agent4 reward is 1.0\n",
      "Agent5 reward is 59.5\n",
      "Agent6 reward is 6.5\n",
      "Agent7 reward is 20.2\n",
      "Agent8 reward is 10.3\n",
      "Agent9 reward is 33.8\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 277.4\n",
      "Av. agent reward = 27.74\n",
      "Agents crossed (2nd food pile) = 3.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 277.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 8.1\n",
      "Agent1 reward is 5.6\n",
      "Agent2 reward is 3.8\n",
      "Agent3 reward is 70.6\n",
      "Agent4 reward is 3.7\n",
      "Agent5 reward is 97.6\n",
      "Agent6 reward is 19.1\n",
      "Agent7 reward is 13.7\n",
      "Agent8 reward is 31.7\n",
      "Agent9 reward is 23.7\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 343.9\n",
      "Av. agent reward = 34.39\n",
      "Agents crossed (2nd food pile) = 3.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 343.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 4.0\n",
      "Agent1 reward is 0.4\n",
      "Agent2 reward is 63.9\n",
      "Agent3 reward is 69.3\n",
      "Agent4 reward is 18.5\n",
      "Agent5 reward is 96.3\n",
      "Agent6 reward is 23.9\n",
      "Agent7 reward is 4.6\n",
      "Agent8 reward is 33.0\n",
      "Agent9 reward is 29.9\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 359.3\n",
      "Av. agent reward = 35.93\n",
      "Agents crossed (2nd food pile) = 4.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 359.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 49.7\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 35.7\n",
      "Agent3 reward is 80.2\n",
      "Agent4 reward is 24.1\n",
      "Agent5 reward is 79.1\n",
      "Agent6 reward is 24.6\n",
      "Agent7 reward is 3.0\n",
      "Agent8 reward is 33.0\n",
      "Agent9 reward is 30.0\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 352.2\n",
      "Av. agent reward = 35.22\n",
      "Agents crossed (2nd food pile) = 3.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 352.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 71.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 23.4\n",
      "Agent3 reward is 76.3\n",
      "Agent4 reward is 20.8\n",
      "Agent5 reward is 88.6\n",
      "Agent6 reward is 12.9\n",
      "Agent7 reward is 26.0\n",
      "Agent8 reward is 33.3\n",
      "Agent9 reward is 0.0\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 367.9\n",
      "Av. agent reward = 36.79\n",
      "Agents crossed (2nd food pile) = 3.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 367.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 96.3\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 17.4\n",
      "Agent3 reward is 74.5\n",
      "Agent4 reward is 8.9\n",
      "Agent5 reward is 86.7\n",
      "Agent6 reward is 21.0\n",
      "Agent7 reward is 29.0\n",
      "Agent8 reward is 33.1\n",
      "Agent9 reward is 0.9\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_600gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 261.9\n",
      "Av. agent reward = 26.19\n",
      "Agents crossed (2nd food pile) = 3.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 261.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 10.1\n",
      "Agent1 reward is 44.3\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 47.4\n",
      "Agent4 reward is 80.9\n",
      "Agent5 reward is 8.2\n",
      "Agent6 reward is 0.9\n",
      "Agent7 reward is 16.4\n",
      "Agent8 reward is 19.8\n",
      "Agent9 reward is 33.8\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 326.9\n",
      "Av. agent reward = 32.69\n",
      "Agents crossed (2nd food pile) = 3.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 326.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 66.7\n",
      "Agent1 reward is 22.4\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 79.1\n",
      "Agent4 reward is 67.8\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.4\n",
      "Agent7 reward is 30.1\n",
      "Agent8 reward is 29.9\n",
      "Agent9 reward is 30.5\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 360.9\n",
      "Av. agent reward = 36.09\n",
      "Agents crossed (2nd food pile) = 3.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 360.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 63.4\n",
      "Agent1 reward is 53.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 55.5\n",
      "Agent4 reward is 97.6\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 1.0\n",
      "Agent7 reward is 28.6\n",
      "Agent8 reward is 29.0\n",
      "Agent9 reward is 32.8\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 372.8\n",
      "Av. agent reward = 37.28\n",
      "Agents crossed (2nd food pile) = 2.8\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 372.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 90.1\n",
      "Agent1 reward is 107.2\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 5.4\n",
      "Agent4 reward is 77.3\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.1\n",
      "Agent7 reward is 29.7\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 32.9\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 2500 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 363.9\n",
      "Av. agent reward = 36.39\n",
      "Agents crossed (2nd food pile) = 2.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 363.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 142.7\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 128.6\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.1\n",
      "Agent7 reward is 29.6\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 355.8\n",
      "Av. agent reward = 35.58\n",
      "Agents crossed (2nd food pile) = 2.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 355.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 137.6\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 125.2\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 30.0\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_1200gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 321.3\n",
      "Av. agent reward = 32.13\n",
      "Agents crossed (2nd food pile) = 4.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 321.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 80.4\n",
      "Agent1 reward is 13.4\n",
      "Agent2 reward is 37.8\n",
      "Agent3 reward is 50.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 17.3\n",
      "Agent6 reward is 15.4\n",
      "Agent7 reward is 24.7\n",
      "Agent8 reward is 12.5\n",
      "Agent9 reward is 69.8\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 370.9\n",
      "Av. agent reward = 37.09\n",
      "Agents crossed (2nd food pile) = 3.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 370.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 41.9\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 55.5\n",
      "Agent3 reward is 90.6\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 19.2\n",
      "Agent6 reward is 24.5\n",
      "Agent7 reward is 24.8\n",
      "Agent8 reward is 15.7\n",
      "Agent9 reward is 98.8\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 388.3\n",
      "Av. agent reward = 38.83\n",
      "Agents crossed (2nd food pile) = 3.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 388.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 92.2\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 8.6\n",
      "Agent3 reward is 114.6\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 6.6\n",
      "Agent6 reward is 24.7\n",
      "Agent7 reward is 30.6\n",
      "Agent8 reward is 26.1\n",
      "Agent9 reward is 84.9\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 353.5\n",
      "Av. agent reward = 35.35\n",
      "Agents crossed (2nd food pile) = 1.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 353.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.2\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 24.3\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 11.1\n",
      "Agent6 reward is 21.9\n",
      "Agent7 reward is 25.9\n",
      "Agent8 reward is 25.9\n",
      "Agent9 reward is 244.2\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 141.4\n",
      "Av. agent reward = 14.14\n",
      "Agents crossed (2nd food pile) = 0.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 141.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 11.7\n",
      "Agent6 reward is 27.0\n",
      "Agent7 reward is 23.4\n",
      "Agent8 reward is 21.1\n",
      "Agent9 reward is 58.1\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 359.2\n",
      "Av. agent reward = 35.92\n",
      "Agents crossed (2nd food pile) = 1.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 359.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 4.9\n",
      "Agent6 reward is 26.0\n",
      "Agent7 reward is 40.0\n",
      "Agent8 reward is 17.6\n",
      "Agent9 reward is 270.7\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 72.8\n",
      "Av. agent reward = 7.28\n",
      "Agents crossed (2nd food pile) = 1.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 72.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.5\n",
      "Agent1 reward is 4.4\n",
      "Agent2 reward is 1.4\n",
      "Agent3 reward is 1.6\n",
      "Agent4 reward is 3.2\n",
      "Agent5 reward is 3.4\n",
      "Agent6 reward is 1.0\n",
      "Agent7 reward is 21.5\n",
      "Agent8 reward is 28.8\n",
      "Agent9 reward is 6.9\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 311.4\n",
      "Av. agent reward = 31.14\n",
      "Agents crossed (2nd food pile) = 3.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 311.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.2\n",
      "Agent1 reward is 59.5\n",
      "Agent2 reward is 62.3\n",
      "Agent3 reward is 4.1\n",
      "Agent4 reward is 1.7\n",
      "Agent5 reward is 17.3\n",
      "Agent6 reward is 44.1\n",
      "Agent7 reward is 62.2\n",
      "Agent8 reward is 34.0\n",
      "Agent9 reward is 26.0\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 355.4\n",
      "Av. agent reward = 35.54\n",
      "Agents crossed (2nd food pile) = 4.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 355.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 3.7\n",
      "Agent1 reward is 54.0\n",
      "Agent2 reward is 65.8\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 5.0\n",
      "Agent5 reward is 8.2\n",
      "Agent6 reward is 81.8\n",
      "Agent7 reward is 67.9\n",
      "Agent8 reward is 39.0\n",
      "Agent9 reward is 30.1\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 372.4\n",
      "Av. agent reward = 37.24\n",
      "Agents crossed (2nd food pile) = 3.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 372.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.6\n",
      "Agent1 reward is 76.3\n",
      "Agent2 reward is 77.5\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 28.0\n",
      "Agent6 reward is 43.9\n",
      "Agent7 reward is 83.8\n",
      "Agent8 reward is 30.4\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 337.9\n",
      "Av. agent reward = 33.79\n",
      "Agents crossed (2nd food pile) = 2.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 337.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 72.5\n",
      "Agent2 reward is 84.9\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 2.0\n",
      "Agent6 reward is 89.5\n",
      "Agent7 reward is 22.7\n",
      "Agent8 reward is 35.4\n",
      "Agent9 reward is 30.9\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 3000 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 308.0\n",
      "Av. agent reward = 30.80\n",
      "Agents crossed (2nd food pile) = 1.8\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 308.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.9\n",
      "Agent2 reward is 120.1\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 99.9\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 56.0\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_600gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 101.2\n",
      "Av. agent reward = 10.12\n",
      "Agents crossed (2nd food pile) = 1.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 101.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 4.0\n",
      "Agent1 reward is 10.7\n",
      "Agent2 reward is 2.9\n",
      "Agent3 reward is 19.0\n",
      "Agent4 reward is 8.2\n",
      "Agent5 reward is 8.7\n",
      "Agent6 reward is 1.5\n",
      "Agent7 reward is 14.9\n",
      "Agent8 reward is 9.2\n",
      "Agent9 reward is 22.1\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 302.4\n",
      "Av. agent reward = 30.24\n",
      "Agents crossed (2nd food pile) = 2.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 302.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 7.4\n",
      "Agent1 reward is 27.7\n",
      "Agent2 reward is 1.7\n",
      "Agent3 reward is 79.5\n",
      "Agent4 reward is 31.4\n",
      "Agent5 reward is 15.5\n",
      "Agent6 reward is 0.9\n",
      "Agent7 reward is 83.9\n",
      "Agent8 reward is 23.6\n",
      "Agent9 reward is 30.7\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 321.0\n",
      "Av. agent reward = 32.10\n",
      "Agents crossed (2nd food pile) = 2.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 321.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.6\n",
      "Agent1 reward is 64.6\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 124.8\n",
      "Agent4 reward is 36.4\n",
      "Agent5 reward is 4.8\n",
      "Agent6 reward is 9.4\n",
      "Agent7 reward is 16.6\n",
      "Agent8 reward is 29.1\n",
      "Agent9 reward is 33.8\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 368.2\n",
      "Av. agent reward = 36.82\n",
      "Agents crossed (2nd food pile) = 1.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 368.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 8.4\n",
      "Agent1 reward is 135.7\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 139.8\n",
      "Agent4 reward is 2.3\n",
      "Agent5 reward is 1.7\n",
      "Agent6 reward is 21.7\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 33.4\n",
      "Agent9 reward is 25.3\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 335.8\n",
      "Av. agent reward = 33.58\n",
      "Agents crossed (2nd food pile) = 1.8\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 335.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 3.7\n",
      "Agent1 reward is 110.4\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 136.5\n",
      "Agent4 reward is 2.8\n",
      "Agent5 reward is 9.7\n",
      "Agent6 reward is 13.5\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 23.4\n",
      "Agent9 reward is 35.7\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 188.6\n",
      "Av. agent reward = 18.86\n",
      "Agents crossed (2nd food pile) = 0.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 188.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.1\n",
      "Agent1 reward is 67.8\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 57.7\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 30.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_1200gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 89.9\n",
      "Av. agent reward = 8.99\n",
      "Agents crossed (2nd food pile) = 0.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 89.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 2.0\n",
      "Agent1 reward is 0.6\n",
      "Agent2 reward is 18.9\n",
      "Agent3 reward is 6.4\n",
      "Agent4 reward is 3.1\n",
      "Agent5 reward is 17.9\n",
      "Agent6 reward is 2.8\n",
      "Agent7 reward is 7.3\n",
      "Agent8 reward is 3.5\n",
      "Agent9 reward is 27.4\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 155.2\n",
      "Av. agent reward = 15.52\n",
      "Agents crossed (2nd food pile) = 0.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 155.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.6\n",
      "Agent1 reward is 0.9\n",
      "Agent2 reward is 14.3\n",
      "Agent3 reward is 1.1\n",
      "Agent4 reward is 0.3\n",
      "Agent5 reward is 19.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 22.2\n",
      "Agent8 reward is 64.8\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 139.6\n",
      "Av. agent reward = 13.96\n",
      "Agents crossed (2nd food pile) = 0.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 139.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 2.5\n",
      "Agent1 reward is 38.3\n",
      "Agent2 reward is 6.5\n",
      "Agent3 reward is 5.5\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 20.3\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 7.2\n",
      "Agent8 reward is 30.2\n",
      "Agent9 reward is 29.2\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 253.9\n",
      "Av. agent reward = 25.39\n",
      "Agents crossed (2nd food pile) = 2.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 253.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.0\n",
      "Agent1 reward is 56.5\n",
      "Agent2 reward is 47.8\n",
      "Agent3 reward is 4.7\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 24.8\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 54.1\n",
      "Agent8 reward is 34.2\n",
      "Agent9 reward is 30.7\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 106.6\n",
      "Av. agent reward = 10.66\n",
      "Agents crossed (2nd food pile) = 0.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 106.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.8\n",
      "Agent1 reward is 25.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 6.9\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 33.0\n",
      "Agent8 reward is 27.9\n",
      "Agent9 reward is 12.0\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.3\n",
      "Av. agent reward = 9.23\n",
      "Agents crossed (2nd food pile) = 0.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.4\n",
      "Agent1 reward is 4.8\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 16.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 6.2\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 33.0\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 1.0\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 13.1\n",
      "Av. agent reward = 1.31\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 13.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.9\n",
      "Agent1 reward is 1.4\n",
      "Agent2 reward is 0.6\n",
      "Agent3 reward is 1.5\n",
      "Agent4 reward is 0.5\n",
      "Agent5 reward is 1.1\n",
      "Agent6 reward is 1.3\n",
      "Agent7 reward is 2.2\n",
      "Agent8 reward is 2.1\n",
      "Agent9 reward is 1.4\n",
      "Training time per epochs: 2.00 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 152.3\n",
      "Av. agent reward = 15.23\n",
      "Agents crossed (2nd food pile) = 1.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 152.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.5\n",
      "Agent1 reward is 2.8\n",
      "Agent2 reward is 0.9\n",
      "Agent3 reward is 11.0\n",
      "Agent4 reward is 2.3\n",
      "Agent5 reward is 10.3\n",
      "Agent6 reward is 8.4\n",
      "Agent7 reward is 15.4\n",
      "Agent8 reward is 75.3\n",
      "Agent9 reward is 24.5\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 303.2\n",
      "Av. agent reward = 30.32\n",
      "Agents crossed (2nd food pile) = 2.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 303.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.3\n",
      "Agent2 reward is 38.6\n",
      "Agent3 reward is 10.6\n",
      "Agent4 reward is 6.4\n",
      "Agent5 reward is 20.0\n",
      "Agent6 reward is 88.4\n",
      "Agent7 reward is 25.8\n",
      "Agent8 reward is 89.5\n",
      "Agent9 reward is 23.7\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 329.8\n",
      "Av. agent reward = 32.98\n",
      "Agents crossed (2nd food pile) = 3.5\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 329.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 51.7\n",
      "Agent2 reward is 43.9\n",
      "Agent3 reward is 7.2\n",
      "Agent4 reward is 2.5\n",
      "Agent5 reward is 24.0\n",
      "Agent6 reward is 81.1\n",
      "Agent7 reward is 28.3\n",
      "Agent8 reward is 62.6\n",
      "Agent9 reward is 28.4\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 323.5\n",
      "Av. agent reward = 32.35\n",
      "Agents crossed (2nd food pile) = 3.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 323.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 78.2\n",
      "Agent2 reward is 19.3\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.3\n",
      "Agent5 reward is 1.8\n",
      "Agent6 reward is 86.5\n",
      "Agent7 reward is 31.0\n",
      "Agent8 reward is 74.9\n",
      "Agent9 reward is 31.6\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 293.1\n",
      "Av. agent reward = 29.31\n",
      "Agents crossed (2nd food pile) = 2.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 293.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 86.4\n",
      "Agent2 reward is 6.6\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 2.0\n",
      "Agent6 reward is 77.4\n",
      "Agent7 reward is 30.0\n",
      "Agent8 reward is 57.7\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_600gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 57.2\n",
      "Av. agent reward = 5.72\n",
      "Agents crossed (2nd food pile) = 0.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 57.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.2\n",
      "Agent1 reward is 2.0\n",
      "Agent2 reward is 0.6\n",
      "Agent3 reward is 4.9\n",
      "Agent4 reward is 1.5\n",
      "Agent5 reward is 1.7\n",
      "Agent6 reward is 10.7\n",
      "Agent7 reward is 8.6\n",
      "Agent8 reward is 3.1\n",
      "Agent9 reward is 23.9\n",
      "Training time per epochs: 2.04 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 296.4\n",
      "Av. agent reward = 29.64\n",
      "Agents crossed (2nd food pile) = 2.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 296.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.5\n",
      "Agent1 reward is 66.5\n",
      "Agent2 reward is 7.9\n",
      "Agent3 reward is 10.7\n",
      "Agent4 reward is 66.4\n",
      "Agent5 reward is 78.2\n",
      "Agent6 reward is 8.2\n",
      "Agent7 reward is 11.8\n",
      "Agent8 reward is 23.1\n",
      "Agent9 reward is 22.2\n",
      "Training time per epochs: 2.03 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 349.6\n",
      "Av. agent reward = 34.96\n",
      "Agents crossed (2nd food pile) = 3.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 349.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.8\n",
      "Agent1 reward is 78.5\n",
      "Agent2 reward is 0.2\n",
      "Agent3 reward is 8.3\n",
      "Agent4 reward is 109.9\n",
      "Agent5 reward is 65.6\n",
      "Agent6 reward is 9.5\n",
      "Agent7 reward is 20.7\n",
      "Agent8 reward is 28.7\n",
      "Agent9 reward is 27.6\n",
      "Training time per epochs: 2.04 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 332.8\n",
      "Av. agent reward = 33.28\n",
      "Agents crossed (2nd food pile) = 1.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 332.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.1\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 188.1\n",
      "Agent5 reward is 51.7\n",
      "Agent6 reward is 15.8\n",
      "Agent7 reward is 14.3\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 31.8\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 238.5\n",
      "Av. agent reward = 23.85\n",
      "Agents crossed (2nd food pile) = 1.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 238.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 102.8\n",
      "Agent6 reward is 56.7\n",
      "Agent7 reward is 16.3\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 32.6\n",
      "Training time per epochs: 2.02 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 93.0\n",
      "Av. agent reward = 9.30\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 93.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 31.0\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 32.0\n",
      "Training time per epochs: 2.01 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_1200gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 76.5\n",
      "Av. agent reward = 7.65\n",
      "Agents crossed (2nd food pile) = 0.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 76.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 7.9\n",
      "Agent1 reward is 1.2\n",
      "Agent2 reward is 3.3\n",
      "Agent3 reward is 2.0\n",
      "Agent4 reward is 1.0\n",
      "Agent5 reward is 13.2\n",
      "Agent6 reward is 0.3\n",
      "Agent7 reward is 7.5\n",
      "Agent8 reward is 11.9\n",
      "Agent9 reward is 28.2\n",
      "Training time per epochs: 2.07 sec\n",
      "###### Trained episodes = 1000 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 252.3\n",
      "Av. agent reward = 25.23\n",
      "Agents crossed (2nd food pile) = 1.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 252.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 2.0\n",
      "Agent1 reward is 11.3\n",
      "Agent2 reward is 156.2\n",
      "Agent3 reward is 18.3\n",
      "Agent4 reward is 2.3\n",
      "Agent5 reward is 13.8\n",
      "Agent6 reward is 2.1\n",
      "Agent7 reward is 9.6\n",
      "Agent8 reward is 24.6\n",
      "Agent9 reward is 12.2\n",
      "Training time per epochs: 2.14 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 309.0\n",
      "Av. agent reward = 30.90\n",
      "Agents crossed (2nd food pile) = 1.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 309.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 7.0\n",
      "Agent1 reward is 2.2\n",
      "Agent2 reward is 212.0\n",
      "Agent3 reward is 0.7\n",
      "Agent4 reward is 1.0\n",
      "Agent5 reward is 19.9\n",
      "Agent6 reward is 3.2\n",
      "Agent7 reward is 22.1\n",
      "Agent8 reward is 26.0\n",
      "Agent9 reward is 14.9\n",
      "Training time per epochs: 2.11 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 353.6\n",
      "Av. agent reward = 35.36\n",
      "Agents crossed (2nd food pile) = 1.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 353.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 2.3\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 159.3\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 100.2\n",
      "Agent5 reward is 6.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 25.0\n",
      "Agent8 reward is 28.0\n",
      "Agent9 reward is 32.8\n",
      "Training time per epochs: 2.11 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 309.0\n",
      "Av. agent reward = 30.90\n",
      "Agents crossed (2nd food pile) = 1.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 309.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.9\n",
      "Agent2 reward is 149.3\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 66.7\n",
      "Agent5 reward is 26.0\n",
      "Agent6 reward is 3.8\n",
      "Agent7 reward is 30.1\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 32.2\n",
      "Training time per epochs: 2.08 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 137.1\n",
      "Av. agent reward = 13.71\n",
      "Agents crossed (2nd food pile) = 0.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 137.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 44.1\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 30.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 30.0\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 2.09 sec\n",
      "[27.47, 35.36, 36.11, 35.96666666666667, 31.376666666666665, 35.46333333333333]\n",
      "[29.676666666666666, 34.10333333333334, 33.25666666666667, 35.25666666666667, 35.3, 37.160000000000004]\n",
      "[24.61, 34.11666666666667, 35.593333333333334, 35.21, 34.31666666666667, 35.50333333333334]\n",
      "[31.22, 34.946666666666665, 35.626666666666665, 37.08, 36.836666666666666, 27.253333333333337]\n",
      "[17.97, 34.2, 36.53333333333333, 36.473333333333336, 33.71666666666667, 37.50333333333334]\n",
      "[29.526666666666664, 32.986666666666665, 33.056666666666665, 34.93333333333333, 35.626666666666665, 9.3]\n",
      "[27.706666666666667, 29.956666666666667, 31.05, 34.663333333333334, 35.53666666666667, 32.89666666666666]\n",
      "[19.616666666666667, 27.743333333333332, 34.38666666666667, 35.93333333333333, 35.223333333333336, 36.79333333333334]\n",
      "[26.189999999999998, 32.69, 36.086666666666666, 37.28, 36.39333333333333, 35.58]\n",
      "[32.126666666666665, 37.089999999999996, 38.83, 35.35, 14.136666666666667, 35.92]\n",
      "[7.276666666666666, 31.136666666666667, 35.54, 37.24333333333333, 33.79333333333334, 30.796666666666663]\n",
      "[10.120000000000001, 30.243333333333332, 32.1, 36.81666666666667, 33.57666666666667, 18.863333333333333]\n",
      "[8.986666666666666, 15.523333333333332, 13.959999999999999, 25.386666666666667, 10.66, 9.233333333333333]\n",
      "[1.31, 15.23, 30.323333333333334, 32.97666666666667, 32.35333333333334, 29.306666666666665]\n",
      "[5.720000000000001, 29.643333333333334, 34.96333333333333, 33.276666666666664, 23.846666666666668, 9.3]\n",
      "[7.653333333333333, 25.23, 30.9, 35.35666666666667, 30.896666666666665, 13.706666666666667]\n",
      "[6.266666666666667, 6.0, 5.1, 4.8, 4.933333333333334, 4.266666666666667]\n",
      "[3.5, 3.433333333333333, 4.933333333333334, 5.033333333333333, 4.766666666666667, 5.2]\n",
      "[3.7333333333333334, 5.1, 5.266666666666667, 4.533333333333333, 3.3666666666666667, 3.433333333333333]\n",
      "[3.6333333333333333, 4.9, 5.566666666666666, 4.066666666666666, 3.5, 1.1333333333333333]\n",
      "[4.466666666666667, 4.7, 5.066666666666666, 5.6, 2.7333333333333334, 4.466666666666667]\n",
      "[2.3333333333333335, 2.6, 1.0333333333333334, 1.2, 1.0, 0.0]\n",
      "[1.9333333333333333, 2.033333333333333, 1.8333333333333333, 2.933333333333333, 2.7, 1.1]\n",
      "[2.8, 3.6, 3.533333333333333, 4.1, 3.2, 3.433333333333333]\n",
      "[3.4, 3.1333333333333333, 3.0, 2.7666666666666666, 2.0, 2.0]\n",
      "[3.966666666666667, 3.566666666666667, 3.1333333333333333, 1.3333333333333333, 0.4666666666666667, 1.0]\n",
      "[1.1333333333333333, 3.6333333333333333, 4.0, 3.6, 2.7, 1.8333333333333333]\n",
      "[1.2, 2.8666666666666667, 2.433333333333333, 1.9333333333333333, 1.7666666666666666, 0.5666666666666667]\n",
      "[0.1, 0.5333333333333333, 0.4, 2.066666666666667, 0.13333333333333333, 0.16666666666666666]\n",
      "[0.03333333333333333, 1.2666666666666666, 2.7333333333333334, 3.5, 3.2333333333333334, 2.7333333333333334]\n",
      "[0.3, 2.933333333333333, 3.1, 1.7, 1.1, 0.0]\n",
      "[0.36666666666666664, 1.1666666666666667, 1.1333333333333333, 1.9, 1.2666666666666666, 0.16666666666666666]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dir_names = [\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_600gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_1200gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_1200gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_1200gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_1200gs/\"    \n",
    "             ]\n",
    "episodes = [500, 1000, 1500, 2000, 2500, 3000] \n",
    "game = 'Crossing'\n",
    "culture = \"pacifist\"\n",
    "map_name = \"food_d37\"\n",
    "\n",
    "# Performance Statistics - for Research Report\n",
    "av_agent_reward = [[0 for i in episodes] for j in dir_names]\n",
    "av_agent_crossed = [[0 for i in episodes] for j in dir_names]  \n",
    "dominating_tribe = [[None for i in episodes] for j in dir_names]\n",
    "dom_tribe_reward = [[0 for i in episodes] for j in dir_names]\n",
    "dominance = [[0 for i in episodes] for j in dir_names]\n",
    "\n",
    "# There will be 10 agents - 0 teams of 0 AI agents each and 0 random agent\n",
    "num_ai_agents = 10\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Initialize environment\n",
    "render = False\n",
    "SPEED = 1/30\n",
    "river_penalty = -1\n",
    "num_actions = 8                       # There are 8 actions defined in Gathering\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 30\n",
    "max_frames = 500\n",
    "verbose = False\n",
    "\n",
    "\n",
    "for dir_num, dir_name in enumerate(dir_names):\n",
    "    print (\"###### Dir = {} #######\".format(dir_name))\n",
    "    \n",
    "    for eps_num, eps in enumerate(episodes):\n",
    "        print (\"###### Trained episodes = {} #######\".format(eps))\n",
    "    \n",
    "        # Load models for AI agents\n",
    "        agents= [[] for i in range(num_ai_agents)]\n",
    "        # If episodes is provided (not 0), load the model for each AI agent\n",
    "        for i in range(num_ai_agents):\n",
    "            model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,eps)\n",
    "            try:\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    # Model File include both model and optim parameters\n",
    "                    saved_model = pickle.load(f)\n",
    "                    agents[i], _ = saved_model\n",
    "                    # print(\"Load saved model for agent {}\".format(i))\n",
    "            except OSError:\n",
    "                print('Model file not found.')\n",
    "                raise\n",
    "\n",
    "        # Load random agents    \n",
    "        for i in range(num_ai_agents,num_agents):\n",
    "            # print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "        \n",
    "        # Establish tribal association\n",
    "        tribes = []\n",
    "        tribes.append(Tribe(name='Vikings',color='blue', culture=culture, \\\n",
    "                    agents=[agents[0], agents[1], agents[2], agents[3], agents[4], \\\n",
    "                           agents[5], agents[6], agents[7], agents[8], agents[9]]))\n",
    "\n",
    "        # Set up agent and tribe info to pass into env\n",
    "        agent_colors = [agent.color for agent in agents]\n",
    "        agent_tribes = [agent.tribe for agent in agents]\n",
    "        tribe_names = [tribe.name for tribe in tribes]\n",
    "        \n",
    "        env = CrossingEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty, tribes=tribe_names, \\\n",
    "                  debug_agent=0)\n",
    "\n",
    "        # Used to accumulate episode stats for averaging\n",
    "        cum_rewards = 0\n",
    "        cum_crossed = 0\n",
    "        cum_tags = 0\n",
    "        cum_US_hits = 0\n",
    "        cum_THEM_hits = 0\n",
    "        cum_agent_rewards = [0 for agent in agents]\n",
    "        cum_agent_tags = [0 for agent in agents]\n",
    "        cum_agent_US_hits = [0 for agent in agents]\n",
    "        cum_agent_THEM_hits = [0 for agent in agents]\n",
    "        cum_tribe_rewards = [0 for t in tribes if t.name is not 'Crazies']\n",
    "\n",
    "        cuda = False\n",
    "        start = time.time()\n",
    "\n",
    "        for ep in range(max_episodes):\n",
    "    \n",
    "            print('.', end='')  # To show progress\n",
    "    \n",
    "            # Initialize AI and random agent data\n",
    "            actions = [0 for i in range(num_agents)]\n",
    "            tags = [0 for i in range(num_agents)]\n",
    "            US_hits = [0 for i in range(num_agents)]\n",
    "            THEM_hits = [0 for i in range(num_agents)]\n",
    "            \n",
    "            # Keep track of agents gathering from 2nd food pile\n",
    "            crossed = [0 for i in range(num_ai_agents)]\n",
    "\n",
    "            env_obs = env.reset()  # Environment return observations\n",
    "            \"\"\"\n",
    "            # For Debug only\n",
    "            print (len(agents_obs))\n",
    "            print (agents_obs[0].shape)\n",
    "            \"\"\"\n",
    "    \n",
    "            # Unpack observations into data structure compatible with agent Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "            for i in range(num_ai_agents):    # Reset agent info - laser tag statistics\n",
    "                agents[i].reset_info()    \n",
    "    \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "            \"\"\"\n",
    "            # For Debug only\n",
    "            print (len(agents_obs))\n",
    "            print (agents_obs[0].shape)\n",
    "            \"\"\"\n",
    "    \n",
    "            \"\"\"\n",
    "            For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "            state = np.stack([state]*num_frames)\n",
    "\n",
    "            # Reset LSTM hidden units when episode begins\n",
    "            cx = Variable(torch.zeros(1, 256))\n",
    "            hx = Variable(torch.zeros(1, 256))\n",
    "            \"\"\"\n",
    "\n",
    "            for frame in range(max_frames):\n",
    "\n",
    "                for i in range(num_ai_agents):    # For AI agents\n",
    "                    actions[i], _ = select_action(agents[i], agents_obs[i], cuda=cuda)\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "                for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "                    actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                    if actions[i] is 6:\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "                \"\"\"\n",
    "                For now, we do not implement LSTM\n",
    "                # Select action\n",
    "                action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "                \"\"\"\n",
    "\n",
    "                # if frame % 10 == 0:\n",
    "                #     print (actions)    \n",
    "            \n",
    "                # Perform step        \n",
    "                env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "                \"\"\"\n",
    "                For Debug only\n",
    "                print (env_obs)\n",
    "                print (reward)\n",
    "                print (done) \n",
    "                \"\"\"\n",
    "\n",
    "                for i in range(num_ai_agents):\n",
    "                    agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "                # Unpack observations into data structure compatible with agent Policy\n",
    "                agents_obs = unpack_env_obs(env_obs)\n",
    "                load_info(agents, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "                for i in range(num_agents):\n",
    "                    US_hits[i] += agents[i].US_hit\n",
    "                    THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "                \"\"\"\n",
    "                For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "                # Evict oldest diff add new diff to state\n",
    "                next_state = np.stack([next_state]*num_frames)\n",
    "                next_state[1:, :, :] = state[:-1, :, :]\n",
    "                state = next_state\n",
    "                \"\"\"\n",
    "        \n",
    "                if render and ep is 0: \n",
    "                    env.render()\n",
    "                    time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "                if any(done):\n",
    "                    print(\"Done after {} frames\".format(frame))\n",
    "                    break\n",
    "                    \n",
    "                for (i, loc) in env.consumption:\n",
    "                    if loc[0] > second_pile_x:\n",
    "                        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "                        crossed[i] = 1\n",
    "            \n",
    "            # Print out statistics of AI agents\n",
    "            ep_rewards = 0\n",
    "            ep_tags = 0\n",
    "            ep_US_hits = 0\n",
    "            ep_THEM_hits = 0\n",
    "            ep_crossed = sum(crossed)     # calculated num agents gathering in 2nd pile for episode\n",
    "\n",
    "            if verbose:\n",
    "                print ('\\nStatistics by Agent')\n",
    "                print ('===================')\n",
    "            for i in range(num_ai_agents):\n",
    "                agent_tags = sum(agents[i].tag_hist)\n",
    "                ep_tags += agent_tags\n",
    "                cum_agent_tags[i] += agent_tags\n",
    "\n",
    "                agent_reward = sum(agents[i].rewards)\n",
    "                ep_rewards += agent_reward\n",
    "                cum_agent_rewards[i] += agent_reward\n",
    "\n",
    "                agent_US_hits = sum(agents[i].US_hits)\n",
    "                agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "                ep_US_hits += agent_US_hits\n",
    "                ep_THEM_hits += agent_THEM_hits\n",
    "                cum_agent_US_hits[i] += agent_US_hits\n",
    "                cum_agent_THEM_hits[i] += agent_THEM_hits\n",
    "        \n",
    "                if verbose:\n",
    "                    print (\"Agent{} aggressiveness is {:.2f}\".format(i, agent_tags/frame))\n",
    "                    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "                    # print('US agents hit = {}'.format(agent_US_hits))\n",
    "                    # print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "        \n",
    "            cum_rewards += ep_rewards\n",
    "            cum_crossed += ep_crossed\n",
    "            cum_tags += ep_tags\n",
    "            cum_US_hits += ep_US_hits\n",
    "            cum_THEM_hits += ep_THEM_hits\n",
    "    \n",
    "            if verbose:\n",
    "                print ('\\nStatistics in Aggregate')\n",
    "                print ('=======================')\n",
    "                print ('Total rewards gathered = {}'.format(ep_rewards))\n",
    "                print ('Num agents crossed = {}'.format(ep_crossed))\n",
    "                # print ('Num laser fired = {}'.format(ep_tags))\n",
    "                # print ('Total US Hit (friendly fire) = {}'.format(ep_US_hits))\n",
    "                # print ('Total THEM Hit = {}'.format(ep_THEM_hits))\n",
    "                # print ('friendly fire (%) = {0:.3f}'.format(ep_US_hits/(ep_US_hits+ep_THEM_hits+1e-7)))\n",
    "\n",
    "            if verbose:\n",
    "                print ('\\nStatistics by Tribe')\n",
    "                print ('===================')\n",
    "            for i, t in enumerate(tribes):\n",
    "                if t.name is not 'Crazies':\n",
    "                    ep_tribe_reward = sum(t.sum_rewards())\n",
    "                    cum_tribe_rewards[i] += ep_tribe_reward\n",
    "                    if verbose:\n",
    "                        print ('Tribe {} has total reward of {}'.format(t.name, ep_tribe_reward))\n",
    "\n",
    "            for i in range(num_ai_agents):\n",
    "                agents[i].clear_history()\n",
    "\n",
    "        env.close()  # Close the rendering window\n",
    "        end = time.time()\n",
    "\n",
    "        print ('\\nAverage Statistics in Aggregate')\n",
    "        print ('=================================')\n",
    "        total_rewards = cum_rewards/max_episodes\n",
    "        print ('Total rewards gathered = {:.1f}'.format(total_rewards))\n",
    "        av_agent_reward[dir_num][eps_num] = cum_rewards/max_episodes/num_ai_agents\n",
    "        print ('Av. agent reward = {:.2f}'.format(av_agent_reward[dir_num][eps_num]))\n",
    "        av_agent_crossed[dir_num][eps_num] = cum_crossed/max_episodes\n",
    "        print ('Agents crossed (2nd food pile) = {:.1f}'.format(av_agent_crossed[dir_num][eps_num]))\n",
    "        # print ('Num laser fired = {:.1f}'.format(cum_tags/max_episodes))\n",
    "        # print ('Total US Hit (friendly fire) = {:.1f}'.format(cum_US_hits/max_episodes))\n",
    "        # print ('Total THEM Hit = {:.1f}'.format(cum_THEM_hits/max_episodes))\n",
    "        # print ('friendly fire (%) = {:.3f}'.format(cum_US_hits/(cum_US_hits+cum_THEM_hits+1e-7)))\n",
    "\n",
    "        print ('\\nAverage Statistics by Tribe')\n",
    "        print ('=============================')\n",
    "       \n",
    "        for i, tribe in enumerate(tribes):\n",
    "            if tribe.name is not 'Crazies':\n",
    "                tribe_reward = cum_tribe_rewards[i]/max_episodes\n",
    "                print ('Tribe {} has total reward of {:.1f}'.format(tribe.name, tribe_reward))    \n",
    "                \n",
    "                # Keep track of dominating team and the rewards gathered (only if more than 1 tribe)\n",
    "                if len(tribes) > 1:\n",
    "                    if tribe_reward > dom_tribe_reward[dir_num][eps_num]:   \n",
    "                        dom_tribe_reward[dir_num][eps_num] = tribe_reward\n",
    "                        dominating_tribe[dir_num][eps_num]  = tribe.name\n",
    "\n",
    "        # Team dominance calculation (only if more than 1 tribe)\n",
    "        if len(tribes) > 1:\n",
    "            print ('Dominating Tribe: {}'.format(dominating_tribe[dir_num][eps_num]))\n",
    "            dominance[dir_num][eps_num] = dom_tribe_reward[dir_num][eps_num]/((total_rewards - \\\n",
    "                                                dom_tribe_reward[dir_num][eps_num]+1.1e-7)/(len(tribes)-1))    \n",
    "            print ('Team dominance: {0:.2f}x'.format(dominance[dir_num][eps_num]))\n",
    "\n",
    "        print ('\\nAverage Statistics by Agent')\n",
    "        print ('=============================')\n",
    "        for i in range(num_ai_agents):\n",
    "            # print (\"Agent{} of {} aggressiveness is {:.2f}\".format(i, agents[i].tribe, \\\n",
    "            #                                               cum_agent_tags[i]/(max_episodes*max_frames)))\n",
    "            print (\"Agent{} reward is {:.1f}\".format(i, cum_agent_rewards[i]/max_episodes))\n",
    "            # print('US agents hit = {:.1f}'.format(cum_agent_US_hits[i]/max_episodes))\n",
    "            # print('THEM agents hit = {:.1f}'.format(cum_agent_THEM_hits[i]/max_episodes))\n",
    "\n",
    "        print('Training time per epochs: {:.2f} sec'.format((end-start)/max_episodes))\n",
    "\n",
    "# Note: Statistics for Research Report        \n",
    "for reward in av_agent_reward:   # Average agent reward\n",
    "    print(reward)\n",
    "for agents_crossed in av_agent_crossed:   # Average num agents gathering in 2nd food pile\n",
    "    print(agents_crossed)\n",
    "\n",
    "# print dominating team and dominance factor (only if more than 1 tribe)\n",
    "if len(tribes) > 1:\n",
    "    for tribe in dominating_tribe:   # Dominating team\n",
    "        print(tribe)\n",
    "    for value in dominance:      # Team dominance\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for Research Report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Agent Rewards\n",
      "[27.47, 35.36, 36.11, 35.96666666666667, 31.376666666666665, 35.46333333333333]\n",
      "[29.676666666666666, 34.10333333333334, 33.25666666666667, 35.25666666666667, 35.3, 37.160000000000004]\n",
      "[24.61, 34.11666666666667, 35.593333333333334, 35.21, 34.31666666666667, 35.50333333333334]\n",
      "[31.22, 34.946666666666665, 35.626666666666665, 37.08, 36.836666666666666, 27.253333333333337]\n",
      "[17.97, 34.2, 36.53333333333333, 36.473333333333336, 33.71666666666667, 37.50333333333334]\n",
      "[29.526666666666664, 32.986666666666665, 33.056666666666665, 34.93333333333333, 35.626666666666665, 9.3]\n",
      "[27.706666666666667, 29.956666666666667, 31.05, 34.663333333333334, 35.53666666666667, 32.89666666666666]\n",
      "[19.616666666666667, 27.743333333333332, 34.38666666666667, 35.93333333333333, 35.223333333333336, 36.79333333333334]\n",
      "[26.189999999999998, 32.69, 36.086666666666666, 37.28, 36.39333333333333, 35.58]\n",
      "[32.126666666666665, 37.089999999999996, 38.83, 35.35, 14.136666666666667, 35.92]\n",
      "[7.276666666666666, 31.136666666666667, 35.54, 37.24333333333333, 33.79333333333334, 30.796666666666663]\n",
      "[10.120000000000001, 30.243333333333332, 32.1, 36.81666666666667, 33.57666666666667, 18.863333333333333]\n",
      "[8.986666666666666, 15.523333333333332, 13.959999999999999, 25.386666666666667, 10.66, 9.233333333333333]\n",
      "[1.31, 15.23, 30.323333333333334, 32.97666666666667, 32.35333333333334, 29.306666666666665]\n",
      "[5.720000000000001, 29.643333333333334, 34.96333333333333, 33.276666666666664, 23.846666666666668, 9.3]\n",
      "[7.653333333333333, 25.23, 30.9, 35.35666666666667, 30.896666666666665, 13.706666666666667]\n",
      "Agents Crossed (2nd food pile)\n",
      "[6.266666666666667, 6.0, 5.1, 4.8, 4.933333333333334, 4.266666666666667]\n",
      "[3.5, 3.433333333333333, 4.933333333333334, 5.033333333333333, 4.766666666666667, 5.2]\n",
      "[3.7333333333333334, 5.1, 5.266666666666667, 4.533333333333333, 3.3666666666666667, 3.433333333333333]\n",
      "[3.6333333333333333, 4.9, 5.566666666666666, 4.066666666666666, 3.5, 1.1333333333333333]\n",
      "[4.466666666666667, 4.7, 5.066666666666666, 5.6, 2.7333333333333334, 4.466666666666667]\n",
      "[2.3333333333333335, 2.6, 1.0333333333333334, 1.2, 1.0, 0.0]\n",
      "[1.9333333333333333, 2.033333333333333, 1.8333333333333333, 2.933333333333333, 2.7, 1.1]\n",
      "[2.8, 3.6, 3.533333333333333, 4.1, 3.2, 3.433333333333333]\n",
      "[3.4, 3.1333333333333333, 3.0, 2.7666666666666666, 2.0, 2.0]\n",
      "[3.966666666666667, 3.566666666666667, 3.1333333333333333, 1.3333333333333333, 0.4666666666666667, 1.0]\n",
      "[1.1333333333333333, 3.6333333333333333, 4.0, 3.6, 2.7, 1.8333333333333333]\n",
      "[1.2, 2.8666666666666667, 2.433333333333333, 1.9333333333333333, 1.7666666666666666, 0.5666666666666667]\n",
      "[0.1, 0.5333333333333333, 0.4, 2.066666666666667, 0.13333333333333333, 0.16666666666666666]\n",
      "[0.03333333333333333, 1.2666666666666666, 2.7333333333333334, 3.5, 3.2333333333333334, 2.7333333333333334]\n",
      "[0.3, 2.933333333333333, 3.1, 1.7, 1.1, 0.0]\n",
      "[0.36666666666666664, 1.1666666666666667, 1.1333333333333333, 1.9, 1.2666666666666666, 0.16666666666666666]\n"
     ]
    }
   ],
   "source": [
    "# Note: Statistics for Research Report   \n",
    "print ('Average Agent Rewards')\n",
    "for reward in av_agent_reward:   # Average agent reward\n",
    "    print(reward)\n",
    "    \n",
    "print ('Agents Crossed (2nd food pile)')    \n",
    "for agents_crossed in av_agent_crossed:   # Average num agents gathering in 2nd food pile\n",
    "    print(agents_crossed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Stats - Baseline (Map = food_d37_river_w1_d25)\n",
    "\n",
    "Our research requires gathering game stats for agents and teams over 30 episodes of game play:\n",
    "\n",
    "* Average agent reward - average number of apples gathered per agent per episode  \n",
    "* The number of agents gathering apples at the 2nd food pile \n",
    "\n",
    "Rendering is disabled to speed things up.\n",
    "\n",
    "<img src=\"images/Crossing-river.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Dir = models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 80.8\n",
      "Av. agent reward = 8.08\n",
      "Agents crossed (2nd food pile) = 0.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 80.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.8\n",
      "Agent1 reward is 0.5\n",
      "Agent2 reward is 0.1\n",
      "Agent3 reward is 24.3\n",
      "Agent4 reward is 2.8\n",
      "Agent5 reward is 0.2\n",
      "Agent6 reward is 0.8\n",
      "Agent7 reward is 21.1\n",
      "Agent8 reward is 26.6\n",
      "Agent9 reward is 3.6\n",
      "Training time per epochs: 4.26 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 98.1\n",
      "Av. agent reward = 9.81\n",
      "Agents crossed (2nd food pile) = 0.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 98.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is -0.7\n",
      "Agent3 reward is 14.9\n",
      "Agent4 reward is 21.6\n",
      "Agent5 reward is 0.2\n",
      "Agent6 reward is 11.3\n",
      "Agent7 reward is 21.5\n",
      "Agent8 reward is 2.3\n",
      "Agent9 reward is 27.0\n",
      "Training time per epochs: 4.08 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 91.1\n",
      "Av. agent reward = 9.11\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 91.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 8.1\n",
      "Agent4 reward is 12.3\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 8.5\n",
      "Agent7 reward is 26.5\n",
      "Agent8 reward is 4.7\n",
      "Agent9 reward is 30.9\n",
      "Training time per epochs: 4.40 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 91.5\n",
      "Av. agent reward = 9.15\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 91.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 28.9\n",
      "Agent4 reward is 22.2\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 8.1\n",
      "Agent8 reward is 1.3\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 4.26 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.6\n",
      "Av. agent reward = 9.26\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 15.6\n",
      "Agent4 reward is 11.3\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 27.6\n",
      "Agent8 reward is 7.0\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 4.48 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.0\n",
      "Av. agent reward = 9.20\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 22.8\n",
      "Agent4 reward is 19.2\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 17.9\n",
      "Agent8 reward is 1.2\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 3.95 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 89.7\n",
      "Av. agent reward = 8.97\n",
      "Agents crossed (2nd food pile) = 1.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 89.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.5\n",
      "Agent1 reward is 16.4\n",
      "Agent2 reward is 19.6\n",
      "Agent3 reward is 29.3\n",
      "Agent4 reward is -1.3\n",
      "Agent5 reward is 7.7\n",
      "Agent6 reward is 4.7\n",
      "Agent7 reward is 3.0\n",
      "Agent8 reward is 0.9\n",
      "Agent9 reward is 7.9\n",
      "Training time per epochs: 4.71 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 151.0\n",
      "Av. agent reward = 15.10\n",
      "Agents crossed (2nd food pile) = 0.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 151.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is -0.4\n",
      "Agent1 reward is 9.8\n",
      "Agent2 reward is 14.7\n",
      "Agent3 reward is 15.7\n",
      "Agent4 reward is 1.5\n",
      "Agent5 reward is 8.8\n",
      "Agent6 reward is 8.9\n",
      "Agent7 reward is 55.8\n",
      "Agent8 reward is 16.4\n",
      "Agent9 reward is 19.8\n",
      "Training time per epochs: 4.63 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 120.3\n",
      "Av. agent reward = 12.03\n",
      "Agents crossed (2nd food pile) = 0.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 120.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 1.1\n",
      "Agent2 reward is 1.7\n",
      "Agent3 reward is 24.5\n",
      "Agent4 reward is 0.3\n",
      "Agent5 reward is 3.4\n",
      "Agent6 reward is 0.9\n",
      "Agent7 reward is 29.6\n",
      "Agent8 reward is 30.1\n",
      "Agent9 reward is 28.7\n",
      "Training time per epochs: 4.59 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 139.7\n",
      "Av. agent reward = 13.97\n",
      "Agents crossed (2nd food pile) = 1.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 139.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.4\n",
      "Agent1 reward is 44.3\n",
      "Agent2 reward is 19.1\n",
      "Agent3 reward is 29.8\n",
      "Agent4 reward is 0.1\n",
      "Agent5 reward is 0.9\n",
      "Agent6 reward is 1.0\n",
      "Agent7 reward is 2.7\n",
      "Agent8 reward is 29.6\n",
      "Agent9 reward is 11.8\n",
      "Training time per epochs: 4.67 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 212.7\n",
      "Av. agent reward = 21.27\n",
      "Agents crossed (2nd food pile) = 1.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 212.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 122.4\n",
      "Agent2 reward is 29.2\n",
      "Agent3 reward is 29.1\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 17.7\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is -1.1\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 15.3\n",
      "Training time per epochs: 4.30 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 251.8\n",
      "Av. agent reward = 25.18\n",
      "Agents crossed (2nd food pile) = 1.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 251.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 9.3\n",
      "Agent1 reward is 162.4\n",
      "Agent2 reward is 24.7\n",
      "Agent3 reward is 22.4\n",
      "Agent4 reward is 1.8\n",
      "Agent5 reward is 21.3\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 2.6\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 7.4\n",
      "Training time per epochs: 4.94 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 66.0\n",
      "Av. agent reward = 6.60\n",
      "Agents crossed (2nd food pile) = 0.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 66.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 2.2\n",
      "Agent1 reward is 2.2\n",
      "Agent2 reward is 0.4\n",
      "Agent3 reward is 32.5\n",
      "Agent4 reward is 0.7\n",
      "Agent5 reward is -0.6\n",
      "Agent6 reward is 15.2\n",
      "Agent7 reward is -2.1\n",
      "Agent8 reward is 10.7\n",
      "Agent9 reward is 4.9\n",
      "Training time per epochs: 4.41 sec\n",
      "###### Trained episodes = 1000 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 121.2\n",
      "Av. agent reward = 12.12\n",
      "Agents crossed (2nd food pile) = 0.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 121.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 2.0\n",
      "Agent1 reward is 2.8\n",
      "Agent2 reward is 6.9\n",
      "Agent3 reward is 13.6\n",
      "Agent4 reward is 33.1\n",
      "Agent5 reward is 2.5\n",
      "Agent6 reward is 20.6\n",
      "Agent7 reward is -0.2\n",
      "Agent8 reward is 15.4\n",
      "Agent9 reward is 24.5\n",
      "Training time per epochs: 4.22 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 132.5\n",
      "Av. agent reward = 13.25\n",
      "Agents crossed (2nd food pile) = 0.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 132.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 4.8\n",
      "Agent1 reward is 0.1\n",
      "Agent2 reward is 0.1\n",
      "Agent3 reward is 5.0\n",
      "Agent4 reward is 8.5\n",
      "Agent5 reward is 1.6\n",
      "Agent6 reward is 62.2\n",
      "Agent7 reward is -1.0\n",
      "Agent8 reward is 18.7\n",
      "Agent9 reward is 32.5\n",
      "Training time per epochs: 4.05 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 107.2\n",
      "Av. agent reward = 10.72\n",
      "Agents crossed (2nd food pile) = 0.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 107.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.8\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.9\n",
      "Agent4 reward is 3.0\n",
      "Agent5 reward is 4.2\n",
      "Agent6 reward is 38.3\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 27.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 4.16 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 107.7\n",
      "Av. agent reward = 10.77\n",
      "Agents crossed (2nd food pile) = 0.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 107.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.9\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 1.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 9.2\n",
      "Agent6 reward is 34.7\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 29.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 4.16 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 93.0\n",
      "Av. agent reward = 9.30\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 93.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 30.0\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 4.01 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 48.2\n",
      "Av. agent reward = 4.82\n",
      "Agents crossed (2nd food pile) = 0.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 48.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is -0.1\n",
      "Agent1 reward is 0.1\n",
      "Agent2 reward is -0.8\n",
      "Agent3 reward is -2.8\n",
      "Agent4 reward is -0.1\n",
      "Agent5 reward is 7.6\n",
      "Agent6 reward is 2.7\n",
      "Agent7 reward is 10.8\n",
      "Agent8 reward is 7.0\n",
      "Agent9 reward is 24.0\n",
      "Training time per epochs: 4.08 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 88.5\n",
      "Av. agent reward = 8.85\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 88.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is -1.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.6\n",
      "Agent3 reward is -0.5\n",
      "Agent4 reward is 4.5\n",
      "Agent5 reward is 8.7\n",
      "Agent6 reward is 21.5\n",
      "Agent7 reward is 19.2\n",
      "Agent8 reward is 11.7\n",
      "Agent9 reward is 23.8\n",
      "Training time per epochs: 3.81 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.7\n",
      "Av. agent reward = 9.27\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is -0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 5.9\n",
      "Agent5 reward is 1.9\n",
      "Agent6 reward is 23.2\n",
      "Agent7 reward is 28.1\n",
      "Agent8 reward is 19.4\n",
      "Agent9 reward is 14.3\n",
      "Training time per epochs: 3.85 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.9\n",
      "Av. agent reward = 9.29\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 2.0\n",
      "Agent5 reward is 5.9\n",
      "Agent6 reward is 18.3\n",
      "Agent7 reward is 28.1\n",
      "Agent8 reward is 31.6\n",
      "Agent9 reward is 7.0\n",
      "Training time per epochs: 3.66 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 91.1\n",
      "Av. agent reward = 9.11\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 91.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.1\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 25.2\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 2.9\n",
      "Agent7 reward is 6.3\n",
      "Agent8 reward is 26.8\n",
      "Agent9 reward is 29.9\n",
      "Training time per epochs: 4.12 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.0\n",
      "Av. agent reward = 9.20\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 30.0\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 31.0\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 4.14 sec\n",
      "[8.083333333333332, 9.809999999999999, 9.106666666666666, 9.15, 9.256666666666666, 9.203333333333333]\n",
      "[8.973333333333333, 15.1, 12.033333333333333, 13.969999999999999, 21.27, 25.18]\n",
      "[6.6, 12.123333333333333, 13.246666666666666, 10.723333333333333, 10.77, 9.3]\n",
      "[4.82, 8.85, 9.273333333333333, 9.290000000000001, 9.113333333333333, 9.196666666666667]\n",
      "[0.06666666666666667, 0.2, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.6, 0.9333333333333333, 0.4, 1.1, 1.0, 1.2]\n",
      "[0.2, 0.5666666666666667, 0.4, 0.23333333333333334, 0.13333333333333333, 0.0]\n",
      "[0.43333333333333335, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dir_names = [\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/\",   # scenario=18\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/\",   # scenario=19 \n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/\",   # scenario=20\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/\"   # scenario=21   \n",
    "]  \n",
    "             \n",
    "episodes = [500, 1000, 1500, 2000, 2500, 3000] \n",
    "game = 'Crossing'\n",
    "culture = \"pacifist\"\n",
    "map_name = \"food_d37_river_w1_d25\"\n",
    "\n",
    "# Performance Statistics - for Research Report\n",
    "av_agent_reward = [[0 for i in episodes] for j in dir_names]\n",
    "av_agent_crossed = [[0 for i in episodes] for j in dir_names]  \n",
    "dominating_tribe = [[None for i in episodes] for j in dir_names]\n",
    "dom_tribe_reward = [[0 for i in episodes] for j in dir_names]\n",
    "dominance = [[0 for i in episodes] for j in dir_names]\n",
    "\n",
    "# There will be 10 agents - 0 teams of 0 AI agents each and 0 random agent\n",
    "num_ai_agents = 10\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "river_penalty = -1\n",
    "num_actions = 8                       # There are 8 actions defined in Gathering\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 30\n",
    "max_frames = 500\n",
    "verbose = False\n",
    "\n",
    "\n",
    "for dir_num, dir_name in enumerate(dir_names):\n",
    "    print (\"###### Dir = {} #######\".format(dir_name))\n",
    "    \n",
    "    for eps_num, eps in enumerate(episodes):\n",
    "        print (\"###### Trained episodes = {} #######\".format(eps))\n",
    "    \n",
    "        # Load models for AI agents\n",
    "        agents= [[] for i in range(num_ai_agents)]\n",
    "        # If episodes is provided (not 0), load the model for each AI agent\n",
    "        for i in range(num_ai_agents):\n",
    "            model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,eps)\n",
    "            try:\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    # Model File include both model and optim parameters\n",
    "                    saved_model = pickle.load(f)\n",
    "                    agents[i], _ = saved_model\n",
    "                    # print(\"Load saved model for agent {}\".format(i))\n",
    "            except OSError:\n",
    "                print('Model file not found.')\n",
    "                raise\n",
    "\n",
    "        # Load random agents    \n",
    "        for i in range(num_ai_agents,num_agents):\n",
    "            # print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "        \n",
    "        # Establish tribal association\n",
    "        tribes = []\n",
    "        tribes.append(Tribe(name='Vikings',color='blue', culture=culture, \\\n",
    "                    agents=[agents[0], agents[1], agents[2], agents[3], agents[4], \\\n",
    "                           agents[5], agents[6], agents[7], agents[8], agents[9]]))\n",
    "\n",
    "        # Set up agent and tribe info to pass into env\n",
    "        agent_colors = [agent.color for agent in agents]\n",
    "        agent_tribes = [agent.tribe for agent in agents]\n",
    "        tribe_names = [tribe.name for tribe in tribes]\n",
    "        \n",
    "        env = CrossingEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty, tribes=tribe_names, \\\n",
    "                  debug_agent=0)\n",
    "\n",
    "        # Used to accumulate episode stats for averaging\n",
    "        cum_rewards = 0\n",
    "        cum_crossed = 0\n",
    "        cum_tags = 0\n",
    "        cum_US_hits = 0\n",
    "        cum_THEM_hits = 0\n",
    "        cum_agent_rewards = [0 for agent in agents]\n",
    "        cum_agent_tags = [0 for agent in agents]\n",
    "        cum_agent_US_hits = [0 for agent in agents]\n",
    "        cum_agent_THEM_hits = [0 for agent in agents]\n",
    "        cum_tribe_rewards = [0 for t in tribes if t.name is not 'Crazies']\n",
    "\n",
    "        cuda = False\n",
    "        start = time.time()\n",
    "\n",
    "        for ep in range(max_episodes):\n",
    "    \n",
    "            print('.', end='')  # To show progress\n",
    "    \n",
    "            # Initialize AI and random agent data\n",
    "            actions = [0 for i in range(num_agents)]\n",
    "            tags = [0 for i in range(num_agents)]\n",
    "            US_hits = [0 for i in range(num_agents)]\n",
    "            THEM_hits = [0 for i in range(num_agents)]\n",
    "            \n",
    "            # Keep track of agents gathering from 2nd food pile\n",
    "            crossed = [0 for i in range(num_ai_agents)]\n",
    "\n",
    "            env_obs = env.reset()  # Environment return observations\n",
    "            \"\"\"\n",
    "            # For Debug only\n",
    "            print (len(agents_obs))\n",
    "            print (agents_obs[0].shape)\n",
    "            \"\"\"\n",
    "    \n",
    "            # Unpack observations into data structure compatible with agent Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "            for i in range(num_ai_agents):    # Reset agent info - laser tag statistics\n",
    "                agents[i].reset_info()    \n",
    "    \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "            \"\"\"\n",
    "            # For Debug only\n",
    "            print (len(agents_obs))\n",
    "            print (agents_obs[0].shape)\n",
    "            \"\"\"\n",
    "    \n",
    "            \"\"\"\n",
    "            For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "            state = np.stack([state]*num_frames)\n",
    "\n",
    "            # Reset LSTM hidden units when episode begins\n",
    "            cx = Variable(torch.zeros(1, 256))\n",
    "            hx = Variable(torch.zeros(1, 256))\n",
    "            \"\"\"\n",
    "\n",
    "            for frame in range(max_frames):\n",
    "\n",
    "                for i in range(num_ai_agents):    # For AI agents\n",
    "                    actions[i], _ = select_action(agents[i], agents_obs[i], cuda=cuda)\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "                for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "                    actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                    if actions[i] is 6:\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "                \"\"\"\n",
    "                For now, we do not implement LSTM\n",
    "                # Select action\n",
    "                action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "                \"\"\"\n",
    "\n",
    "                # if frame % 10 == 0:\n",
    "                #     print (actions)    \n",
    "            \n",
    "                # Perform step        \n",
    "                env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "                \"\"\"\n",
    "                For Debug only\n",
    "                print (env_obs)\n",
    "                print (reward)\n",
    "                print (done) \n",
    "                \"\"\"\n",
    "\n",
    "                for i in range(num_ai_agents):\n",
    "                    agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "                # Unpack observations into data structure compatible with agent Policy\n",
    "                agents_obs = unpack_env_obs(env_obs)\n",
    "                load_info(agents, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "                for i in range(num_agents):\n",
    "                    US_hits[i] += agents[i].US_hit\n",
    "                    THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "                \"\"\"\n",
    "                For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "                # Evict oldest diff add new diff to state\n",
    "                next_state = np.stack([next_state]*num_frames)\n",
    "                next_state[1:, :, :] = state[:-1, :, :]\n",
    "                state = next_state\n",
    "                \"\"\"\n",
    "                        \n",
    "                if render and ep is 0:   # render only the 1st episode per batch of 30\n",
    "                    env.render()\n",
    "                    time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "                if any(done):\n",
    "                    print(\"Done after {} frames\".format(frame))\n",
    "                    break\n",
    "                    \n",
    "                for (i, loc) in env.consumption:\n",
    "                    if loc[0] > second_pile_x:\n",
    "                        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "                        crossed[i] = 1\n",
    "            \n",
    "            # Print out statistics of AI agents\n",
    "            ep_rewards = 0\n",
    "            ep_tags = 0\n",
    "            ep_US_hits = 0\n",
    "            ep_THEM_hits = 0\n",
    "            ep_crossed = sum(crossed)     # calculated num agents gathering in 2nd pile for episode\n",
    "\n",
    "            if verbose:\n",
    "                print ('\\nStatistics by Agent')\n",
    "                print ('===================')\n",
    "            for i in range(num_ai_agents):\n",
    "                agent_tags = sum(agents[i].tag_hist)\n",
    "                ep_tags += agent_tags\n",
    "                cum_agent_tags[i] += agent_tags\n",
    "\n",
    "                agent_reward = sum(agents[i].rewards)\n",
    "                ep_rewards += agent_reward\n",
    "                cum_agent_rewards[i] += agent_reward\n",
    "\n",
    "                agent_US_hits = sum(agents[i].US_hits)\n",
    "                agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "                ep_US_hits += agent_US_hits\n",
    "                ep_THEM_hits += agent_THEM_hits\n",
    "                cum_agent_US_hits[i] += agent_US_hits\n",
    "                cum_agent_THEM_hits[i] += agent_THEM_hits\n",
    "        \n",
    "                if verbose:\n",
    "                    print (\"Agent{} aggressiveness is {:.2f}\".format(i, agent_tags/frame))\n",
    "                    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "                    # print('US agents hit = {}'.format(agent_US_hits))\n",
    "                    # print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "        \n",
    "            cum_rewards += ep_rewards\n",
    "            cum_crossed += ep_crossed\n",
    "            cum_tags += ep_tags\n",
    "            cum_US_hits += ep_US_hits\n",
    "            cum_THEM_hits += ep_THEM_hits\n",
    "    \n",
    "            if verbose:\n",
    "                print ('\\nStatistics in Aggregate')\n",
    "                print ('=======================')\n",
    "                print ('Total rewards gathered = {}'.format(ep_rewards))\n",
    "                print ('Num agents crossed = {}'.format(ep_crossed))\n",
    "                # print ('Num laser fired = {}'.format(ep_tags))\n",
    "                # print ('Total US Hit (friendly fire) = {}'.format(ep_US_hits))\n",
    "                # print ('Total THEM Hit = {}'.format(ep_THEM_hits))\n",
    "                # print ('friendly fire (%) = {0:.3f}'.format(ep_US_hits/(ep_US_hits+ep_THEM_hits+1e-7)))\n",
    "\n",
    "            if verbose:\n",
    "                print ('\\nStatistics by Tribe')\n",
    "                print ('===================')\n",
    "            for i, t in enumerate(tribes):\n",
    "                if t.name is not 'Crazies':\n",
    "                    ep_tribe_reward = sum(t.sum_rewards())\n",
    "                    cum_tribe_rewards[i] += ep_tribe_reward\n",
    "                    if verbose:\n",
    "                        print ('Tribe {} has total reward of {}'.format(t.name, ep_tribe_reward))\n",
    "\n",
    "            for i in range(num_ai_agents):\n",
    "                agents[i].clear_history()\n",
    "\n",
    "        env.close()  # Close the rendering window\n",
    "        end = time.time()\n",
    "\n",
    "        print ('\\nAverage Statistics in Aggregate')\n",
    "        print ('=================================')\n",
    "        total_rewards = cum_rewards/max_episodes\n",
    "        print ('Total rewards gathered = {:.1f}'.format(total_rewards))\n",
    "        av_agent_reward[dir_num][eps_num] = cum_rewards/max_episodes/num_ai_agents\n",
    "        print ('Av. agent reward = {:.2f}'.format(av_agent_reward[dir_num][eps_num]))\n",
    "        av_agent_crossed[dir_num][eps_num] = cum_crossed/max_episodes\n",
    "        print ('Agents crossed (2nd food pile) = {:.1f}'.format(av_agent_crossed[dir_num][eps_num]))\n",
    "        # print ('Num laser fired = {:.1f}'.format(cum_tags/max_episodes))\n",
    "        # print ('Total US Hit (friendly fire) = {:.1f}'.format(cum_US_hits/max_episodes))\n",
    "        # print ('Total THEM Hit = {:.1f}'.format(cum_THEM_hits/max_episodes))\n",
    "        # print ('friendly fire (%) = {:.3f}'.format(cum_US_hits/(cum_US_hits+cum_THEM_hits+1e-7)))\n",
    "\n",
    "        print ('\\nAverage Statistics by Tribe')\n",
    "        print ('=============================')\n",
    "       \n",
    "        for i, tribe in enumerate(tribes):\n",
    "            if tribe.name is not 'Crazies':\n",
    "                tribe_reward = cum_tribe_rewards[i]/max_episodes\n",
    "                print ('Tribe {} has total reward of {:.1f}'.format(tribe.name, tribe_reward))    \n",
    "                \n",
    "                # Keep track of dominating team and the rewards gathered (only if more than 1 tribe)\n",
    "                if len(tribes) > 1:\n",
    "                    if tribe_reward > dom_tribe_reward[dir_num][eps_num]:   \n",
    "                        dom_tribe_reward[dir_num][eps_num] = tribe_reward\n",
    "                        dominating_tribe[dir_num][eps_num]  = tribe.name\n",
    "\n",
    "        # Team dominance calculation (only if more than 1 tribe)\n",
    "        if len(tribes) > 1:\n",
    "            print ('Dominating Tribe: {}'.format(dominating_tribe[dir_num][eps_num]))\n",
    "            dominance[dir_num][eps_num] = dom_tribe_reward[dir_num][eps_num]/((total_rewards - \\\n",
    "                                                dom_tribe_reward[dir_num][eps_num]+1.1e-7)/(len(tribes)-1))    \n",
    "            print ('Team dominance: {0:.2f}x'.format(dominance[dir_num][eps_num]))\n",
    "\n",
    "        print ('\\nAverage Statistics by Agent')\n",
    "        print ('=============================')\n",
    "        for i in range(num_ai_agents):\n",
    "            # print (\"Agent{} of {} aggressiveness is {:.2f}\".format(i, agents[i].tribe, \\\n",
    "            #                                               cum_agent_tags[i]/(max_episodes*max_frames)))\n",
    "            print (\"Agent{} reward is {:.1f}\".format(i, cum_agent_rewards[i]/max_episodes))\n",
    "            # print('US agents hit = {:.1f}'.format(cum_agent_US_hits[i]/max_episodes))\n",
    "            # print('THEM agents hit = {:.1f}'.format(cum_agent_THEM_hits[i]/max_episodes))\n",
    "\n",
    "        print('Training time per epochs: {:.2f} sec'.format((end-start)/max_episodes))\n",
    "\n",
    "# Note: Statistics for Research Report        \n",
    "for reward in av_agent_reward:   # Average agent reward\n",
    "    print(reward)\n",
    "for agents_crossed in av_agent_crossed:   # Average num agents gathering in 2nd food pile\n",
    "    print(agents_crossed)\n",
    "\n",
    "# print dominating team and dominance factor (only if more than 1 tribe)\n",
    "if len(tribes) > 1:\n",
    "    for tribe in dominating_tribe:   # Dominating team\n",
    "        print(tribe)\n",
    "    for value in dominance:      # Team dominance\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Agent Rewards\n",
      "[8.083333333333332, 9.809999999999999, 9.106666666666666, 9.15, 9.256666666666666, 9.203333333333333]\n",
      "[8.973333333333333, 15.1, 12.033333333333333, 13.969999999999999, 21.27, 25.18]\n",
      "[6.6, 12.123333333333333, 13.246666666666666, 10.723333333333333, 10.77, 9.3]\n",
      "[4.82, 8.85, 9.273333333333333, 9.290000000000001, 9.113333333333333, 9.196666666666667]\n",
      "Agents Crossed (2nd food pile)\n",
      "[0.06666666666666667, 0.2, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.6, 0.9333333333333333, 0.4, 1.1, 1.0, 1.2]\n",
      "[0.2, 0.5666666666666667, 0.4, 0.23333333333333334, 0.13333333333333333, 0.0]\n",
      "[0.43333333333333335, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Note: Statistics for Research Report   \n",
    "print ('Average Agent Rewards')\n",
    "for reward in av_agent_reward:   # Average agent reward\n",
    "    print(reward)\n",
    "    \n",
    "print ('Agents Crossed (2nd food pile)')    \n",
    "for agents_crossed in av_agent_crossed:   # Average num agents gathering in 2nd food pile\n",
    "    print(agents_crossed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
