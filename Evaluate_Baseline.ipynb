{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Baseline\n",
    "\n",
    "### **1T-10L: 1 Teams composed of 10 agents **\n",
    "\n",
    "We run single/multiple-play to evaluate whether adjusting temperature and steps/episode (Baseline) can induce more agents to cross over and gather from the 2nd food pile.\n",
    "\n",
    "<img src=\"images/Crossing01.png\" width=\"600\">\n",
    "\n",
    "<img src=\"images/Crossing-river.png\" width=\"600\">\n",
    "\n",
    "The Crossing Game presents a more difficult problem than the Gathering game. In place of a single food pile, there are 2 food piles separated by a fixed distance or a barrier:\n",
    "\n",
    "* The smaller food pile is located closed to the agents, but has fewer food units than the number of agents. \n",
    "* The larger food pile has more food units than the number of agents but is located further away. The agents cannot see it unless they move away from the 1st food pile.\n",
    "* If there is a river, the agent suffers a -1.0 penalty for each game step in the river.\n",
    "\n",
    "The game thus deals with two challenging issues that are difficult for reinforcement learning algorithms:\n",
    "\n",
    "1. Sparce reward - the long distance an agent needs to explore with no reward to get to the 2nd food pile\n",
    "2. Local Optima - the presence of the 1st smaller food pile which the agents can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:  3.6.4\n",
      "Pytorch version: 0.4.1.post2\n",
      "OpenAI Gym version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import platform\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# This is the Crossing game environment\n",
    "from teams_env import CrossingEnv\n",
    "from teams_model import *\n",
    "from interface import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "print(\"Python version: \", platform.python_version())\n",
    "print(\"Pytorch version: {}\".format(torch.__version__))\n",
    "print(\"OpenAI Gym version: {}\".format(gym.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Models\n",
    "\n",
    "The code block contains the folder locations of the trained models of follower agents as well as the parameters used in their training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\n",
    "    # Agents trained in map = food_d37\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t0.4_rp-1.0_300gs/',   # scenario=1\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t0.8_rp-1.0_300gs/',   # scenario=2\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/',   # scenario=3\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/',   # scenario=4\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_600gs/',   # scenario=5\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/',   # scenario=6\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_600gs/',   # scenario=7\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_300gs/',   # scenario=8\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_600gs/',   # scenario=9\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_1200gs/',   # scenario=10\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_300gs/',   # scenario=11\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_600gs/',   # scenario=12\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_1200gs/',   # scenario=13\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_300gs/',   # scenario=14\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_600gs/',   # scenario=15\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_1200gs/',   # scenario=16\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_300gs/',   # scenario=17\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_600gs/',   # scenario=18\n",
    "    'models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_1200gs/',   # scenario=19\n",
    "\n",
    "    # Agents trained in map = food_d37_river_w1_d25\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/\",   # scenario=20\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/\",   # scenario=21 \n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/\",   # scenario=22\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/\",   # scenario=23\n",
    "   \n",
    "]\n",
    "\n",
    "# Parameter sets pertaining to the trained models in the folders above (not used in the code)\n",
    "parameters =[ \n",
    "            # Temperature for explore/exploit; penalty per step in river; game steps per episode\n",
    "            {'temp_start':0.4, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':0.8, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':600},    \n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':1.5, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':600},\n",
    "            {'temp_start':8.0, 'river_penalty':-1.0, 'game_steps':1200},\n",
    "            {'temp_start':1.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':1.25, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':2.0, 'river_penalty':-1.0, 'game_steps':300},\n",
    "            {'temp_start':4.0, 'river_penalty':-1.0, 'game_steps':300}\n",
    "            ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play A Single Game - Baseline\n",
    "\n",
    "Play a single game with rendering to observe agents' learning and resulting behaviors.\n",
    "\n",
    "User can change the scenario to load agent models from different folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "\n",
      "Statistics by Agent\n",
      "===================\n",
      "Agent0 aggressiveness is 0.00\n",
      "Agent0 reward is 0\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent1 aggressiveness is 0.00\n",
      "Agent1 reward is 0\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent2 aggressiveness is 0.01\n",
      "Agent2 reward is 39\n",
      "US agents hit = 1\n",
      "THEM agents hit = 0\n",
      "Agent3 aggressiveness is 0.00\n",
      "Agent3 reward is 80\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent4 aggressiveness is 0.00\n",
      "Agent4 reward is 0\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent5 aggressiveness is 0.00\n",
      "Agent5 reward is 0\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent6 aggressiveness is 0.00\n",
      "Agent6 reward is 160\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent7 aggressiveness is 0.00\n",
      "Agent7 reward is 28\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent8 aggressiveness is 0.00\n",
      "Agent8 reward is 30\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "Agent9 aggressiveness is 0.00\n",
      "Agent9 reward is 35\n",
      "US agents hit = 0\n",
      "THEM agents hit = 0\n",
      "\n",
      "Statistics in Aggregate\n",
      "=======================\n",
      "Total rewards gathered = 372\n",
      "Av. rewards per agent = 37.20\n",
      "Num laser fired = 3\n",
      "Total US Hit (friendly fire) = 1\n",
      "Total THEM Hit = 0\n",
      "friendly fire (%) = 1.000\n",
      "Num agents gathering from 2nd food pile: 3\n",
      "\n",
      "Statistics by Team\n",
      "===================\n",
      "Tribe Vikings has total reward of 372\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from teams_env import CrossingEnv\n",
    "\n",
    "game = 'Crossing'\n",
    "# map_name = \"food_d37_river_w1_d25\"\n",
    "map_name = \"food_d37\"\n",
    "\n",
    "culture = \"pacifist\"\n",
    "scenario = 1\n",
    "dir_name = folders[scenario-1]\n",
    "episodes = 3000  # This is used to recall a model file trained to a # of episodes\n",
    "\n",
    "# There will be 10 agents - 0 teams of 0 AI agents each and 0 random agent\n",
    "num_ai_agents = 10\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "num_actions = 8                       # There are 8 actions defined in Gathering\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 1\n",
    "max_frames = 500\n",
    "\n",
    "# Initialize parameters for Crossing and Explore\n",
    "river_penalty = -1\n",
    "crossed = [0 for i in range(num_ai_agents)]  # Keep track of agents gathering from 2nd food pile\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "jumping_zone = True\n",
    "\n",
    "\n",
    "# Load models for AI agents\n",
    "if episodes > 0:\n",
    "    agents= [[] for i in range(num_ai_agents)]\n",
    "    # If episodes is provided (not 0), load the model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,episodes)\n",
    "        try:\n",
    "            with open(model_file, 'rb') as f:\n",
    "                print(\"Load saved model for agent {}\".format(i))\n",
    "                agent = Policy(num_frames, num_actions, 0)\n",
    "                optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                # New way to save and load models - based on: \n",
    "                # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                _ = load_model(agent, optimizer, f)\n",
    "                agent.eval()\n",
    "                agents[i] = agent\n",
    "        except OSError:\n",
    "            print('Model file not found.')\n",
    "            raise\n",
    "else:\n",
    "    # If episodes=0, start with a freshly initialized model for each AI agent\n",
    "    for i in range(num_ai_agents):\n",
    "        print(\"Load AI agent {}\".format(i))\n",
    "        agents.append(Policy(num_frames, num_actions, i))\n",
    "\n",
    "# Load random agents    \n",
    "for i in range(num_ai_agents,num_agents):\n",
    "    print(\"Load random agent {}\".format(i))\n",
    "    agents.append(Rdn_Policy())\n",
    "\n",
    "# Initialize AI and random agent data\n",
    "actions = [0 for i in range(num_agents)]\n",
    "tags = [0 for i in range(num_agents)]\n",
    "\n",
    "# Establish tribal association\n",
    "tribes = []\n",
    "tribes.append(Tribe(name='Vikings',color='blue', culture=culture, \\\n",
    "                    agents=[agents[0], agents[1], agents[2], agents[3], agents[4], \\\n",
    "                           agents[5], agents[6], agents[7], agents[8], agents[9]]))\n",
    "\n",
    "#tribes.append(Tribe(name='Saxons', color='red', culture=culture, \\\n",
    "#                    agents=[agents[4], agents[5], agents[6], agents[7]]))\n",
    "#tribes.append(Tribe(name='Franks', color='purple', culture=culture, \\\n",
    "#                    agents=[agents[8], agents[9], agents[10], agents[11]]))\n",
    "# tribes.append(Tribe(name='Crazies', color='yellow', agents=[agents[3], \\\n",
    "#                    agents[4], agents[5]]))   # random agents are crazy!!!\n",
    "\n",
    "# Set up agent and tribe info to pass into env\n",
    "agent_colors = [agent.color for agent in agents]\n",
    "agent_tribes = [agent.tribe for agent in agents]\n",
    "tribe_names = [tribe.name for tribe in tribes]\n",
    "    \n",
    "env = CrossingEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty, tribes=tribe_names, \\\n",
    "                  debug_agent=0)    \n",
    "    \n",
    "for ep in range(max_episodes):\n",
    "    \n",
    "    US_hits = [0 for i in range(num_agents)]\n",
    "    THEM_hits = [0 for i in range(num_agents)]\n",
    "\n",
    "    env_obs = env.reset()  # Environment return observations\n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Unpack observations into data structure compatible with agent Policy\n",
    "    agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "    for i in range(num_ai_agents):    # Reset agent info - laser tag statistics\n",
    "        agents[i].reset_info()    \n",
    "    \n",
    "    env.render()  \n",
    "    time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "    \"\"\"\n",
    "    # For Debug only\n",
    "    print (len(agents_obs))\n",
    "    print (agents_obs[0].shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "    state = np.stack([state]*num_frames)\n",
    "\n",
    "    # Reset LSTM hidden units when episode begins\n",
    "    cx = Variable(torch.zeros(1, 256))\n",
    "    hx = Variable(torch.zeros(1, 256))\n",
    "    \"\"\"\n",
    "\n",
    "    for frame in range(max_frames):\n",
    "\n",
    "        for i in range(num_ai_agents):    # For AI agents\n",
    "            actions[i], _ = select_action(agents[i], agents_obs[i], cuda=False)\n",
    "            if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "        for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "            actions[i] = agents[i].select_action(agents_obs[i])\n",
    "            if actions[i] is 6:\n",
    "                tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "        \"\"\"\n",
    "        For now, we do not implement LSTM\n",
    "        # Select action\n",
    "        action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "        \"\"\"\n",
    "\n",
    "        # if frame % 10 == 0:\n",
    "        #     print (actions)    \n",
    "            \n",
    "        # Perform step        \n",
    "        env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "        \"\"\"\n",
    "        For Debug only\n",
    "        print (env_obs)\n",
    "        print (reward)\n",
    "        print (done) \n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(num_ai_agents):\n",
    "            agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "        # Unpack observations into data structure compatible with agent Policy\n",
    "        agents_obs = unpack_env_obs(env_obs)\n",
    "        load_info(agents, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            US_hits[i] += agents[i].US_hit\n",
    "            THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "        \"\"\"\n",
    "        For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "        # Evict oldest diff add new diff to state\n",
    "        next_state = np.stack([next_state]*num_frames)\n",
    "        next_state[1:, :, :] = state[:-1, :, :]\n",
    "        state = next_state\n",
    "        \"\"\"\n",
    "        total = 0\n",
    "        for i in range(num_ai_agents):\n",
    "            agent_reward = sum(agents[i].rewards)\n",
    "            total += agent_reward\n",
    "        \n",
    "        env.render()\n",
    "        time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "        if any(done):\n",
    "            print(\"Done after {} frames\".format(frame))\n",
    "            break\n",
    "\n",
    "env.close()  # Close the rendering window\n",
    "\n",
    "# Print out statistics of AI agents\n",
    "\n",
    "total_rewards = 0\n",
    "total_tags = 0\n",
    "total_US_hits = 0\n",
    "total_THEM_hits = 0\n",
    "\n",
    "print ('\\nStatistics by Agent')\n",
    "print ('===================')\n",
    "for i in range(num_ai_agents):\n",
    "    agent_tags = sum(agents[i].tag_hist)\n",
    "    total_tags += agent_tags\n",
    "    print (\"Agent{} aggressiveness is {:.2f}\".format(i, sum(agents[i].tag_hist)/frame))\n",
    "\n",
    "    agent_reward = sum(agents[i].rewards)\n",
    "    total_rewards += agent_reward\n",
    "    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "\n",
    "    agent_US_hits = sum(agents[i].US_hits)\n",
    "    agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "    total_US_hits += agent_US_hits\n",
    "    total_THEM_hits += agent_THEM_hits\n",
    "\n",
    "    print('US agents hit = {}'.format(agent_US_hits))\n",
    "    print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "\n",
    "print ('\\nStatistics in Aggregate')\n",
    "print ('=======================')\n",
    "print ('Total rewards gathered = {}'.format(total_rewards))\n",
    "print ('Av. rewards per agent = {0:.2f}'.format(total_rewards/num_ai_agents))\n",
    "print ('Num laser fired = {}'.format(total_tags))\n",
    "print ('Total US Hit (friendly fire) = {}'.format(total_US_hits))\n",
    "print ('Total THEM Hit = {}'.format(total_THEM_hits))\n",
    "print ('friendly fire (%) = {0:.3f}'.format(total_US_hits/(total_US_hits+total_THEM_hits+1e-7)))\n",
    "\n",
    "for (i, loc) in env.consumption:\n",
    "    if loc[0] > second_pile_x:\n",
    "        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "        crossed[i] = 1\n",
    "        \n",
    "print (\"Num agents gathering from 2nd food pile: {}\".format(sum(crossed)))\n",
    "\n",
    "print ('\\nStatistics by Team')\n",
    "print ('===================')\n",
    "top_tribe = None\n",
    "top_tribe_reward = 0\n",
    "\n",
    "for i, tribe in enumerate(tribes):\n",
    "    if tribe.name is not 'Crazies':\n",
    "        tribe_reward = sum(tribe.sum_rewards())\n",
    "        print ('Tribe {} has total reward of {}'.format(tribe.name, tribe_reward))\n",
    "                           \n",
    "        if tribe_reward > top_tribe_reward:   # Keep track of dominating team\n",
    "            top_tribe_reward = tribe_reward\n",
    "            top_tribe = tribe.name\n",
    "\n",
    "# Team dominance calculation\n",
    "if len(tribes) > 1:\n",
    "    print ('Dominating Team: {}'.format(top_tribe))\n",
    "    dominance = top_tribe_reward/((total_rewards-top_tribe_reward+1.1e-7)/(len(tribes)-1))    \n",
    "    print ('Team dominance: {0:.2f}x'.format(dominance))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Stats - Baseline (Map = food_d37)\n",
    "\n",
    "Our research requires gathering game stats for agents and teams over 30 episodes of game play:\n",
    "\n",
    "* Average agent reward - average number of apples gathered per agent per episode  \n",
    "* The number of agents gathering apples at the 2nd food pile \n",
    "\n",
    "Rendering is disabled to speed things up.\n",
    "\n",
    "<img src=\"images/Crossing01.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t0.4_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 188.1\n",
      "Av. agent reward = 18.81\n",
      "Agents crossed (2nd food pile) = 5.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 188.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 13.4\n",
      "Agent1 reward is 11.2\n",
      "Agent2 reward is 14.6\n",
      "Agent3 reward is 10.6\n",
      "Agent4 reward is 34.7\n",
      "Agent5 reward is 14.5\n",
      "Agent6 reward is 39.2\n",
      "Agent7 reward is 21.3\n",
      "Agent8 reward is 2.9\n",
      "Agent9 reward is 25.6\n",
      "Training time per epochs: 1.82 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 191.0\n",
      "Av. agent reward = 19.10\n",
      "Agents crossed (2nd food pile) = 2.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 191.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 41.6\n",
      "Agent1 reward is 14.6\n",
      "Agent2 reward is 9.1\n",
      "Agent3 reward is 16.8\n",
      "Agent4 reward is 3.7\n",
      "Agent5 reward is 14.9\n",
      "Agent6 reward is 27.0\n",
      "Agent7 reward is 15.8\n",
      "Agent8 reward is 28.0\n",
      "Agent9 reward is 19.4\n",
      "Training time per epochs: 1.81 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 307.2\n",
      "Av. agent reward = 30.72\n",
      "Agents crossed (2nd food pile) = 4.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 307.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 52.4\n",
      "Agent1 reward is 18.7\n",
      "Agent2 reward is 40.4\n",
      "Agent3 reward is 29.9\n",
      "Agent4 reward is 42.9\n",
      "Agent5 reward is 2.4\n",
      "Agent6 reward is 48.4\n",
      "Agent7 reward is 3.7\n",
      "Agent8 reward is 28.2\n",
      "Agent9 reward is 40.3\n",
      "Training time per epochs: 1.85 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 347.8\n",
      "Av. agent reward = 34.78\n",
      "Agents crossed (2nd food pile) = 4.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 347.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 37.9\n",
      "Agent1 reward is 8.2\n",
      "Agent2 reward is 65.3\n",
      "Agent3 reward is 56.4\n",
      "Agent4 reward is 5.9\n",
      "Agent5 reward is 6.7\n",
      "Agent6 reward is 92.2\n",
      "Agent7 reward is 7.3\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 37.9\n",
      "Training time per epochs: 1.80 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 231.1\n",
      "Av. agent reward = 23.11\n",
      "Agents crossed (2nd food pile) = 2.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 231.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 29.6\n",
      "Agent1 reward is 16.9\n",
      "Agent2 reward is 6.3\n",
      "Agent3 reward is 24.7\n",
      "Agent4 reward is 8.7\n",
      "Agent5 reward is 8.0\n",
      "Agent6 reward is 57.5\n",
      "Agent7 reward is 22.3\n",
      "Agent8 reward is 17.2\n",
      "Agent9 reward is 39.8\n",
      "Training time per epochs: 1.84 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 363.3\n",
      "Av. agent reward = 36.33\n",
      "Agents crossed (2nd food pile) = 5.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 363.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 10.9\n",
      "Agent1 reward is 3.0\n",
      "Agent2 reward is 66.6\n",
      "Agent3 reward is 53.9\n",
      "Agent4 reward is 7.0\n",
      "Agent5 reward is 33.0\n",
      "Agent6 reward is 101.4\n",
      "Agent7 reward is 18.0\n",
      "Agent8 reward is 28.3\n",
      "Agent9 reward is 41.2\n",
      "Training time per epochs: 1.85 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37/pacifist/t0.8_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 173.5\n",
      "Av. agent reward = 17.35\n",
      "Agents crossed (2nd food pile) = 4.3\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 173.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 14.5\n",
      "Agent1 reward is 6.8\n",
      "Agent2 reward is 1.1\n",
      "Agent3 reward is 9.5\n",
      "Agent4 reward is 27.7\n",
      "Agent5 reward is 11.1\n",
      "Agent6 reward is 64.8\n",
      "Agent7 reward is 14.3\n",
      "Agent8 reward is 4.9\n",
      "Agent9 reward is 18.6\n",
      "Training time per epochs: 1.92 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 299.0\n",
      "Av. agent reward = 29.90\n",
      "Agents crossed (2nd food pile) = 4.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 299.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 57.9\n",
      "Agent1 reward is 37.9\n",
      "Agent2 reward is 0.1\n",
      "Agent3 reward is 29.8\n",
      "Agent4 reward is 20.1\n",
      "Agent5 reward is 6.9\n",
      "Agent6 reward is 66.3\n",
      "Agent7 reward is 15.5\n",
      "Agent8 reward is 35.2\n",
      "Agent9 reward is 29.4\n",
      "Training time per epochs: 1.83 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 274.0\n",
      "Av. agent reward = 27.40\n",
      "Agents crossed (2nd food pile) = 3.7\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 274.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 77.5\n",
      "Agent1 reward is 0.7\n",
      "Agent2 reward is 4.4\n",
      "Agent3 reward is 38.6\n",
      "Agent4 reward is 6.1\n",
      "Agent5 reward is 7.5\n",
      "Agent6 reward is 62.8\n",
      "Agent7 reward is 21.0\n",
      "Agent8 reward is 25.6\n",
      "Agent9 reward is 29.8\n",
      "Training time per epochs: 1.83 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 292.1\n",
      "Av. agent reward = 29.21\n",
      "Agents crossed (2nd food pile) = 4.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 292.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 62.8\n",
      "Agent1 reward is 16.7\n",
      "Agent2 reward is 10.4\n",
      "Agent3 reward is 27.5\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 6.0\n",
      "Agent6 reward is 78.6\n",
      "Agent7 reward is 25.1\n",
      "Agent8 reward is 31.7\n",
      "Agent9 reward is 33.3\n",
      "Training time per epochs: 1.80 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 328.4\n",
      "Av. agent reward = 32.84\n",
      "Agents crossed (2nd food pile) = 3.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 328.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 45.5\n",
      "Agent1 reward is 110.3\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 45.2\n",
      "Agent4 reward is 0.6\n",
      "Agent5 reward is 10.5\n",
      "Agent6 reward is 33.5\n",
      "Agent7 reward is 20.9\n",
      "Agent8 reward is 29.9\n",
      "Agent9 reward is 32.1\n",
      "Training time per epochs: 1.84 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "Load saved model for agent 0\n",
      "Load saved model for agent 1\n",
      "Load saved model for agent 2\n",
      "Load saved model for agent 3\n",
      "Load saved model for agent 4\n",
      "Load saved model for agent 5\n",
      "Load saved model for agent 6\n",
      "Load saved model for agent 7\n",
      "Load saved model for agent 8\n",
      "Load saved model for agent 9\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 235.4\n",
      "Av. agent reward = 23.54\n",
      "Agents crossed (2nd food pile) = 2.8\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 235.4\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.5\n",
      "Agent1 reward is 35.8\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 41.2\n",
      "Agent4 reward is 4.4\n",
      "Agent5 reward is 6.8\n",
      "Agent6 reward is 60.4\n",
      "Agent7 reward is 26.6\n",
      "Agent8 reward is 27.7\n",
      "Agent9 reward is 30.9\n",
      "Training time per epochs: 1.84 sec\n",
      "[18.81, 19.10333333333333, 30.723333333333336, 34.776666666666664, 23.11, 36.32666666666667]\n",
      "[17.346666666666668, 29.9, 27.403333333333336, 29.21, 32.843333333333334, 23.53666666666667]\n",
      "[5.366666666666666, 2.6, 4.166666666666667, 4.2, 2.066666666666667, 5.1]\n",
      "[4.333333333333333, 4.4, 3.6666666666666665, 4.133333333333334, 3.8666666666666667, 2.7666666666666666]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\"\"\"\n",
    "dir_names = [\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.0_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.25_rp-1.0_600gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t1.5_rp-1.0_1200gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t2.0_rp-1.0_1200gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t4.0_rp-1.0_1200gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_600gs/\", \n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t8.0_rp-1.0_1200gs/\"    \n",
    "             ]\n",
    "\"\"\"\n",
    "\n",
    "dir_names = [\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t0.4_rp-1.0_300gs/\",\n",
    "             \"models/1T-10L/baseline/food_d37/pacifist/t0.8_rp-1.0_300gs/\",     \n",
    "             ]\n",
    "episodes = [500, 1000, 1500, 2000, 2500, 3000] \n",
    "game = 'Crossing'\n",
    "culture = \"pacifist\"\n",
    "map_name = \"food_d37\"\n",
    "\n",
    "# Performance Statistics - for Research Report\n",
    "av_agent_reward = [[0 for i in episodes] for j in dir_names]\n",
    "av_agent_crossed = [[0 for i in episodes] for j in dir_names]  \n",
    "dominating_tribe = [[None for i in episodes] for j in dir_names]\n",
    "dom_tribe_reward = [[0 for i in episodes] for j in dir_names]\n",
    "dominance = [[0 for i in episodes] for j in dir_names]\n",
    "\n",
    "# There will be 10 agents - 0 teams of 0 AI agents each and 0 random agent\n",
    "num_ai_agents = 10\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Initialize environment\n",
    "render = False\n",
    "SPEED = 1/30\n",
    "river_penalty = -1\n",
    "num_actions = 8                       # There are 8 actions defined in Gathering\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 30\n",
    "max_frames = 500\n",
    "verbose = False\n",
    "\n",
    "\n",
    "for dir_num, dir_name in enumerate(dir_names):\n",
    "    print (\"###### Dir = {} #######\".format(dir_name))\n",
    "    \n",
    "    for eps_num, eps in enumerate(episodes):\n",
    "        print (\"###### Trained episodes = {} #######\".format(eps))\n",
    "    \n",
    "        # Load models for AI agents\n",
    "        agents= [[] for i in range(num_ai_agents)]\n",
    "        # If episodes is provided (not 0), load the model for each AI agent\n",
    "        for i in range(num_ai_agents):\n",
    "            model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,eps)\n",
    "            try:\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    print(\"Load saved model for agent {}\".format(i))\n",
    "                    agent = Policy(num_frames, num_actions, 0)\n",
    "                    optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                    # New way to save and load models - based on: \n",
    "                    # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                    _ = load_model(agent, optimizer, f)\n",
    "                    agent.eval()\n",
    "                    agents[i] = agent\n",
    "            except OSError:\n",
    "                print('Model file not found.')\n",
    "                raise\n",
    "\n",
    "        # Load random agents    \n",
    "        for i in range(num_ai_agents,num_agents):\n",
    "            # print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "        \n",
    "        # Establish tribal association\n",
    "        tribes = []\n",
    "        tribes.append(Tribe(name='Vikings',color='blue', culture=culture, \\\n",
    "                    agents=[agents[0], agents[1], agents[2], agents[3], agents[4], \\\n",
    "                           agents[5], agents[6], agents[7], agents[8], agents[9]]))\n",
    "\n",
    "        # Set up agent and tribe info to pass into env\n",
    "        agent_colors = [agent.color for agent in agents]\n",
    "        agent_tribes = [agent.tribe for agent in agents]\n",
    "        tribe_names = [tribe.name for tribe in tribes]\n",
    "        \n",
    "        env = CrossingEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty, tribes=tribe_names, \\\n",
    "                  debug_agent=0)\n",
    "\n",
    "        # Used to accumulate episode stats for averaging\n",
    "        cum_rewards = 0\n",
    "        cum_crossed = 0\n",
    "        cum_tags = 0\n",
    "        cum_US_hits = 0\n",
    "        cum_THEM_hits = 0\n",
    "        cum_agent_rewards = [0 for agent in agents]\n",
    "        cum_agent_tags = [0 for agent in agents]\n",
    "        cum_agent_US_hits = [0 for agent in agents]\n",
    "        cum_agent_THEM_hits = [0 for agent in agents]\n",
    "        cum_tribe_rewards = [0 for t in tribes if t.name is not 'Crazies']\n",
    "\n",
    "        cuda = False\n",
    "        start = time.time()\n",
    "\n",
    "        for ep in range(max_episodes):\n",
    "    \n",
    "            print('.', end='')  # To show progress\n",
    "    \n",
    "            # Initialize AI and random agent data\n",
    "            actions = [0 for i in range(num_agents)]\n",
    "            tags = [0 for i in range(num_agents)]\n",
    "            US_hits = [0 for i in range(num_agents)]\n",
    "            THEM_hits = [0 for i in range(num_agents)]\n",
    "            \n",
    "            # Keep track of agents gathering from 2nd food pile\n",
    "            crossed = [0 for i in range(num_ai_agents)]\n",
    "\n",
    "            env_obs = env.reset()  # Environment return observations\n",
    "            \"\"\"\n",
    "            # For Debug only\n",
    "            print (len(agents_obs))\n",
    "            print (agents_obs[0].shape)\n",
    "            \"\"\"\n",
    "    \n",
    "            # Unpack observations into data structure compatible with agent Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "            for i in range(num_ai_agents):    # Reset agent info - laser tag statistics\n",
    "                agents[i].reset_info()    \n",
    "    \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "            \"\"\"\n",
    "            # For Debug only\n",
    "            print (len(agents_obs))\n",
    "            print (agents_obs[0].shape)\n",
    "            \"\"\"\n",
    "    \n",
    "            \"\"\"\n",
    "            For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "            state = np.stack([state]*num_frames)\n",
    "\n",
    "            # Reset LSTM hidden units when episode begins\n",
    "            cx = Variable(torch.zeros(1, 256))\n",
    "            hx = Variable(torch.zeros(1, 256))\n",
    "            \"\"\"\n",
    "\n",
    "            for frame in range(max_frames):\n",
    "\n",
    "                for i in range(num_ai_agents):    # For AI agents\n",
    "                    actions[i], _ = select_action(agents[i], agents_obs[i], cuda=cuda)\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "                for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "                    actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                    if actions[i] is 6:\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "                \"\"\"\n",
    "                For now, we do not implement LSTM\n",
    "                # Select action\n",
    "                action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "                \"\"\"\n",
    "\n",
    "                # if frame % 10 == 0:\n",
    "                #     print (actions)    \n",
    "            \n",
    "                # Perform step        \n",
    "                env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "                \"\"\"\n",
    "                For Debug only\n",
    "                print (env_obs)\n",
    "                print (reward)\n",
    "                print (done) \n",
    "                \"\"\"\n",
    "\n",
    "                for i in range(num_ai_agents):\n",
    "                    agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "                # Unpack observations into data structure compatible with agent Policy\n",
    "                agents_obs = unpack_env_obs(env_obs)\n",
    "                load_info(agents, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "                for i in range(num_agents):\n",
    "                    US_hits[i] += agents[i].US_hit\n",
    "                    THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "                \"\"\"\n",
    "                For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "                # Evict oldest diff add new diff to state\n",
    "                next_state = np.stack([next_state]*num_frames)\n",
    "                next_state[1:, :, :] = state[:-1, :, :]\n",
    "                state = next_state\n",
    "                \"\"\"\n",
    "        \n",
    "                if render and ep is 0: \n",
    "                    env.render()\n",
    "                    time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "                if any(done):\n",
    "                    print(\"Done after {} frames\".format(frame))\n",
    "                    break\n",
    "                    \n",
    "                for (i, loc) in env.consumption:\n",
    "                    if loc[0] > second_pile_x:\n",
    "                        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "                        crossed[i] = 1\n",
    "            \n",
    "            # Print out statistics of AI agents\n",
    "            ep_rewards = 0\n",
    "            ep_tags = 0\n",
    "            ep_US_hits = 0\n",
    "            ep_THEM_hits = 0\n",
    "            ep_crossed = sum(crossed)     # calculated num agents gathering in 2nd pile for episode\n",
    "\n",
    "            if verbose:\n",
    "                print ('\\nStatistics by Agent')\n",
    "                print ('===================')\n",
    "            for i in range(num_ai_agents):\n",
    "                agent_tags = sum(agents[i].tag_hist)\n",
    "                ep_tags += agent_tags\n",
    "                cum_agent_tags[i] += agent_tags\n",
    "\n",
    "                agent_reward = sum(agents[i].rewards)\n",
    "                ep_rewards += agent_reward\n",
    "                cum_agent_rewards[i] += agent_reward\n",
    "\n",
    "                agent_US_hits = sum(agents[i].US_hits)\n",
    "                agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "                ep_US_hits += agent_US_hits\n",
    "                ep_THEM_hits += agent_THEM_hits\n",
    "                cum_agent_US_hits[i] += agent_US_hits\n",
    "                cum_agent_THEM_hits[i] += agent_THEM_hits\n",
    "        \n",
    "                if verbose:\n",
    "                    print (\"Agent{} aggressiveness is {:.2f}\".format(i, agent_tags/frame))\n",
    "                    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "                    # print('US agents hit = {}'.format(agent_US_hits))\n",
    "                    # print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "        \n",
    "            cum_rewards += ep_rewards\n",
    "            cum_crossed += ep_crossed\n",
    "            cum_tags += ep_tags\n",
    "            cum_US_hits += ep_US_hits\n",
    "            cum_THEM_hits += ep_THEM_hits\n",
    "    \n",
    "            if verbose:\n",
    "                print ('\\nStatistics in Aggregate')\n",
    "                print ('=======================')\n",
    "                print ('Total rewards gathered = {}'.format(ep_rewards))\n",
    "                print ('Num agents crossed = {}'.format(ep_crossed))\n",
    "                # print ('Num laser fired = {}'.format(ep_tags))\n",
    "                # print ('Total US Hit (friendly fire) = {}'.format(ep_US_hits))\n",
    "                # print ('Total THEM Hit = {}'.format(ep_THEM_hits))\n",
    "                # print ('friendly fire (%) = {0:.3f}'.format(ep_US_hits/(ep_US_hits+ep_THEM_hits+1e-7)))\n",
    "\n",
    "            if verbose:\n",
    "                print ('\\nStatistics by Tribe')\n",
    "                print ('===================')\n",
    "            for i, t in enumerate(tribes):\n",
    "                if t.name is not 'Crazies':\n",
    "                    ep_tribe_reward = sum(t.sum_rewards())\n",
    "                    cum_tribe_rewards[i] += ep_tribe_reward\n",
    "                    if verbose:\n",
    "                        print ('Tribe {} has total reward of {}'.format(t.name, ep_tribe_reward))\n",
    "\n",
    "            for i in range(num_ai_agents):\n",
    "                agents[i].clear_history()\n",
    "\n",
    "        env.close()  # Close the rendering window\n",
    "        end = time.time()\n",
    "\n",
    "        print ('\\nAverage Statistics in Aggregate')\n",
    "        print ('=================================')\n",
    "        total_rewards = cum_rewards/max_episodes\n",
    "        print ('Total rewards gathered = {:.1f}'.format(total_rewards))\n",
    "        av_agent_reward[dir_num][eps_num] = cum_rewards/max_episodes/num_ai_agents\n",
    "        print ('Av. agent reward = {:.2f}'.format(av_agent_reward[dir_num][eps_num]))\n",
    "        av_agent_crossed[dir_num][eps_num] = cum_crossed/max_episodes\n",
    "        print ('Agents crossed (2nd food pile) = {:.1f}'.format(av_agent_crossed[dir_num][eps_num]))\n",
    "        # print ('Num laser fired = {:.1f}'.format(cum_tags/max_episodes))\n",
    "        # print ('Total US Hit (friendly fire) = {:.1f}'.format(cum_US_hits/max_episodes))\n",
    "        # print ('Total THEM Hit = {:.1f}'.format(cum_THEM_hits/max_episodes))\n",
    "        # print ('friendly fire (%) = {:.3f}'.format(cum_US_hits/(cum_US_hits+cum_THEM_hits+1e-7)))\n",
    "\n",
    "        print ('\\nAverage Statistics by Tribe')\n",
    "        print ('=============================')\n",
    "       \n",
    "        for i, tribe in enumerate(tribes):\n",
    "            if tribe.name is not 'Crazies':\n",
    "                tribe_reward = cum_tribe_rewards[i]/max_episodes\n",
    "                print ('Tribe {} has total reward of {:.1f}'.format(tribe.name, tribe_reward))    \n",
    "                \n",
    "                # Keep track of dominating team and the rewards gathered (only if more than 1 tribe)\n",
    "                if len(tribes) > 1:\n",
    "                    if tribe_reward > dom_tribe_reward[dir_num][eps_num]:   \n",
    "                        dom_tribe_reward[dir_num][eps_num] = tribe_reward\n",
    "                        dominating_tribe[dir_num][eps_num]  = tribe.name\n",
    "\n",
    "        # Team dominance calculation (only if more than 1 tribe)\n",
    "        if len(tribes) > 1:\n",
    "            print ('Dominating Tribe: {}'.format(dominating_tribe[dir_num][eps_num]))\n",
    "            dominance[dir_num][eps_num] = dom_tribe_reward[dir_num][eps_num]/((total_rewards - \\\n",
    "                                                dom_tribe_reward[dir_num][eps_num]+1.1e-7)/(len(tribes)-1))    \n",
    "            print ('Team dominance: {0:.2f}x'.format(dominance[dir_num][eps_num]))\n",
    "\n",
    "        print ('\\nAverage Statistics by Agent')\n",
    "        print ('=============================')\n",
    "        for i in range(num_ai_agents):\n",
    "            # print (\"Agent{} of {} aggressiveness is {:.2f}\".format(i, agents[i].tribe, \\\n",
    "            #                                               cum_agent_tags[i]/(max_episodes*max_frames)))\n",
    "            print (\"Agent{} reward is {:.1f}\".format(i, cum_agent_rewards[i]/max_episodes))\n",
    "            # print('US agents hit = {:.1f}'.format(cum_agent_US_hits[i]/max_episodes))\n",
    "            # print('THEM agents hit = {:.1f}'.format(cum_agent_THEM_hits[i]/max_episodes))\n",
    "\n",
    "        print('Training time per epochs: {:.2f} sec'.format((end-start)/max_episodes))\n",
    "\n",
    "# Note: Statistics for Research Report        \n",
    "for reward in av_agent_reward:   # Average agent reward\n",
    "    print(reward)\n",
    "for agents_crossed in av_agent_crossed:   # Average num agents gathering in 2nd food pile\n",
    "    print(agents_crossed)\n",
    "\n",
    "# print dominating team and dominance factor (only if more than 1 tribe)\n",
    "if len(tribes) > 1:\n",
    "    for tribe in dominating_tribe:   # Dominating team\n",
    "        print(tribe)\n",
    "    for value in dominance:      # Team dominance\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics for Research Report \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Agent Rewards\n",
      "[18.81, 19.10333333333333, 30.723333333333336, 34.776666666666664, 23.11, 36.32666666666667]\n",
      "[17.346666666666668, 29.9, 27.403333333333336, 29.21, 32.843333333333334, 23.53666666666667]\n",
      "Agents Crossed (2nd food pile)\n",
      "[5.366666666666666, 2.6, 4.166666666666667, 4.2, 2.066666666666667, 5.1]\n",
      "[4.333333333333333, 4.4, 3.6666666666666665, 4.133333333333334, 3.8666666666666667, 2.7666666666666666]\n"
     ]
    }
   ],
   "source": [
    "# Note: Statistics for Research Report   \n",
    "print ('Average Agent Rewards')\n",
    "for reward in av_agent_reward:   # Average agent reward\n",
    "    print(reward)\n",
    "    \n",
    "print ('Agents Crossed (2nd food pile)')    \n",
    "for agents_crossed in av_agent_crossed:   # Average num agents gathering in 2nd food pile\n",
    "    print(agents_crossed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Stats - Baseline (Map = food_d37_river_w1_d25)\n",
    "\n",
    "Our research requires gathering game stats for agents and teams over 30 episodes of game play:\n",
    "\n",
    "* Average agent reward - average number of apples gathered per agent per episode  \n",
    "* The number of agents gathering apples at the 2nd food pile \n",
    "\n",
    "Rendering is disabled to speed things up.\n",
    "\n",
    "<img src=\"images/Crossing-river.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### Dir = models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 80.8\n",
      "Av. agent reward = 8.08\n",
      "Agents crossed (2nd food pile) = 0.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 80.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.8\n",
      "Agent1 reward is 0.5\n",
      "Agent2 reward is 0.1\n",
      "Agent3 reward is 24.3\n",
      "Agent4 reward is 2.8\n",
      "Agent5 reward is 0.2\n",
      "Agent6 reward is 0.8\n",
      "Agent7 reward is 21.1\n",
      "Agent8 reward is 26.6\n",
      "Agent9 reward is 3.6\n",
      "Training time per epochs: 4.26 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 98.1\n",
      "Av. agent reward = 9.81\n",
      "Agents crossed (2nd food pile) = 0.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 98.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is -0.7\n",
      "Agent3 reward is 14.9\n",
      "Agent4 reward is 21.6\n",
      "Agent5 reward is 0.2\n",
      "Agent6 reward is 11.3\n",
      "Agent7 reward is 21.5\n",
      "Agent8 reward is 2.3\n",
      "Agent9 reward is 27.0\n",
      "Training time per epochs: 4.08 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 91.1\n",
      "Av. agent reward = 9.11\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 91.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 8.1\n",
      "Agent4 reward is 12.3\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 8.5\n",
      "Agent7 reward is 26.5\n",
      "Agent8 reward is 4.7\n",
      "Agent9 reward is 30.9\n",
      "Training time per epochs: 4.40 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 91.5\n",
      "Av. agent reward = 9.15\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 91.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 28.9\n",
      "Agent4 reward is 22.2\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 8.1\n",
      "Agent8 reward is 1.3\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 4.26 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.6\n",
      "Av. agent reward = 9.26\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.6\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 15.6\n",
      "Agent4 reward is 11.3\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 27.6\n",
      "Agent8 reward is 7.0\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 4.48 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.0\n",
      "Av. agent reward = 9.20\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 22.8\n",
      "Agent4 reward is 19.2\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 17.9\n",
      "Agent8 reward is 1.2\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 3.95 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 89.7\n",
      "Av. agent reward = 8.97\n",
      "Agents crossed (2nd food pile) = 1.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 89.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 1.5\n",
      "Agent1 reward is 16.4\n",
      "Agent2 reward is 19.6\n",
      "Agent3 reward is 29.3\n",
      "Agent4 reward is -1.3\n",
      "Agent5 reward is 7.7\n",
      "Agent6 reward is 4.7\n",
      "Agent7 reward is 3.0\n",
      "Agent8 reward is 0.9\n",
      "Agent9 reward is 7.9\n",
      "Training time per epochs: 4.71 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 151.0\n",
      "Av. agent reward = 15.10\n",
      "Agents crossed (2nd food pile) = 0.9\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 151.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is -0.4\n",
      "Agent1 reward is 9.8\n",
      "Agent2 reward is 14.7\n",
      "Agent3 reward is 15.7\n",
      "Agent4 reward is 1.5\n",
      "Agent5 reward is 8.8\n",
      "Agent6 reward is 8.9\n",
      "Agent7 reward is 55.8\n",
      "Agent8 reward is 16.4\n",
      "Agent9 reward is 19.8\n",
      "Training time per epochs: 4.63 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 120.3\n",
      "Av. agent reward = 12.03\n",
      "Agents crossed (2nd food pile) = 0.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 120.3\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 1.1\n",
      "Agent2 reward is 1.7\n",
      "Agent3 reward is 24.5\n",
      "Agent4 reward is 0.3\n",
      "Agent5 reward is 3.4\n",
      "Agent6 reward is 0.9\n",
      "Agent7 reward is 29.6\n",
      "Agent8 reward is 30.1\n",
      "Agent9 reward is 28.7\n",
      "Training time per epochs: 4.59 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 139.7\n",
      "Av. agent reward = 13.97\n",
      "Agents crossed (2nd food pile) = 1.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 139.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.4\n",
      "Agent1 reward is 44.3\n",
      "Agent2 reward is 19.1\n",
      "Agent3 reward is 29.8\n",
      "Agent4 reward is 0.1\n",
      "Agent5 reward is 0.9\n",
      "Agent6 reward is 1.0\n",
      "Agent7 reward is 2.7\n",
      "Agent8 reward is 29.6\n",
      "Agent9 reward is 11.8\n",
      "Training time per epochs: 4.67 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 212.7\n",
      "Av. agent reward = 21.27\n",
      "Agents crossed (2nd food pile) = 1.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 212.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 122.4\n",
      "Agent2 reward is 29.2\n",
      "Agent3 reward is 29.1\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 17.7\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is -1.1\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 15.3\n",
      "Training time per epochs: 4.30 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 251.8\n",
      "Av. agent reward = 25.18\n",
      "Agents crossed (2nd food pile) = 1.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 251.8\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 9.3\n",
      "Agent1 reward is 162.4\n",
      "Agent2 reward is 24.7\n",
      "Agent3 reward is 22.4\n",
      "Agent4 reward is 1.8\n",
      "Agent5 reward is 21.3\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 2.6\n",
      "Agent8 reward is 0.0\n",
      "Agent9 reward is 7.4\n",
      "Training time per epochs: 4.94 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 66.0\n",
      "Av. agent reward = 6.60\n",
      "Agents crossed (2nd food pile) = 0.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 66.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 2.2\n",
      "Agent1 reward is 2.2\n",
      "Agent2 reward is 0.4\n",
      "Agent3 reward is 32.5\n",
      "Agent4 reward is 0.7\n",
      "Agent5 reward is -0.6\n",
      "Agent6 reward is 15.2\n",
      "Agent7 reward is -2.1\n",
      "Agent8 reward is 10.7\n",
      "Agent9 reward is 4.9\n",
      "Training time per epochs: 4.41 sec\n",
      "###### Trained episodes = 1000 #######\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 121.2\n",
      "Av. agent reward = 12.12\n",
      "Agents crossed (2nd food pile) = 0.6\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 121.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 2.0\n",
      "Agent1 reward is 2.8\n",
      "Agent2 reward is 6.9\n",
      "Agent3 reward is 13.6\n",
      "Agent4 reward is 33.1\n",
      "Agent5 reward is 2.5\n",
      "Agent6 reward is 20.6\n",
      "Agent7 reward is -0.2\n",
      "Agent8 reward is 15.4\n",
      "Agent9 reward is 24.5\n",
      "Training time per epochs: 4.22 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 132.5\n",
      "Av. agent reward = 13.25\n",
      "Agents crossed (2nd food pile) = 0.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 132.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 4.8\n",
      "Agent1 reward is 0.1\n",
      "Agent2 reward is 0.1\n",
      "Agent3 reward is 5.0\n",
      "Agent4 reward is 8.5\n",
      "Agent5 reward is 1.6\n",
      "Agent6 reward is 62.2\n",
      "Agent7 reward is -1.0\n",
      "Agent8 reward is 18.7\n",
      "Agent9 reward is 32.5\n",
      "Training time per epochs: 4.05 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 107.2\n",
      "Av. agent reward = 10.72\n",
      "Agents crossed (2nd food pile) = 0.2\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 107.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.8\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.9\n",
      "Agent4 reward is 3.0\n",
      "Agent5 reward is 4.2\n",
      "Agent6 reward is 38.3\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 27.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 4.16 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 107.7\n",
      "Av. agent reward = 10.77\n",
      "Agents crossed (2nd food pile) = 0.1\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 107.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.9\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 1.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 9.2\n",
      "Agent6 reward is 34.7\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 29.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 4.16 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 93.0\n",
      "Av. agent reward = 9.30\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 93.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 0.0\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 30.0\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 30.0\n",
      "Agent9 reward is 33.0\n",
      "Training time per epochs: 4.01 sec\n",
      "###### Dir = models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/ #######\n",
      "###### Trained episodes = 500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 48.2\n",
      "Av. agent reward = 4.82\n",
      "Agents crossed (2nd food pile) = 0.4\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 48.2\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is -0.1\n",
      "Agent1 reward is 0.1\n",
      "Agent2 reward is -0.8\n",
      "Agent3 reward is -2.8\n",
      "Agent4 reward is -0.1\n",
      "Agent5 reward is 7.6\n",
      "Agent6 reward is 2.7\n",
      "Agent7 reward is 10.8\n",
      "Agent8 reward is 7.0\n",
      "Agent9 reward is 24.0\n",
      "Training time per epochs: 4.08 sec\n",
      "###### Trained episodes = 1000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 88.5\n",
      "Av. agent reward = 8.85\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 88.5\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is -1.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.6\n",
      "Agent3 reward is -0.5\n",
      "Agent4 reward is 4.5\n",
      "Agent5 reward is 8.7\n",
      "Agent6 reward is 21.5\n",
      "Agent7 reward is 19.2\n",
      "Agent8 reward is 11.7\n",
      "Agent9 reward is 23.8\n",
      "Training time per epochs: 3.81 sec\n",
      "###### Trained episodes = 1500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.7\n",
      "Av. agent reward = 9.27\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.7\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is -0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 5.9\n",
      "Agent5 reward is 1.9\n",
      "Agent6 reward is 23.2\n",
      "Agent7 reward is 28.1\n",
      "Agent8 reward is 19.4\n",
      "Agent9 reward is 14.3\n",
      "Training time per epochs: 3.85 sec\n",
      "###### Trained episodes = 2000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.9\n",
      "Av. agent reward = 9.29\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.9\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 2.0\n",
      "Agent5 reward is 5.9\n",
      "Agent6 reward is 18.3\n",
      "Agent7 reward is 28.1\n",
      "Agent8 reward is 31.6\n",
      "Agent9 reward is 7.0\n",
      "Training time per epochs: 3.66 sec\n",
      "###### Trained episodes = 2500 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 91.1\n",
      "Av. agent reward = 9.11\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 91.1\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.1\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 25.2\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 2.9\n",
      "Agent7 reward is 6.3\n",
      "Agent8 reward is 26.8\n",
      "Agent9 reward is 29.9\n",
      "Training time per epochs: 4.12 sec\n",
      "###### Trained episodes = 3000 #######\n",
      "..............................\n",
      "Average Statistics in Aggregate\n",
      "=================================\n",
      "Total rewards gathered = 92.0\n",
      "Av. agent reward = 9.20\n",
      "Agents crossed (2nd food pile) = 0.0\n",
      "\n",
      "Average Statistics by Tribe\n",
      "=============================\n",
      "Tribe Vikings has total reward of 92.0\n",
      "\n",
      "Average Statistics by Agent\n",
      "=============================\n",
      "Agent0 reward is 0.0\n",
      "Agent1 reward is 0.0\n",
      "Agent2 reward is 0.0\n",
      "Agent3 reward is 0.0\n",
      "Agent4 reward is 30.0\n",
      "Agent5 reward is 0.0\n",
      "Agent6 reward is 0.0\n",
      "Agent7 reward is 0.0\n",
      "Agent8 reward is 31.0\n",
      "Agent9 reward is 31.0\n",
      "Training time per epochs: 4.14 sec\n",
      "[8.083333333333332, 9.809999999999999, 9.106666666666666, 9.15, 9.256666666666666, 9.203333333333333]\n",
      "[8.973333333333333, 15.1, 12.033333333333333, 13.969999999999999, 21.27, 25.18]\n",
      "[6.6, 12.123333333333333, 13.246666666666666, 10.723333333333333, 10.77, 9.3]\n",
      "[4.82, 8.85, 9.273333333333333, 9.290000000000001, 9.113333333333333, 9.196666666666667]\n",
      "[0.06666666666666667, 0.2, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.6, 0.9333333333333333, 0.4, 1.1, 1.0, 1.2]\n",
      "[0.2, 0.5666666666666667, 0.4, 0.23333333333333334, 0.13333333333333333, 0.0]\n",
      "[0.43333333333333335, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "dir_names = [\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.0_rp-1.0_300gs/\",   # scenario=18\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t1.25_rp-1.0_300gs/\",   # scenario=19 \n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t2.0_rp-1.0_300gs/\",   # scenario=20\n",
    "    \"models/1T-10L/baseline/food_d37_river_w1_d25/pacifist/t4.0_rp-1.0_300gs/\"   # scenario=21   \n",
    "]  \n",
    "             \n",
    "episodes = [500, 1000, 1500, 2000, 2500, 3000] \n",
    "game = 'Crossing'\n",
    "culture = \"pacifist\"\n",
    "map_name = \"food_d37_river_w1_d25\"\n",
    "\n",
    "# Performance Statistics - for Research Report\n",
    "av_agent_reward = [[0 for i in episodes] for j in dir_names]\n",
    "av_agent_crossed = [[0 for i in episodes] for j in dir_names]  \n",
    "dominating_tribe = [[None for i in episodes] for j in dir_names]\n",
    "dom_tribe_reward = [[0 for i in episodes] for j in dir_names]\n",
    "dominance = [[0 for i in episodes] for j in dir_names]\n",
    "\n",
    "# There will be 10 agents - 0 teams of 0 AI agents each and 0 random agent\n",
    "num_ai_agents = 10\n",
    "num_rdn_agents = 0\n",
    "num_agents = num_ai_agents+num_rdn_agents  # just the sum of the two\n",
    "\n",
    "# Data structure for AI agents (agents will form their own Class later on)\n",
    "agents = []\n",
    "actions = []\n",
    "tags = []\n",
    "\n",
    "# Initialize environment\n",
    "render = True\n",
    "SPEED = 1/30\n",
    "river_penalty = -1\n",
    "num_actions = 8                       # There are 8 actions defined in Gathering\n",
    "second_pile_x = 50   # x-coordinate of the 2nd food pile\n",
    "\n",
    "# Initialize constants\n",
    "num_frames = 7\n",
    "max_episodes = 30\n",
    "max_frames = 500\n",
    "verbose = False\n",
    "\n",
    "\n",
    "for dir_num, dir_name in enumerate(dir_names):\n",
    "    print (\"###### Dir = {} #######\".format(dir_name))\n",
    "    \n",
    "    for eps_num, eps in enumerate(episodes):\n",
    "        print (\"###### Trained episodes = {} #######\".format(eps))\n",
    "    \n",
    "        # Load models for AI agents\n",
    "        agents= [[] for i in range(num_ai_agents)]\n",
    "        # If episodes is provided (not 0), load the model for each AI agent\n",
    "        for i in range(num_ai_agents):\n",
    "            model_file = dir_name+'MA{}_{}_ep{}.p'.format(i,game,eps)\n",
    "            try:\n",
    "                with open(model_file, 'rb') as f:\n",
    "                    print(\"Load saved model for agent {}\".format(i))\n",
    "                    agent = Policy(num_frames, num_actions, 0)\n",
    "                    optimizer = optim.Adam(agent.parameters(), lr=0.1)\n",
    "\n",
    "                    # New way to save and load models - based on: \n",
    "                    # https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "                    _ = load_model(agent, optimizer, f)\n",
    "                    agent.eval()\n",
    "                    agents[i] = agent\n",
    "            except OSError:\n",
    "                print('Model file not found.')\n",
    "                raise\n",
    "\n",
    "        # Load random agents    \n",
    "        for i in range(num_ai_agents,num_agents):\n",
    "            # print(\"Load random agent {}\".format(i))\n",
    "            agents.append(Rdn_Policy())\n",
    "        \n",
    "        # Establish tribal association\n",
    "        tribes = []\n",
    "        tribes.append(Tribe(name='Vikings',color='blue', culture=culture, \\\n",
    "                    agents=[agents[0], agents[1], agents[2], agents[3], agents[4], \\\n",
    "                           agents[5], agents[6], agents[7], agents[8], agents[9]]))\n",
    "\n",
    "        # Set up agent and tribe info to pass into env\n",
    "        agent_colors = [agent.color for agent in agents]\n",
    "        agent_tribes = [agent.tribe for agent in agents]\n",
    "        tribe_names = [tribe.name for tribe in tribes]\n",
    "        \n",
    "        env = CrossingEnv(n_agents=num_agents,agent_colors=agent_colors, agent_tribes=agent_tribes, \\\n",
    "                  map_name=map_name, river_penalty=river_penalty, tribes=tribe_names, \\\n",
    "                  debug_agent=0)\n",
    "\n",
    "        # Used to accumulate episode stats for averaging\n",
    "        cum_rewards = 0\n",
    "        cum_crossed = 0\n",
    "        cum_tags = 0\n",
    "        cum_US_hits = 0\n",
    "        cum_THEM_hits = 0\n",
    "        cum_agent_rewards = [0 for agent in agents]\n",
    "        cum_agent_tags = [0 for agent in agents]\n",
    "        cum_agent_US_hits = [0 for agent in agents]\n",
    "        cum_agent_THEM_hits = [0 for agent in agents]\n",
    "        cum_tribe_rewards = [0 for t in tribes if t.name is not 'Crazies']\n",
    "\n",
    "        cuda = False\n",
    "        start = time.time()\n",
    "\n",
    "        for ep in range(max_episodes):\n",
    "    \n",
    "            print('.', end='')  # To show progress\n",
    "    \n",
    "            # Initialize AI and random agent data\n",
    "            actions = [0 for i in range(num_agents)]\n",
    "            tags = [0 for i in range(num_agents)]\n",
    "            US_hits = [0 for i in range(num_agents)]\n",
    "            THEM_hits = [0 for i in range(num_agents)]\n",
    "            \n",
    "            # Keep track of agents gathering from 2nd food pile\n",
    "            crossed = [0 for i in range(num_ai_agents)]\n",
    "\n",
    "            env_obs = env.reset()  # Environment return observations\n",
    "            \"\"\"\n",
    "            # For Debug only\n",
    "            print (len(agents_obs))\n",
    "            print (agents_obs[0].shape)\n",
    "            \"\"\"\n",
    "    \n",
    "            # Unpack observations into data structure compatible with agent Policy\n",
    "            agents_obs = unpack_env_obs(env_obs)\n",
    "    \n",
    "            for i in range(num_ai_agents):    # Reset agent info - laser tag statistics\n",
    "                agents[i].reset_info()    \n",
    "    \n",
    "            if render:\n",
    "                env.render()\n",
    "                time.sleep(SPEED)  # Change speed of video rendering\n",
    "    \n",
    "            \"\"\"\n",
    "            # For Debug only\n",
    "            print (len(agents_obs))\n",
    "            print (agents_obs[0].shape)\n",
    "            \"\"\"\n",
    "    \n",
    "            \"\"\"\n",
    "            For now, we do not stack observations, and we do not implement LSTM\n",
    "    \n",
    "            state = np.stack([state]*num_frames)\n",
    "\n",
    "            # Reset LSTM hidden units when episode begins\n",
    "            cx = Variable(torch.zeros(1, 256))\n",
    "            hx = Variable(torch.zeros(1, 256))\n",
    "            \"\"\"\n",
    "\n",
    "            for frame in range(max_frames):\n",
    "\n",
    "                for i in range(num_ai_agents):    # For AI agents\n",
    "                    actions[i], _ = select_action(agents[i], agents_obs[i], cuda=cuda)\n",
    "                    if actions[i] is 6:  # action[i] is a tensor, .item() returns the integer\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "                \n",
    "                for i in range(num_ai_agents, num_agents):   # For random agents\n",
    "                    actions[i] = agents[i].select_action(agents_obs[i])\n",
    "                    if actions[i] is 6:\n",
    "                        tags[i] += 1   # record a tag for accessing aggressiveness\n",
    "        \n",
    "                \"\"\"\n",
    "                For now, we do not implement LSTM\n",
    "                # Select action\n",
    "                action, log_prob, state_value, (hx,cx)  = select_action(model, state, (hx,cx))        \n",
    "                \"\"\"\n",
    "\n",
    "                # if frame % 10 == 0:\n",
    "                #     print (actions)    \n",
    "            \n",
    "                # Perform step        \n",
    "                env_obs, reward, done, info = env.step(actions)\n",
    "        \n",
    "                \"\"\"\n",
    "                For Debug only\n",
    "                print (env_obs)\n",
    "                print (reward)\n",
    "                print (done) \n",
    "                \"\"\"\n",
    "\n",
    "                for i in range(num_ai_agents):\n",
    "                    agents[i].rewards.append(reward[i])  # Stack rewards\n",
    "\n",
    "        \n",
    "                # Unpack observations into data structure compatible with agent Policy\n",
    "                agents_obs = unpack_env_obs(env_obs)\n",
    "                load_info(agents, info, narrate=False)   # Load agent info for AI agents\n",
    "        \n",
    "                for i in range(num_agents):\n",
    "                    US_hits[i] += agents[i].US_hit\n",
    "                    THEM_hits[i] += agents[i].THEM_hit\n",
    "            \n",
    "                \"\"\"\n",
    "                For now, we do not stack observation, may come in handy later on\n",
    "        \n",
    "                # Evict oldest diff add new diff to state\n",
    "                next_state = np.stack([next_state]*num_frames)\n",
    "                next_state[1:, :, :] = state[:-1, :, :]\n",
    "                state = next_state\n",
    "                \"\"\"\n",
    "                        \n",
    "                if render and ep is 0:   # render only the 1st episode per batch of 30\n",
    "                    env.render()\n",
    "                    time.sleep(SPEED)  # Change speed of video rendering\n",
    "\n",
    "                if any(done):\n",
    "                    print(\"Done after {} frames\".format(frame))\n",
    "                    break\n",
    "                    \n",
    "                for (i, loc) in env.consumption:\n",
    "                    if loc[0] > second_pile_x:\n",
    "                        # print ('agent {} gathered an apple in 2nd pile'.format(i))\n",
    "                        crossed[i] = 1\n",
    "            \n",
    "            # Print out statistics of AI agents\n",
    "            ep_rewards = 0\n",
    "            ep_tags = 0\n",
    "            ep_US_hits = 0\n",
    "            ep_THEM_hits = 0\n",
    "            ep_crossed = sum(crossed)     # calculated num agents gathering in 2nd pile for episode\n",
    "\n",
    "            if verbose:\n",
    "                print ('\\nStatistics by Agent')\n",
    "                print ('===================')\n",
    "            for i in range(num_ai_agents):\n",
    "                agent_tags = sum(agents[i].tag_hist)\n",
    "                ep_tags += agent_tags\n",
    "                cum_agent_tags[i] += agent_tags\n",
    "\n",
    "                agent_reward = sum(agents[i].rewards)\n",
    "                ep_rewards += agent_reward\n",
    "                cum_agent_rewards[i] += agent_reward\n",
    "\n",
    "                agent_US_hits = sum(agents[i].US_hits)\n",
    "                agent_THEM_hits = sum(agents[i].THEM_hits)\n",
    "                ep_US_hits += agent_US_hits\n",
    "                ep_THEM_hits += agent_THEM_hits\n",
    "                cum_agent_US_hits[i] += agent_US_hits\n",
    "                cum_agent_THEM_hits[i] += agent_THEM_hits\n",
    "        \n",
    "                if verbose:\n",
    "                    print (\"Agent{} aggressiveness is {:.2f}\".format(i, agent_tags/frame))\n",
    "                    print (\"Agent{} reward is {:d}\".format(i, agent_reward))\n",
    "                    # print('US agents hit = {}'.format(agent_US_hits))\n",
    "                    # print('THEM agents hit = {}'.format(agent_THEM_hits ))\n",
    "        \n",
    "            cum_rewards += ep_rewards\n",
    "            cum_crossed += ep_crossed\n",
    "            cum_tags += ep_tags\n",
    "            cum_US_hits += ep_US_hits\n",
    "            cum_THEM_hits += ep_THEM_hits\n",
    "    \n",
    "            if verbose:\n",
    "                print ('\\nStatistics in Aggregate')\n",
    "                print ('=======================')\n",
    "                print ('Total rewards gathered = {}'.format(ep_rewards))\n",
    "                print ('Num agents crossed = {}'.format(ep_crossed))\n",
    "                # print ('Num laser fired = {}'.format(ep_tags))\n",
    "                # print ('Total US Hit (friendly fire) = {}'.format(ep_US_hits))\n",
    "                # print ('Total THEM Hit = {}'.format(ep_THEM_hits))\n",
    "                # print ('friendly fire (%) = {0:.3f}'.format(ep_US_hits/(ep_US_hits+ep_THEM_hits+1e-7)))\n",
    "\n",
    "            if verbose:\n",
    "                print ('\\nStatistics by Tribe')\n",
    "                print ('===================')\n",
    "            for i, t in enumerate(tribes):\n",
    "                if t.name is not 'Crazies':\n",
    "                    ep_tribe_reward = sum(t.sum_rewards())\n",
    "                    cum_tribe_rewards[i] += ep_tribe_reward\n",
    "                    if verbose:\n",
    "                        print ('Tribe {} has total reward of {}'.format(t.name, ep_tribe_reward))\n",
    "\n",
    "            for i in range(num_ai_agents):\n",
    "                agents[i].clear_history()\n",
    "\n",
    "        env.close()  # Close the rendering window\n",
    "        end = time.time()\n",
    "\n",
    "        print ('\\nAverage Statistics in Aggregate')\n",
    "        print ('=================================')\n",
    "        total_rewards = cum_rewards/max_episodes\n",
    "        print ('Total rewards gathered = {:.1f}'.format(total_rewards))\n",
    "        av_agent_reward[dir_num][eps_num] = cum_rewards/max_episodes/num_ai_agents\n",
    "        print ('Av. agent reward = {:.2f}'.format(av_agent_reward[dir_num][eps_num]))\n",
    "        av_agent_crossed[dir_num][eps_num] = cum_crossed/max_episodes\n",
    "        print ('Agents crossed (2nd food pile) = {:.1f}'.format(av_agent_crossed[dir_num][eps_num]))\n",
    "        # print ('Num laser fired = {:.1f}'.format(cum_tags/max_episodes))\n",
    "        # print ('Total US Hit (friendly fire) = {:.1f}'.format(cum_US_hits/max_episodes))\n",
    "        # print ('Total THEM Hit = {:.1f}'.format(cum_THEM_hits/max_episodes))\n",
    "        # print ('friendly fire (%) = {:.3f}'.format(cum_US_hits/(cum_US_hits+cum_THEM_hits+1e-7)))\n",
    "\n",
    "        print ('\\nAverage Statistics by Tribe')\n",
    "        print ('=============================')\n",
    "       \n",
    "        for i, tribe in enumerate(tribes):\n",
    "            if tribe.name is not 'Crazies':\n",
    "                tribe_reward = cum_tribe_rewards[i]/max_episodes\n",
    "                print ('Tribe {} has total reward of {:.1f}'.format(tribe.name, tribe_reward))    \n",
    "                \n",
    "                # Keep track of dominating team and the rewards gathered (only if more than 1 tribe)\n",
    "                if len(tribes) > 1:\n",
    "                    if tribe_reward > dom_tribe_reward[dir_num][eps_num]:   \n",
    "                        dom_tribe_reward[dir_num][eps_num] = tribe_reward\n",
    "                        dominating_tribe[dir_num][eps_num]  = tribe.name\n",
    "\n",
    "        # Team dominance calculation (only if more than 1 tribe)\n",
    "        if len(tribes) > 1:\n",
    "            print ('Dominating Tribe: {}'.format(dominating_tribe[dir_num][eps_num]))\n",
    "            dominance[dir_num][eps_num] = dom_tribe_reward[dir_num][eps_num]/((total_rewards - \\\n",
    "                                                dom_tribe_reward[dir_num][eps_num]+1.1e-7)/(len(tribes)-1))    \n",
    "            print ('Team dominance: {0:.2f}x'.format(dominance[dir_num][eps_num]))\n",
    "\n",
    "        print ('\\nAverage Statistics by Agent')\n",
    "        print ('=============================')\n",
    "        for i in range(num_ai_agents):\n",
    "            # print (\"Agent{} of {} aggressiveness is {:.2f}\".format(i, agents[i].tribe, \\\n",
    "            #                                               cum_agent_tags[i]/(max_episodes*max_frames)))\n",
    "            print (\"Agent{} reward is {:.1f}\".format(i, cum_agent_rewards[i]/max_episodes))\n",
    "            # print('US agents hit = {:.1f}'.format(cum_agent_US_hits[i]/max_episodes))\n",
    "            # print('THEM agents hit = {:.1f}'.format(cum_agent_THEM_hits[i]/max_episodes))\n",
    "\n",
    "        print('Training time per epochs: {:.2f} sec'.format((end-start)/max_episodes))\n",
    "\n",
    "# Note: Statistics for Research Report        \n",
    "for reward in av_agent_reward:   # Average agent reward\n",
    "    print(reward)\n",
    "for agents_crossed in av_agent_crossed:   # Average num agents gathering in 2nd food pile\n",
    "    print(agents_crossed)\n",
    "\n",
    "# print dominating team and dominance factor (only if more than 1 tribe)\n",
    "if len(tribes) > 1:\n",
    "    for tribe in dominating_tribe:   # Dominating team\n",
    "        print(tribe)\n",
    "    for value in dominance:      # Team dominance\n",
    "        print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Agent Rewards\n",
      "[8.083333333333332, 9.809999999999999, 9.106666666666666, 9.15, 9.256666666666666, 9.203333333333333]\n",
      "[8.973333333333333, 15.1, 12.033333333333333, 13.969999999999999, 21.27, 25.18]\n",
      "[6.6, 12.123333333333333, 13.246666666666666, 10.723333333333333, 10.77, 9.3]\n",
      "[4.82, 8.85, 9.273333333333333, 9.290000000000001, 9.113333333333333, 9.196666666666667]\n",
      "Agents Crossed (2nd food pile)\n",
      "[0.06666666666666667, 0.2, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.6, 0.9333333333333333, 0.4, 1.1, 1.0, 1.2]\n",
      "[0.2, 0.5666666666666667, 0.4, 0.23333333333333334, 0.13333333333333333, 0.0]\n",
      "[0.43333333333333335, 0.03333333333333333, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Note: Statistics for Research Report   \n",
    "print ('Average Agent Rewards')\n",
    "for reward in av_agent_reward:   # Average agent reward\n",
    "    print(reward)\n",
    "    \n",
    "print ('Agents Crossed (2nd food pile)')    \n",
    "for agents_crossed in av_agent_crossed:   # Average num agents gathering in 2nd food pile\n",
    "    print(agents_crossed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
